# End-to-end latency versus normalized vehicle density. The proposed Hydra-RAN maintains superior performance across all traffic conditions, with particularly pronounced gains in high-density scenarios.

# -*- coding: utf-8 -*-
"""
Dynamic Performance Evaluation Simulation Framework
IEEE Transactions on Communications
Hydra-RAN Enhanced Autonomous Vehicles

Author: Rafid I. Abd et al.
Modified: Fully dynamic and responsive simulation framework
"""

import numpy as np
import matplotlib.pyplot as plt
from dataclasses import dataclass, field
from typing import Dict, List, Tuple, Optional, Callable, Any
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

# ===============================
# IEEE TCOM Global Style
# ===============================
plt.rcParams.update({
    "font.family": "Times New Roman",
    "axes.linewidth": 1.4,
    "axes.edgecolor": "0.2",
    "figure.figsize": (14, 10),
    "font.size": 12
})

# ===============================
# Enhanced Data Structures
# ===============================

class ApproachType(Enum):
    """Types of approaches for categorization."""
    BASELINE = "baseline"
    PROPOSED = "proposed"
    HYBRID = "hybrid"

@dataclass
class DynamicParameters:
    """Centralized dynamic simulation parameters."""
    # Latency parameters
    latency_min: float = 10.0
    latency_max: float = 120.0
    latency_points: int = 20
    
    # Network parameters
    bandwidth: float = 100.0  # MHz
    noise_power: float = -174.0  # dBm/Hz
    tx_power: float = 23.0  # dBm
    path_loss_exponent: float = 3.5
    shadowing_std: float = 8.0
    network_congestion: float = 0.3  # 0-1 scale
    
    # Dataset parameters
    dataset_size: int = 1000
    feature_dim: int = 512
    num_classes: int = 10
    
    # KD model parameters with dynamic ranges
    kd_alpha_range: Tuple[float, float] = (0.001, 0.05)
    kd_beta_range: Tuple[float, float] = (0.001, 0.02)
    kd_max_gain_range: Tuple[float, float] = (20.0, 100.0)
    
    # System parameters
    vehicle_density: float = 50.0  # vehicles/km²
    rsu_density: float = 5.0  # RSUs/km
    mobility_speed: float = 60.0  # km/h
    channel_coherence_time: float = 10.0  # ms
    packet_loss_rate: float = 0.05  # 0-1
    
    # Computation parameters
    gpu_capacity: float = 10.0  # TFLOPS
    memory_budget: float = 8.0  # GB
    energy_budget: float = 100.0  # Watt-hours
    
    # Performance weights
    weight_latency: float = 0.3
    weight_reliability: float = 0.3
    weight_accuracy: float = 0.4
    weight_energy: float = 0.1
    
    # Random seed
    seed: int = 7
    
    def __post_init__(self):
        """Initialize derived parameters."""
        np.random.seed(self.seed)
        self.latency = np.linspace(
            self.latency_min, 
            self.latency_max, 
            self.latency_points
        )
        
    def update(self, **kwargs):
        """Update parameters dynamically."""
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
        self.__post_init__()
        return self
    
    def compute_channel_quality(self) -> float:
        """Compute dynamic channel quality index."""
        sinr = self.tx_power - self.noise_power - 10*np.log10(self.bandwidth*1e6)
        sinr -= self.path_loss_exponent * 20 * np.log10(100)
        sinr -= np.random.randn() * self.shadowing_std
        sinr_penalty = self.packet_loss_rate * 20
        congestion_penalty = self.network_congestion * 10
        return max(0.1, sinr - sinr_penalty - congestion_penalty) / 100
    
    def compute_system_load(self) -> float:
        """Compute normalized system load dynamically."""
        comm_load = (self.vehicle_density * self.mobility_speed) / \
                    (self.rsu_density * self.bandwidth)
        comp_load = self.feature_dim * self.num_classes / (self.gpu_capacity * 1000)
        return min(1.0, 0.6 * comm_load + 0.4 * comp_load)
    
    def compute_energy_efficiency(self) -> float:
        """Compute energy efficiency factor."""
        energy_per_operation = 0.1 * (self.feature_dim / 512) * (self.num_classes / 10)
        available_energy = self.energy_budget / 100
        return min(1.0, available_energy / (energy_per_operation + 0.1))

@dataclass
class DynamicDataset:
    """Represents dynamic input dataset characteristics."""
    name: str
    size: int
    feature_dim: int
    num_classes: int
    complexity: float  # 0-1 scale
    noise_level: float  # 0-1 scale
    spatial_variation: float  # 0-1 scale
    temporal_variation: float  # 0-1 scale
    label_noise: float  # 0-1 scale
    data_drift_rate: float  # 0-1 scale
    imbalance_ratio: float = 0.2
    missing_data_rate: float = 0.05
    
    def __post_init__(self):
        """Initialize dynamic properties."""
        self.current_complexity = self.complexity
        self.current_noise = self.noise_level
        self.current_variation = self.spatial_variation
        self.drift_counter = 0
        
    def apply_drift(self, step: int = 1):
        """Apply data drift over time."""
        self.drift_counter += step
        drift_factor = 1 + self.data_drift_rate * np.sin(self.drift_counter / 10)
        noise_factor = 1 + 0.2 * np.sin(self.drift_counter / 5)
        
        self.current_complexity = min(1.0, self.complexity * drift_factor)
        self.current_noise = min(1.0, self.noise_level * noise_factor)
        self.current_variation = min(1.0, self.spatial_variation * drift_factor)
        
    def get_impact_factors(self, params: DynamicParameters) -> Dict[str, float]:
        """Calculate dynamic impact factors on model performance."""
        channel_quality = params.compute_channel_quality()
        system_load = params.compute_system_load()
        energy_efficiency = params.compute_energy_efficiency()
        
        return {
            'data_quality': max(0.1, 1.0 - self.current_noise - self.label_noise - self.missing_data_rate),
            'model_complexity': self.current_complexity * (1 + 0.5 * np.log2(self.num_classes)),
            'variability': self.current_variation * (1 + self.temporal_variation),
            'channel_quality': channel_quality,
            'system_load': system_load,
            'energy_efficiency': energy_efficiency,
            'data_freshness': 1.0 - min(1.0, self.drift_counter / 100),
            'imbalance_effect': 1.0 - 0.3 * self.imbalance_ratio
        }
    
    def generate_performance_noise(self, base_value: float, params: DynamicParameters) -> float:
        """Add dataset-specific performance noise."""
        variability = self.get_impact_factors(params)['variability']
        noise_magnitude = variability * 0.15 * base_value
        return base_value * (1 + np.random.randn() * 0.1) + np.random.randn() * noise_magnitude

# ===============================
# Dynamic Core Functions
# ===============================

def dynamic_kd_gain(lat: np.ndarray, alpha: float, beta: float, 
                   max_gain: float, factors: Dict[str, float],
                   approach_type: ApproachType) -> np.ndarray:
    """
    Dynamic KD gain model with multiple influencing factors.
    """
    # Base saturating curve
    base_gain = max_gain * (1 - np.exp(-alpha * lat)) * np.exp(-beta * lat)
    
    # Apply approach-specific scaling
    if approach_type == ApproachType.PROPOSED:
        # Hydra-RAN benefits more from good conditions
        channel_effect = 0.6 + 0.4 * factors['channel_quality']
        data_quality_boost = 0.6 + 0.4 * factors['data_quality']
        complexity_benefit = 1 + 0.3 * factors['model_complexity']
    elif approach_type == ApproachType.HYBRID:
        # Hybrid approaches have moderate benefits
        channel_effect = 0.5 + 0.5 * factors['channel_quality']
        data_quality_boost = 0.7 + 0.3 * factors['data_quality']
        complexity_benefit = 1 + 0.2 * factors['model_complexity']
    else:
        # Baseline approaches have standard scaling
        channel_effect = 0.4 + 0.6 * factors['channel_quality']
        data_quality_boost = 0.8 + 0.2 * factors['data_quality']
        complexity_benefit = 1 + 0.1 * factors['model_complexity']
    
    # Apply all effects
    base_gain *= channel_effect
    base_gain *= data_quality_boost
    base_gain *= complexity_benefit
    
    # Apply system load penalty
    load_penalty = 1.0 - 0.4 * factors['system_load']
    base_gain *= load_penalty
    
    # Apply energy efficiency effect
    energy_effect = 0.7 + 0.3 * factors['energy_efficiency']
    base_gain *= energy_effect
    
    # Apply data imbalance effect
    base_gain *= factors['imbalance_effect']
    
    # Add variability-based noise
    variability_noise = factors['variability'] * 0.1 * base_gain
    base_gain += np.random.randn(len(lat)) * variability_noise
    
    return np.clip(base_gain, 0, max_gain * 1.3)

def compute_reliability(lat: np.ndarray, factors: Dict[str, float], 
                       packet_loss_rate: float) -> np.ndarray:
    """Compute reliability as function of latency and conditions."""
    base_reliability = np.exp(-lat / 100)
    channel_effect = factors['channel_quality']
    load_effect = 1.0 - 0.5 * factors['system_load']
    packet_loss_effect = 1.0 - packet_loss_rate
    
    reliability = base_reliability * channel_effect * load_effect * packet_loss_effect
    
    # Add small noise
    reliability *= (1 + np.random.randn(len(lat)) * 0.05)
    
    return np.clip(reliability, 0, 1)

def compute_accuracy(gain: np.ndarray, factors: Dict[str, float]) -> np.ndarray:
    """Compute accuracy from KD gain and conditions."""
    base_accuracy = 0.7 + 0.3 * (gain / 100)
    data_quality_effect = 0.5 + 0.5 * factors['data_quality']
    complexity_effect = 1.0 - 0.2 * factors['model_complexity']
    
    accuracy = base_accuracy * data_quality_effect * complexity_effect
    
    # Apply freshness effect
    freshness_effect = 0.8 + 0.2 * factors['data_freshness']
    accuracy *= freshness_effect
    
    return np.clip(accuracy, 0.5, 0.99)

def compute_energy_consumption(lat: np.ndarray, gain: np.ndarray, 
                             factors: Dict[str, float]) -> np.ndarray:
    """Compute energy consumption."""
    base_energy = 0.1 * lat + 0.01 * gain  # Simplified model
    energy_efficiency = factors['energy_efficiency']
    load_penalty = 1 + 0.5 * factors['system_load']
    
    return base_energy * load_penalty / (energy_efficiency + 0.1)

def compute_composite_score(gain: np.ndarray, lat: np.ndarray, 
                           reliability: np.ndarray, accuracy: np.ndarray,
                           energy: np.ndarray, weights: Tuple) -> np.ndarray:
    """Compute composite performance score."""
    # Normalize metrics
    norm_gain = gain / 100
    norm_lat = 1.0 - (lat - lat.min()) / (lat.max() - lat.min())
    norm_rel = reliability
    norm_acc = accuracy
    norm_energy = 1.0 - (energy - energy.min()) / (energy.max() - energy.min() + 1e-10)
    
    # Weighted sum
    return (weights[0] * norm_gain + 
            weights[1] * norm_lat + 
            weights[2] * norm_rel + 
            weights[3] * norm_acc + 
            weights[4] * norm_energy)

# ===============================
# Dynamic Approach System
# ===============================

class DynamicApproach:
    """Dynamic approach configuration with adaptive parameters."""
    
    def __init__(self, name: str, approach_type: ApproachType,
                 base_alpha: float, base_beta: float, base_max_gain: float,
                 adaptation_rate: float = 0.1,
                 metadata: Optional[Dict] = None):
        self.name = name
        self.approach_type = approach_type
        self.base_alpha = base_alpha
        self.base_beta = base_beta
        self.base_max_gain = base_max_gain
        self.adaptation_rate = adaptation_rate
        self.metadata = metadata or {}
        self.performance_history = []
        self.parameter_history = []
        
        # Set dynamic plot style
        if self.approach_type == ApproachType.PROPOSED:
            self.plot_style = {'color': 'k', 'linewidth': 5, 'linestyle': '-', 'alpha': 0.9}
        elif self.approach_type == ApproachType.HYBRID:
            self.plot_style = {'color': 'purple', 'linewidth': 3, 'linestyle': '--', 'marker': 'o'}
        else:
            markers = ['s', '^', 'd', 'v', 'p', 'h', '8']
            colors = ['b', 'g', 'r', 'c', 'm', 'y', 'orange']
            idx = hash(self.name) % len(markers)
            self.plot_style = {
                'marker': markers[idx],
                'linestyle': '-',
                'markersize': 8,
                'linewidth': 2,
                'color': colors[idx],
                'alpha': 0.7
            }
    
    def adapt_parameters(self, factors: Dict[str, float], 
                        params: DynamicParameters, 
                        previous_params: Optional[Dict] = None) -> Dict[str, float]:
        """Dynamically adapt parameters based on current conditions."""
        
        # Base adaptations
        alpha_adapt = factors['data_quality'] * (1 - 0.3 * factors['system_load'])
        beta_adapt = (1 + factors['variability']) * (1 + 0.2 * factors['system_load'])
        gain_adapt = (1 + 0.4 * factors['model_complexity']) * factors['channel_quality']
        
        # Approach-specific adaptations
        if 'hierarchical' in self.metadata.get('features', []):
            # Hydra-RAN specific adaptations
            alpha_adapt *= 1.2  # Faster learning
            beta_adapt *= 0.7   # Less sensitive to latency
            gain_adapt *= 1.3   # Higher gain potential
            # Adapt to energy efficiency
            gain_adapt *= (0.8 + 0.4 * factors['energy_efficiency'])
            
        elif 'federated' in self.metadata.get('model_type', ''):
            # Federated learning adaptations
            gain_adapt *= factors['data_freshness']
            alpha_adapt *= 0.9  # Slower convergence
            beta_adapt *= 1.1   # More sensitive to latency
            
        elif 'semantic' in self.metadata.get('model_type', ''):
            # Semantic communication adaptations
            gain_adapt *= (0.7 + 0.3 * factors['channel_quality'])
            alpha_adapt *= 1.1
            beta_adapt *= 0.9
            
        # Apply adaptation with momentum if previous parameters exist
        if previous_params:
            momentum = 0.3
            alpha_adapt = momentum * previous_params['alpha'] + (1 - momentum) * alpha_adapt
            beta_adapt = momentum * previous_params['beta'] + (1 - momentum) * beta_adapt
            gain_adapt = momentum * previous_params['max_gain'] + (1 - momentum) * gain_adapt
        
        # Compute adapted parameters
        adapted_alpha = self.base_alpha * alpha_adapt
        adapted_beta = self.base_beta * beta_adapt
        adapted_max_gain = self.base_max_gain * gain_adapt
        
        # Clip to valid ranges
        adapted_alpha = np.clip(adapted_alpha, 
                               params.kd_alpha_range[0], 
                               params.kd_alpha_range[1])
        adapted_beta = np.clip(adapted_beta,
                              params.kd_beta_range[0],
                              params.kd_beta_range[1])
        adapted_max_gain = np.clip(adapted_max_gain,
                                  params.kd_max_gain_range[0],
                                  params.kd_max_gain_range[1])
        
        # Add small random variation for realism (10% chance)
        if np.random.rand() < 0.1:
            variation = np.random.uniform(0.95, 1.05)
            adapted_max_gain *= variation
            
        return {
            'alpha': adapted_alpha,
            'beta': adapted_beta,
            'max_gain': adapted_max_gain
        }
    
    def compute_performance(self, lat: np.ndarray, params: DynamicParameters,
                          factors: Dict[str, float]) -> Dict[str, np.ndarray]:
        """Compute complete performance metrics."""
        # Get adapted parameters
        prev_params = self.parameter_history[-1] if self.parameter_history else None
        adapted_params = self.adapt_parameters(factors, params, prev_params)
        self.parameter_history.append(adapted_params)
        
        # Compute KD gain
        kd_gain = dynamic_kd_gain(lat, adapted_params['alpha'], 
                                 adapted_params['beta'], adapted_params['max_gain'],
                                 factors, self.approach_type)
        
        # Add dataset-specific noise
        kd_gain = np.array([params.dataset.generate_performance_noise(v, params) 
                           for v in kd_gain])
        
        # Compute other metrics
        reliability = compute_reliability(lat, factors, params.packet_loss_rate)
        accuracy = compute_accuracy(kd_gain, factors)
        energy = compute_energy_consumption(lat, kd_gain, factors)
        
        # Composite score
        composite = compute_composite_score(
            kd_gain, lat, reliability, accuracy, energy,
            (params.weight_latency, params.weight_reliability,
             params.weight_accuracy, params.weight_energy, 0.1)
        )
        
        return {
            'kd_gain': kd_gain,
            'reliability': reliability,
            'accuracy': accuracy,
            'energy_consumption': energy,
            'composite_score': composite,
            'adapted_params': adapted_params
        }

# ===============================
# Main Dynamic Simulation Framework
# ===============================

class DynamicSimulationFramework:
    """Main dynamic simulation framework."""
    
    def __init__(self):
        self.params = DynamicParameters()
        self.dataset = None
        self.approaches: List[DynamicApproach] = []
        self.results: Dict[str, Dict] = {}
        self.history: List[Dict] = []
        self.current_step = 0
        
    def set_dataset(self, dataset: DynamicDataset):
        """Set the current dataset."""
        self.dataset = dataset
        return self
    
    def add_approach(self, approach: DynamicApproach):
        """Add an approach to simulation."""
        self.approaches.append(approach)
        return self
    
    def reset(self):
        """Reset simulation state."""
        self.results.clear()
        self.history.clear()
        self.current_step = 0
        for approach in self.approaches:
            approach.performance_history.clear()
            approach.parameter_history.clear()
        return self
    
    def simulate_step(self, step_duration: float = 1.0) -> Dict:
        """Simulate one time step."""
        if self.dataset is None:
            raise ValueError("No dataset set. Use set_dataset() first.")
        
        # Apply dataset drift
        self.dataset.apply_drift(self.current_step)
        
        # Get current environmental factors
        factors = self.dataset.get_impact_factors(self.params)
        
        # Simulate each approach
        step_results = {}
        for approach in self.approaches:
            perf = approach.compute_performance(
                self.params.latency, self.params, factors
            )
            step_results[approach.name] = perf
            approach.performance_history.append(perf)
        
        # Record system state
        system_state = {
            'step': self.current_step,
            'timestamp': self.current_step * step_duration,
            'factors': factors.copy(),
            'params': {
                'latency_range': (self.params.latency_min, self.params.latency_max),
                'bandwidth': self.params.bandwidth,
                'vehicle_density': self.params.vehicle_density,
                'dataset_complexity': self.dataset.current_complexity,
                'dataset_noise': self.dataset.current_noise,
                'channel_quality': factors['channel_quality'],
                'system_load': factors['system_load']
            },
            'results': {name: {k: v.mean() if isinstance(v, np.ndarray) else v 
                             for k, v in perf.items() if k != 'adapted_params'}
                      for name, perf in step_results.items()}
        }
        
        self.history.append(system_state)
        self.results[f'step_{self.current_step}'] = step_results
        self.current_step += 1
        
        return step_results
    
    def simulate(self, num_steps: int = 10, step_duration: float = 1.0):
        """Run multiple simulation steps."""
        self.reset()
        for _ in range(num_steps):
            self.simulate_step(step_duration)
        return self
    
    def update_parameters(self, **kwargs):
        """Update simulation parameters."""
        self.params.update(**kwargs)
        return self
    
    def modify_dataset(self, **kwargs):
        """Modify dataset characteristics."""
        if self.dataset:
            for key, value in kwargs.items():
                if hasattr(self.dataset, key):
                    setattr(self.dataset, key, value)
            # Reset drift counter when modifying dataset
            self.dataset.drift_counter = 0
        return self

# ===============================
# Visualization and Analysis
# ===============================

class SimulationVisualizer:
    """Visualization tools for dynamic simulation."""
    
    @staticmethod
    def plot_dynamic_performance(sim: DynamicSimulationFramework, 
                                step: Optional[int] = None):
        """Plot dynamic performance results."""
        if step is None:
            step = sim.current_step - 1 if sim.current_step > 0 else 0
        
        results = sim.results.get(f'step_{step}', {})
        if not results:
            print(f"No results for step {step}")
            return
        
        fig, axes = plt.subplots(3, 2, figsize=(16, 18))
        axes = axes.flatten()
        
        # Plot 1: KD Gain vs Latency
        ax = axes[0]
        for approach in sim.approaches:
            if approach.name in results:
                data = results[approach.name]
                ax.plot(sim.params.latency, data['kd_gain'], 
                       label=approach.name, **approach.plot_style)
        ax.set_xlabel('End-to-End Latency (ms)', fontsize=12)
        ax.set_ylabel('KD Gain (%)', fontsize=12)
        ax.set_title('KD Gain vs Latency (Current Step)', fontsize=14)
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=10, loc='best')
        
        # Plot 2: Composite Score
        ax = axes[1]
        for approach in sim.approaches:
            if approach.name in results:
                data = results[approach.name]
                ax.plot(sim.params.latency, data['composite_score'],
                       label=approach.name, **approach.plot_style)
        ax.set_xlabel('End-to-End Latency (ms)', fontsize=12)
        ax.set_ylabel('Composite Score', fontsize=12)
        ax.set_title('Composite Performance Score', fontsize=14)
        ax.grid(True, alpha=0.3)
        
        # Plot 3: Performance Over Time
        ax = axes[2]
        for approach in sim.approaches:
            if approach.performance_history:
                avg_gains = [np.mean(step['kd_gain']) 
                           for step in approach.performance_history]
                ax.plot(range(len(avg_gains)), avg_gains, 
                       label=approach.name, **approach.plot_style)
        ax.set_xlabel('Simulation Step', fontsize=12)
        ax.set_ylabel('Average KD Gain (%)', fontsize=12)
        ax.set_title('Performance Evolution Over Time', fontsize=14)
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=10, loc='best')
        
        # Plot 4: Parameter Adaptation
        ax = axes[3]
        for approach in sim.approaches:
            if approach.parameter_history:
                alphas = [p['alpha'] for p in approach.parameter_history]
                betas = [p['beta'] for p in approach.parameter_history]
                ax.plot(range(len(alphas)), alphas, 
                       label=f'{approach.name} (α)', 
                       linestyle='-', alpha=0.7)
                ax.plot(range(len(betas)), betas, 
                       label=f'{approach.name} (β)', 
                       linestyle='--', alpha=0.7)
        ax.set_xlabel('Simulation Step', fontsize=12)
        ax.set_ylabel('Parameter Value', fontsize=12)
        ax.set_title('Parameter Adaptation Over Time', fontsize=14)
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=9, loc='best')
        
        # Plot 5: System State
        ax = axes[4]
        if sim.history:
            steps = [h['step'] for h in sim.history]
            channel_qualities = [h['factors']['channel_quality'] for h in sim.history]
            system_loads = [h['factors']['system_load'] for h in sim.history]
            data_qualities = [h['factors']['data_quality'] for h in sim.history]
            
            ax.plot(steps, channel_qualities, label='Channel Quality', 
                   linewidth=2, color='blue')
            ax.plot(steps, system_loads, label='System Load', 
                   linewidth=2, color='red')
            ax.plot(steps, data_qualities, label='Data Quality', 
                   linewidth=2, color='green')
            ax.set_xlabel('Simulation Step', fontsize=12)
            ax.set_ylabel('Normalized Value', fontsize=12)
            ax.set_title('System State Evolution', fontsize=14)
            ax.legend(fontsize=10)
            ax.grid(True, alpha=0.3)
        
        # Plot 6: Performance Distribution
        ax = axes[5]
        all_gains = []
        labels = []
        colors = []
        for approach in sim.approaches:
            if approach.name in results:
                avg_gain = np.mean(results[approach.name]['kd_gain'])
                all_gains.append(avg_gain)
                labels.append(approach.name)
                colors.append(approach.plot_style['color'])
        
        bars = ax.bar(range(len(all_gains)), all_gains, color=colors, alpha=0.7)
        ax.set_xticks(range(len(all_gains)))
        ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=10)
        ax.set_ylabel('Average KD Gain (%)', fontsize=12)
        ax.set_title('Performance Distribution (Current Step)', fontsize=14)
        
        plt.suptitle(f'Dynamic Performance Evaluation - Step {step}', fontsize=16)
        plt.tight_layout()
        plt.show()
    
    @staticmethod
    def plot_sensitivity_analysis(sim: DynamicSimulationFramework, 
                                 param_name: str, 
                                 values: np.ndarray):
        """Plot sensitivity analysis for a parameter."""
        original_value = getattr(sim.params, param_name)
        avg_gains = {approach.name: [] for approach in sim.approaches}
        
        for value in values:
            sim.update_parameters(**{param_name: value})
            sim.reset()
            sim.simulate(num_steps=1)
            
            for approach in sim.approaches:
                if approach.performance_history:
                    avg_gain = np.mean(approach.performance_history[-1]['kd_gain'])
                    avg_gains[approach.name].append(avg_gain)
        
        # Restore original value
        sim.update_parameters(**{param_name: original_value})
        
        # Plot
        plt.figure(figsize=(12, 8))
        for approach in sim.approaches:
            plt.plot(values, avg_gains[approach.name], 
                    label=approach.name, **approach.plot_style)
        
        plt.xlabel(f'{param_name.replace("_", " ").title()}', fontsize=14)
        plt.ylabel('Average KD Gain (%)', fontsize=14)
        plt.title(f'Sensitivity Analysis: {param_name.replace("_", " ").title()}', fontsize=16)
        plt.grid(True, alpha=0.3)
        plt.legend(fontsize=11)
        plt.tight_layout()
        plt.show()
        
        return avg_gains

# ===============================
# Dataset Factory
# ===============================

def create_dynamic_dataset_scenarios() -> Dict[str, DynamicDataset]:
    """Create dynamic dataset scenarios."""
    return {
        'urban_dynamic': DynamicDataset(
            name='Urban Driving Scenario',
            size=2000,
            feature_dim=768,
            num_classes=12,
            complexity=0.7,
            noise_level=0.4,
            spatial_variation=0.6,
            temporal_variation=0.5,
            label_noise=0.2,
            data_drift_rate=0.1,
            imbalance_ratio=0.25,
            missing_data_rate=0.08
        ),
        'highway_stable': DynamicDataset(
            name='Highway Scenario',
            size=1500,
            feature_dim=640,
            num_classes=8,
            complexity=0.5,
            noise_level=0.2,
            spatial_variation=0.3,
            temporal_variation=0.2,
            label_noise=0.1,
            data_drift_rate=0.05,
            imbalance_ratio=0.15,
            missing_data_rate=0.03
        ),
        'dense_urban': DynamicDataset(
            name='Dense Urban Scenario',
            size=3000,
            feature_dim=896,
            num_classes=18,
            complexity=0.8,
            noise_level=0.6,
            spatial_variation=0.7,
            temporal_variation=0.6,
            label_noise=0.3,
            data_drift_rate=0.15,
            imbalance_ratio=0.35,
            missing_data_rate=0.12
        ),
        'edge_case': DynamicDataset(
            name='Edge Case Scenario',
            size=800,
            feature_dim=512,
            num_classes=6,
            complexity=0.9,
            noise_level=0.8,
            spatial_variation=0.9,
            temporal_variation=0.8,
            label_noise=0.4,
            data_drift_rate=0.25,
            imbalance_ratio=0.5,
            missing_data_rate=0.2
        ),
        'mixed_traffic': DynamicDataset(
            name='Mixed Traffic Scenario',
            size=2500,
            feature_dim=832,
            num_classes=15,
            complexity=0.75,
            noise_level=0.5,
            spatial_variation=0.65,
            temporal_variation=0.55,
            label_noise=0.25,
            data_drift_rate=0.12,
            imbalance_ratio=0.3,
            missing_data_rate=0.1
        )
    }

# ===============================
# Approach Factory
# ===============================

def create_dynamic_approaches() -> List[DynamicApproach]:
    """Create dynamic approach configurations."""
    return [
        DynamicApproach(
            name='Cooperative Perception',
            approach_type=ApproachType.BASELINE,
            base_alpha=0.015,
            base_beta=0.010,
            base_max_gain=28,
            adaptation_rate=0.1,
            metadata={'model_type': 'raw_exchange', 'features': ['basic']}
        ),
        DynamicApproach(
            name='V2X-ViTv2',
            approach_type=ApproachType.BASELINE,
            base_alpha=0.020,
            base_beta=0.009,
            base_max_gain=40,
            adaptation_rate=0.15,
            metadata={'model_type': 'transformer', 'features': ['attention']}
        ),
        DynamicApproach(
            name='Semantic Communication',
            approach_type=ApproachType.BASELINE,
            base_alpha=0.025,
            base_beta=0.007,
            base_max_gain=52,
            adaptation_rate=0.18,
            metadata={'model_type': 'semantic', 'features': ['semantic']}
        ),
        DynamicApproach(
            name='Task-Oriented Comm.',
            approach_type=ApproachType.BASELINE,
            base_alpha=0.030,
            base_beta=0.006,
            base_max_gain=60,
            adaptation_rate=0.2,
            metadata={'model_type': 'task_aware', 'features': ['task']}
        ),
        DynamicApproach(
            name='Digital Twin',
            approach_type=ApproachType.BASELINE,
            base_alpha=0.028,
            base_beta=0.0065,
            base_max_gain=58,
            adaptation_rate=0.17,
            metadata={'model_type': 'digital_twin', 'features': ['simulation']}
        ),
        DynamicApproach(
            name='Edge-Collaborative',
            approach_type=ApproachType.BASELINE,
            base_alpha=0.032,
            base_beta=0.006,
            base_max_gain=62,
            adaptation_rate=0.22,
            metadata={'model_type': 'edge_drl', 'features': ['distributed']}
        ),
        DynamicApproach(
            name='FedRSU',
            approach_type=ApproachType.BASELINE,
            base_alpha=0.026,
            base_beta=0.0072,
            base_max_gain=55,
            adaptation_rate=0.19,
            metadata={'model_type': 'federated', 'features': ['privacy']}
        ),
        DynamicApproach(
            name='Hydra-RAN (Proposed)',
            approach_type=ApproachType.PROPOSED,
            base_alpha=0.027,
            base_beta=0.003,
            base_max_gain=82,
            adaptation_rate=0.25,
            metadata={
                'model_type': 'hierarchical_kd',
                'features': ['goal_aware', 'belief_driven', 'adaptive', 'hierarchical']
            }
        ),
        DynamicApproach(
            name='Adaptive Edge AI',
            approach_type=ApproachType.HYBRID,
            base_alpha=0.022,
            base_beta=0.005,
            base_max_gain=65,
            adaptation_rate=0.23,
            metadata={'model_type': 'adaptive', 'features': ['adaptive', 'edge']}
        )
    ]

# ===============================
# Performance Analyzer
# ===============================

class PerformanceAnalyzer:
    """Analyze simulation performance results."""
    
    @staticmethod
    def print_summary(sim: DynamicSimulationFramework, step: Optional[int] = None):
        """Print comprehensive performance summary."""
        if step is None:
            step = sim.current_step - 1 if sim.current_step > 0 else 0
        
        results = sim.results.get(f'step_{step}', {})
        
        print("=" * 90)
        print("DYNAMIC PERFORMANCE EVALUATION SUMMARY")
        print("=" * 90)
        print(f"Simulation Step: {step}")
        print(f"Dataset: {sim.dataset.name if sim.dataset else 'None'}")
        print(f"System Conditions:")
        print(f"  - Latency Range: {sim.params.latency_min}-{sim.params.latency_max} ms")
        print(f"  - Bandwidth: {sim.params.bandwidth} MHz")
        print(f"  - Vehicle Density: {sim.params.vehicle_density} veh/km²")
        print(f"  - Network Congestion: {sim.params.network_congestion:.2f}")
        print("-" * 90)
        
        if sim.history and step < len(sim.history):
            factors = sim.history[step]['factors']
            print("Current Environmental Factors:")
            for factor, value in factors.items():
                print(f"  - {factor}: {value:.3f}")
            print("-" * 90)
        
        # Sort approaches by average KD gain
        sorted_approaches = sorted(sim.approaches, 
                                  key=lambda a: np.mean(results.get(a.name, {}).get('kd_gain', [0])), 
                                  reverse=True)
        
        print("\nAPPROACH PERFORMANCE RANKING:")
        print("-" * 90)
        
        for i, approach in enumerate(sorted_approaches, 1):
            if approach.name in results:
                data = results[approach.name]
                avg_gain = np.mean(data['kd_gain'])
                max_gain = np.max(data['kd_gain'])
                avg_composite = np.mean(data['composite_score'])
                latency_at_max = sim.params.latency[np.argmax(data['kd_gain'])]
                
                print(f"{i}. {approach.name} ({approach.approach_type.value}):")
                print(f"   Avg KD Gain: {avg_gain:.2f}%")
                print(f"   Max KD Gain: {max_gain:.2f}%")
                print(f"   Avg Composite: {avg_composite:.3f}")
                print(f"   Latency at Max Gain: {latency_at_max:.1f} ms")
                
                if approach.parameter_history and len(approach.parameter_history) > step:
                    params = approach.parameter_history[step]
                    base_params = {'alpha': approach.base_alpha, 
                                  'beta': approach.base_beta, 
                                  'max_gain': approach.base_max_gain}
                    
                    print(f"   Parameter Adaptation:")
                    print(f"     α: {base_params['alpha']:.4f} → {params['alpha']:.4f} "
                          f"({((params['alpha']-base_params['alpha'])/base_params['alpha'])*100:+.1f}%)")
                    print(f"     β: {base_params['beta']:.4f} → {params['beta']:.4f} "
                          f"({((params['beta']-base_params['beta'])/base_params['beta'])*100:+.1f}%)")
                    print(f"     Max Gain: {base_params['max_gain']:.1f} → {params['max_gain']:.1f} "
                          f"({((params['max_gain']-base_params['max_gain'])/base_params['max_gain'])*100:+.1f}%)")
                print()
        
        # Calculate improvements over baselines
        if 'Hydra-RAN (Proposed)' in results:
            hydra_avg = np.mean(results['Hydra-RAN (Proposed)']['kd_gain'])
            print("IMPROVEMENT OVER BASELINES:")
            print("-" * 90)
            for approach in sim.approaches:
                if (approach.approach_type == ApproachType.BASELINE and 
                    approach.name in results):
                    baseline_avg = np.mean(results[approach.name]['kd_gain'])
                    improvement = ((hydra_avg - baseline_avg) / baseline_avg) * 100
                    print(f"  vs {approach.name}: {improvement:+.1f}%")
        
        print("=" * 90)
    
    @staticmethod
    def export_results(sim: DynamicSimulationFramework, filename: str):
        """Export simulation results to file."""
        import json
        import pickle
        
        export_data = {
            'parameters': {
                k: v for k, v in sim.params.__dict__.items() 
                if not k.startswith('_') and not callable(v)
            },
            'dataset': {
                k: v for k, v in sim.dataset.__dict__.items() 
                if not k.startswith('_') and not callable(v)
            } if sim.dataset else None,
            'history_summary': [
                {
                    'step': h['step'],
                    'factors': h['factors'],
                    'results': h['results']
                }
                for h in sim.history
            ],
            'approaches': [
                {
                    'name': a.name,
                    'type': a.approach_type.value,
                    'base_parameters': {
                        'alpha': a.base_alpha,
                        'beta': a.base_beta,
                        'max_gain': a.base_max_gain
                    },
                    'final_performance': (
                        sim.results.get(f'step_{sim.current_step-1}', {})
                        .get(a.name, {})
                        if sim.current_step > 0 else {}
                    )
                }
                for a in sim.approaches
            ]
        }
        
        # Save as JSON
        with open(f'{filename}.json', 'w') as f:
            json.dump(export_data, f, indent=2, default=str)
        
        # Save full simulation as pickle
        with open(f'{filename}.pkl', 'wb') as f:
            pickle.dump(sim, f)
        
        print(f"Results exported to {filename}.json and {filename}.pkl")

# ===============================
# Interactive Simulation Controller
# ===============================

class InteractiveSimulationController:
    """Controller for interactive simulation experiments."""
    
    def __init__(self):
        self.sim = DynamicSimulationFramework()
        self.visualizer = SimulationVisualizer()
        self.analyzer = PerformanceAnalyzer()
        
        # Initialize with default approaches
        approaches = create_dynamic_approaches()
        for approach in approaches:
            self.sim.add_approach(approach)
    
    def run_scenario(self, dataset_name: str = 'urban_dynamic',
                    num_steps: int = 10, step_duration: float = 1.0):
        """Run a complete simulation scenario."""
        print(f"\n{'='*90}")
        print(f"RUNNING SCENARIO: {dataset_name.upper()}")
        print(f"{'='*90}")
        
        # Load dataset
        datasets = create_dynamic_dataset_scenarios()
        self.sim.set_dataset(datasets[dataset_name])
        
        # Reset and run simulation
        self.sim.reset()
        self.sim.simulate(num_steps=num_steps, step_duration=step_duration)
        
        # Visualize results
        print(f"\nVisualizing results for step {num_steps-1}...")
        self.visualizer.plot_dynamic_performance(self.sim, step=num_steps-1)
        
        # Print summary
        self.analyzer.print_summary(self.sim, step=num_steps-1)
        
        return self.sim
    
    def run_parameter_sweep(self, param_name: str, 
                           min_val: float, max_val: float, 
                           num_points: int = 10):
        """Run parameter sweep analysis."""
        print(f"\n{'='*90}")
        print(f"PARAMETER SWEEP: {param_name.upper()}")
        print(f"{'='*90}")
        
        values = np.linspace(min_val, max_val, num_points)
        self.visualizer.plot_sensitivity_analysis(self.sim, param_name, values)
        
        return values
    
    def run_comparative_analysis(self, dataset_names: List[str]):
        """Run comparative analysis across multiple datasets."""
        print(f"\n{'='*90}")
        print("COMPARATIVE ANALYSIS ACROSS DATASETS")
        print(f"{'='*90}")
        
        results_summary = {}
        datasets = create_dynamic_dataset_scenarios()
        
        for dataset_name in dataset_names:
            print(f"\nAnalyzing {dataset_name}...")
            self.sim.set_dataset(datasets[dataset_name])
            self.sim.reset()
            self.sim.simulate(num_steps=5)
            
            # Collect average performance
            results_summary[dataset_name] = {}
            for approach in self.sim.approaches:
                if approach.performance_history:
                    avg_gains = [np.mean(step['kd_gain']) 
                               for step in approach.performance_history]
                    results_summary[dataset_name][approach.name] = {
                        'avg_gain': np.mean(avg_gains),
                        'std_gain': np.std(avg_gains),
                        'max_gain': np.max(avg_gains)
                    }
        
        # Plot comparative results
        self._plot_comparative_results(results_summary)
        
        return results_summary
    
    def _plot_comparative_results(self, results_summary: Dict):
        """Plot comparative results."""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        axes = axes.flatten()
        
        # Extract data for plotting
        dataset_names = list(results_summary.keys())
        approach_names = list(results_summary[dataset_names[0]].keys())
        
        # Plot 1: Average performance by dataset
        ax = axes[0]
        x = np.arange(len(dataset_names))
        width = 0.8 / len(approach_names)
        
        for i, approach in enumerate(approach_names):
            offsets = x + (i - len(approach_names)/2 + 0.5) * width
            values = [results_summary[ds][approach]['avg_gain'] for ds in dataset_names]
            ax.bar(offsets, values, width=width, label=approach, alpha=0.7)
        
        ax.set_xlabel('Dataset', fontsize=12)
        ax.set_ylabel('Average KD Gain (%)', fontsize=12)
        ax.set_title('Performance Across Datasets', fontsize=14)
        ax.set_xticks(x)
        ax.set_xticklabels(dataset_names, rotation=45, ha='right')
        ax.legend(fontsize=9, bbox_to_anchor=(1.05, 1))
        ax.grid(True, alpha=0.3, axis='y')
        
        # Plot 2: Performance variation
        ax = axes[1]
        variations = []
        for approach in approach_names:
            stds = [results_summary[ds][approach]['std_gain'] for ds in dataset_names]
            variations.append(np.mean(stds))
        
        bars = ax.bar(range(len(variations)), variations)
        ax.set_xticks(range(len(variations)))
        ax.set_xticklabels(approach_names, rotation=45, ha='right', fontsize=9)
        ax.set_ylabel('Performance Variation (Std Dev)', fontsize=12)
        ax.set_title('Approach Stability', fontsize=14)
        
        # Color bars by approach type
        for i, (bar, approach) in enumerate(zip(bars, approach_names)):
            if 'Hydra-RAN' in approach:
                bar.set_color('black')
            elif 'Proposed' in approach:
                bar.set_color('red')
        
        # Plot 3: Relative performance heatmap
        ax = axes[2]
        performance_matrix = np.zeros((len(dataset_names), len(approach_names)))
        for i, ds in enumerate(dataset_names):
            for j, approach in enumerate(approach_names):
                performance_matrix[i, j] = results_summary[ds][approach]['avg_gain']
        
        im = ax.imshow(performance_matrix, cmap='YlOrRd', aspect='auto')
        ax.set_xticks(range(len(approach_names)))
        ax.set_xticklabels(approach_names, rotation=45, ha='right', fontsize=8)
        ax.set_yticks(range(len(dataset_names)))
        ax.set_yticklabels(dataset_names, fontsize=9)
        ax.set_title('Performance Heatmap', fontsize=14)
        plt.colorbar(im, ax=ax, label='KD Gain (%)')
        
        # Plot 4: Improvement over best baseline
        ax = axes[3]
        improvements = []
        for ds in dataset_names:
            baseline_max = max([results_summary[ds][a]['avg_gain'] 
                              for a in approach_names 
                              if 'Hydra-RAN' not in a and 'Proposed' not in a])
            hydra_perf = results_summary[ds]['Hydra-RAN (Proposed)']['avg_gain']
            improvements.append(((hydra_perf - baseline_max) / baseline_max) * 100)
        
        bars = ax.bar(range(len(improvements)), improvements, color=['green' if x > 0 else 'red' for x in improvements])
        ax.set_xticks(range(len(improvements)))
        ax.set_xticklabels(dataset_names, rotation=45, ha='right')
        ax.set_ylabel('Improvement Over Best Baseline (%)', fontsize=12)
        ax.set_title('Hydra-RAN Performance Improvement', fontsize=14)
        ax.grid(True, alpha=0.3, axis='y')
        
        # Add value labels
        for bar, val in zip(bars, improvements):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{val:+.1f}%', ha='center', va='bottom' if val > 0 else 'top')
        
        plt.suptitle('Comparative Performance Analysis', fontsize=16)
        plt.tight_layout()
        plt.show()

# ===============================
# Main Execution
# ===============================

def main():
    """Main execution function."""
    print("Initializing Dynamic Simulation Framework...")
    print("=" * 90)
    
    # Create interactive controller
    controller = InteractiveSimulationController()
    
    # Run main scenario
    print("\n1. Running Urban Dynamic Scenario...")
    sim = controller.run_scenario('urban_dynamic', num_steps=8)
    
    # Run parameter sensitivity analysis
    print("\n\n2. Running Parameter Sensitivity Analysis...")
    controller.run_parameter_sweep('bandwidth', 20, 200, 8)
    controller.run_parameter_sweep('vehicle_density', 10, 150, 8)
    
    # Run comparative analysis
    print("\n\n3. Running Comparative Analysis...")
    controller.run_comparative_analysis(['urban_dynamic', 'highway_stable', 'dense_urban'])
    
    # Demonstrate dynamic modifications
    print("\n\n4. Demonstrating Dynamic Modifications...")
    print("-" * 90)
    
    # Modify parameters and show effect
    print("\na) Increasing network congestion from 0.3 to 0.8...")
    controller.sim.update_parameters(network_congestion=0.8)
    controller.sim.reset()
    controller.sim.simulate(num_steps=3)
    controller.analyzer.print_summary(controller.sim)
    
    print("\nb) Modifying dataset complexity from 0.7 to 0.9...")
    controller.sim.modify_dataset(complexity=0.9)
    controller.sim.reset()
    controller.sim.simulate(num_steps=3)
    controller.analyzer.print_summary(controller.sim)
    
    print("\nc) Changing latency range from 10-120ms to 20-200ms...")
    controller.sim.update_parameters(latency_min=20, latency_max=200)
    controller.sim.reset()
    controller.sim.simulate(num_steps=3)
    controller.visualizer.plot_dynamic_performance(controller.sim)
    
    # Export results
    print("\n\n5. Exporting Results...")
    controller.analyzer.export_results(controller.sim, 'simulation_results')
    
    print("\n" + "=" * 90)
    print("SIMULATION COMPLETE")
    print("=" * 90)
    
    return controller.sim

def quick_demo():
    """Quick demonstration of dynamic behavior."""
    print("QUICK DEMONSTRATION OF DYNAMIC SIMULATION")
    print("=" * 90)
    
    # Create simulation
    sim = DynamicSimulationFramework()
    datasets = create_dynamic_dataset_scenarios()
    
    # Add approaches
    approaches = create_dynamic_approaches()
    for approach in approaches:
        sim.add_approach(approach)
    
    # Test 1: Baseline performance
    print("\nTest 1: Baseline performance with urban dataset")
    sim.set_dataset(datasets['urban_dynamic'])
    sim.simulate(num_steps=5)
    PerformanceAnalyzer.print_summary(sim)
    
    # Test 2: Change to noisy dataset
    print("\n\nTest 2: Switching to noisy dense urban dataset")
    sim.set_dataset(datasets['dense_urban'])
    sim.reset()
    sim.simulate(num_steps=5)
    PerformanceAnalyzer.print_summary(sim)
    
    # Test 3: Modify system parameters
    print("\n\nTest 3: Increasing bandwidth and reducing congestion")
    sim.update_parameters(bandwidth=150, network_congestion=0.1)
    sim.reset()
    sim.simulate(num_steps=5)
    PerformanceAnalyzer.print_summary(sim)
    
    return sim

# ===============================
# Execution Entry Points
# ===============================

if __name__ == "__main__":
    # Run full simulation
    simulation = main()
    
    # Uncomment for quick demo
    # simulation = quick_demo()
    
    # Uncomment for specific experiments
    # controller = InteractiveSimulationController()
    # controller.run_scenario('highway_stable', num_steps=10)



# Control signaling load versus update threshold $\delta$ for various V2X frameworks. Hydra-RAN achieves significantly lower overhead across all thresholds through belief-driven event triggering.

# -*- coding: utf-8 -*-
"""
Dynamic Adaptive Control Signaling Load Simulation Framework
Performance metrics adapt in real-time to parameter changes

Author: Rafid I. Abd et al. (Enhanced for real-time parameter sensitivity)
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Callable, Any, Tuple, Optional
from dataclasses import dataclass, field
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

# ===============================
# IEEE TCOM Style Configuration
# ===============================
plt.rcParams.update({
    "font.family": "Times New Roman",
    "axes.linewidth": 1.4,
    "axes.edgecolor": "0.2",
    "figure.figsize": (14, 10),
    "font.size": 12
})

# ===============================
# Core Data Structures
# ===============================

class ApproachType(Enum):
    """Type of approach for categorization."""
    BASELINE = "baseline"
    PROPOSED = "proposed"
    HYBRID = "hybrid"

@dataclass
class DynamicUncertaintyProfile:
    """Dynamic uncertainty profile that changes over time."""
    name: str
    base_mean: float
    base_std: float
    temporal_variation: float = 0.1
    spatial_variation: float = 0.05
    drift_rate: float = 0.01
    correlation_factor: float = 0.3
    
    def __post_init__(self):
        """Initialize dynamic properties."""
        self.current_mean = self.base_mean
        self.current_std = self.base_std
        self.time_step = 0
        self.history = []
        
    def evolve(self, step: int = 1, external_factor: float = 1.0):
        """Evolve uncertainty profile over time."""
        self.time_step += step
        
        # Temporal variation
        temporal_change = self.temporal_variation * np.sin(self.time_step / 10)
        
        # Random drift
        drift = self.drift_rate * np.random.randn()
        
        # Update parameters
        self.current_mean = np.clip(
            self.base_mean * external_factor + temporal_change + drift,
            0.1, 0.9
        )
        
        # Vary standard deviation
        self.current_std = np.clip(
            self.base_std * (1 + 0.2 * np.sin(self.time_step / 5)),
            0.05, 0.25
        )
        
        # Record history
        self.history.append({
            'step': self.time_step,
            'mean': self.current_mean,
            'std': self.current_std,
            'external_factor': external_factor
        })

@dataclass
class DynamicNetworkParameters:
    """Dynamic network parameters that change during simulation."""
    bandwidth: float = 100.0  # MHz
    latency: float = 50.0  # ms
    packet_loss: float = 0.05
    congestion_level: float = 0.3
    channel_quality: float = 0.8
    mobility_speed: float = 60.0  # km/h
    
    def update(self, time_step: int, vehicle_density: float):
        """Update network parameters dynamically."""
        # Time-varying effects
        time_variation = 0.1 * np.sin(time_step / 20)
        
        # Congestion effect based on vehicle density
        congestion_effect = 0.3 * (vehicle_density / 100)
        
        # Update parameters
        self.congestion_level = np.clip(
            0.2 + 0.6 * (vehicle_density / 150) + time_variation,
            0.1, 0.9
        )
        
        self.latency = 10 + 40 * self.congestion_level + 5 * np.random.randn()
        self.packet_loss = np.clip(0.01 + 0.1 * self.congestion_level, 0, 0.2)
        self.channel_quality = np.clip(
            0.9 - 0.4 * self.congestion_level - 0.1 * np.random.randn(),
            0.3, 0.95
        )

@dataclass
class DynamicSimulationParameters:
    """Centralized parameter configuration with dynamic capabilities."""
    # Core simulation parameters
    T: int = 10000                    # Total time slots
    delta_range: Tuple[float, float, int] = (0, 1, 20)
    seed: int = 42
    
    # Dynamic environment parameters
    vehicle_density: float = 50.0  # vehicles/km²
    rsu_density: float = 5.0  # RSUs/km
    data_rate: float = 100.0  # Mbps
    computation_capacity: float = 10.0  # TFLOPS
    energy_budget: float = 100.0  # Wh
    
    # Uncertainty model configurations
    uncertainty_models: Dict = field(default_factory=lambda: {
        'normal': lambda size, mean, std: np.clip(np.random.normal(mean, std, size), 0, 1),
        'uniform': lambda size, mean, std: np.random.uniform(max(0, mean-std), min(1, mean+std), size),
        'beta': lambda size, mean, std: np.random.beta(
            alpha=max(0.1, mean*10), 
            beta=max(0.1, (1-mean)*10), 
            size=size
        ),
        'time_correlated': lambda size, mean, std: np.clip(
            mean + std * np.cumsum(np.random.randn(size)) * 0.1, 0, 1
        ),
        'bursty': lambda size, mean, std: np.clip(
            mean + std * np.random.poisson(1, size) * 0.3, 0, 1
        )
    })
    
    # Approach-specific dynamic profiles
    approach_profiles: Dict = field(default_factory=lambda: {
        'Cooperative_Perception': {
            'type': ApproachType.BASELINE,
            'uncertainty_profile': DynamicUncertaintyProfile(
                'Cooperative_Perception', 0.65, 0.15, 0.12, 0.08, 0.015
            ),
            'adaptation_rate': 0.1,
            'load_sensitivity': 1.2,
            'metadata': {'model_type': 'raw_exchange', 'features': ['basic']}
        },
        'V2X_ViTv2': {
            'type': ApproachType.BASELINE,
            'uncertainty_profile': DynamicUncertaintyProfile(
                'V2X_ViTv2', 0.70, 0.12, 0.10, 0.06, 0.012
            ),
            'adaptation_rate': 0.15,
            'load_sensitivity': 1.1,
            'metadata': {'model_type': 'transformer', 'features': ['attention']}
        },
        'Semantic_Comm': {
            'type': ApproachType.BASELINE,
            'uncertainty_profile': DynamicUncertaintyProfile(
                'Semantic_Comm', 0.50, 0.15, 0.15, 0.10, 0.020
            ),
            'adaptation_rate': 0.18,
            'load_sensitivity': 0.9,
            'metadata': {'model_type': 'semantic', 'features': ['semantic']}
        },
        'Task_Oriented': {
            'type': ApproachType.BASELINE,
            'uncertainty_profile': DynamicUncertaintyProfile(
                'Task_Oriented', 0.45, 0.12, 0.08, 0.05, 0.010
            ),
            'adaptation_rate': 0.2,
            'load_sensitivity': 0.8,
            'metadata': {'model_type': 'task_aware', 'features': ['task']}
        },
        'Digital_Twin': {
            'type': ApproachType.BASELINE,
            'uncertainty_profile': DynamicUncertaintyProfile(
                'Digital_Twin', 0.75, 0.10, 0.05, 0.03, 0.008
            ),
            'adaptation_rate': 0.17,
            'load_sensitivity': 1.3,
            'metadata': {'model_type': 'digital_twin', 'features': ['simulation']}
        },
        'Edge_Collaborative': {
            'type': ApproachType.BASELINE,
            'uncertainty_profile': DynamicUncertaintyProfile(
                'Edge_Collaborative', 0.55, 0.12, 0.10, 0.07, 0.014
            ),
            'adaptation_rate': 0.22,
            'load_sensitivity': 1.0,
            'metadata': {'model_type': 'edge_drl', 'features': ['distributed']}
        },
        'FedRSU': {
            'type': ApproachType.BASELINE,
            'uncertainty_profile': DynamicUncertaintyProfile(
                'FedRSU', 0.60, 0.10, 0.07, 0.04, 0.011
            ),
            'adaptation_rate': 0.19,
            'load_sensitivity': 0.7,
            'metadata': {'model_type': 'federated', 'features': ['privacy']}
        },
        'Hydra_RAN': {
            'type': ApproachType.PROPOSED,
            'uncertainty_profile': DynamicUncertaintyProfile(
                'Hydra_RAN', 0.35, 0.10, 0.03, 0.02, 0.005
            ),
            'adaptation_rate': 0.25,
            'load_sensitivity': 0.5,
            'metadata': {
                'model_type': 'hierarchical_kd',
                'features': ['goal_aware', 'belief_driven', 'adaptive', 'hierarchical']
            }
        }
    })
    
    # Dynamic load calculation parameters
    load_calculation: Dict = field(default_factory=lambda: {
        'normalization': 'adaptive',  # Options: 'max_updates', 'T', 'adaptive'
        'load_function': 'dynamic_threshold',  # Options: 'threshold_based', 'weighted', 'dynamic_threshold'
        'weighting_scheme': 'exponential_decay',
        'decay_factor': 0.99,
        'adaptive_threshold': True,
        'threshold_sensitivity': 0.1
    })
    
    # Performance metrics weights
    performance_weights: Dict = field(default_factory=lambda: {
        'mean_load': 0.4,
        'load_variability': 0.2,
        'efficiency': 0.3,
        'robustness': 0.1
    })
    
    # Plot styling with dynamic color maps
    plot_style: Dict = field(default_factory=lambda: {
        'figure_facecolor': '#EAEAF2',
        'axes_facecolor': '#EAEAF2',
        'grid_style': {'linestyle': '--', 'linewidth': 1.2, 'alpha': 0.6, 'color': 'gray'},
        'font_sizes': {'title': 38, 'labels': 34, 'ticks': 30, 'legend': 28},
        'line_widths': {'baseline': 2.5, 'proposed': 5, 'hybrid': 3},
        'color_palette': {
            'proposed': ['#000000', '#333333', '#666666'],
            'baseline': ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2'],
            'hybrid': ['#17becf', '#bcbd22']
        },
        'markers': ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h']
    })
    
    def __post_init__(self):
        """Initialize derived parameters."""
        np.random.seed(self.seed)
        self.delta = np.linspace(*self.delta_range)
        self.network_params = DynamicNetworkParameters()
        
    def update_environment(self, time_step: int):
        """Update all environment parameters dynamically."""
        # Update network parameters
        self.network_params.update(time_step, self.vehicle_density)
        
        # Update approach profiles
        for profile in self.approach_profiles.values():
            external_factor = 1.0 - 0.3 * self.network_params.congestion_level
            profile['uncertainty_profile'].evolve(1, external_factor)
        
        # Update load calculation based on network conditions
        if self.load_calculation['adaptive_threshold']:
            self.load_calculation['threshold_sensitivity'] = np.clip(
                0.05 + 0.1 * self.network_params.congestion_level,
                0.05, 0.2
            )

# ===============================
# Dynamic Simulation Framework
# ===============================

class DynamicAdaptiveSimulator:
    """Simulator with real-time adaptive performance metrics."""
    
    def __init__(self, params: DynamicSimulationParameters = None):
        """Initialize simulator with dynamic parameters."""
        self.params = params if params else DynamicSimulationParameters()
        self.delta = self.params.delta.copy()
        
        # Dynamic storage
        self.uncertainty_data: Dict[str, List[np.ndarray]] = {}
        self.signaling_loads: Dict[str, List[np.ndarray]] = {}
        self.performance_metrics: Dict[str, List[Dict]] = {}
        self.environment_history: List[Dict] = []
        self.current_step = 0
        
        # Initialize approach performance trackers
        for approach_name in self.params.approach_profiles.keys():
            self.uncertainty_data[approach_name] = []
            self.signaling_loads[approach_name] = []
            self.performance_metrics[approach_name] = []
    
    def generate_dynamic_uncertainty(self, approach_name: str, size: int = None) -> np.ndarray:
        """Generate uncertainty with dynamic characteristics."""
        if size is None:
            size = min(1000, self.params.T // 10)
        
        profile = self.params.approach_profiles[approach_name]
        uncertainty_profile = profile['uncertainty_profile']
        
        # Get current uncertainty parameters
        mean = uncertainty_profile.current_mean
        std = uncertainty_profile.current_std
        adaptation_rate = profile['adaptation_rate']
        
        # Select distribution based on approach type
        if profile['type'] == ApproachType.PROPOSED:
            # Hydra-RAN: Beta distribution for bounded uncertainty
            uncertainty = np.random.beta(
                alpha=max(0.1, mean*10 * (1 - 0.3*self.params.network_params.congestion_level)),
                beta=max(0.1, (1-mean)*10 * (1 + 0.2*self.params.network_params.congestion_level)),
                size=size
            )
        elif 'semantic' in profile['metadata'].get('model_type', ''):
            # Semantic approaches: Normal with correlation
            base = np.random.normal(mean, std, size)
            # Add temporal correlation
            for i in range(1, size):
                base[i] = adaptation_rate * base[i] + (1 - adaptation_rate) * base[i-1]
            uncertainty = np.clip(base, 0, 1)
        else:
            # Other baselines: Normal distribution
            uncertainty = np.clip(np.random.normal(mean, std, size), 0, 1)
        
        # Apply network effects
        packet_loss_effect = 1.0 - 0.3 * self.params.network_params.packet_loss
        channel_effect = 0.7 + 0.3 * self.params.network_params.channel_quality
        
        uncertainty = uncertainty * packet_loss_effect * channel_effect
        
        # Add burstiness for certain approaches
        if 'edge' in profile['metadata'].get('model_type', ''):
            burst_mask = np.random.poisson(0.1, size) > 0
            uncertainty[burst_mask] *= 1.5
        
        return np.clip(uncertainty, 0, 1)
    
    def calculate_dynamic_signaling_load(self, uncertainty: np.ndarray, 
                                       approach_name: str,
                                       delta_values: np.ndarray = None) -> np.ndarray:
        """Calculate signaling load with dynamic threshold adaptation."""
        if delta_values is None:
            delta_values = self.delta
        
        profile = self.params.approach_profiles[approach_name]
        load_function = self.params.load_calculation['load_function']
        network_params = self.params.network_params
        
        # Determine normalization factor dynamically
        if self.params.load_calculation['normalization'] == 'adaptive':
            # Adaptive normalization based on network conditions
            congestion_factor = 1 + 0.5 * network_params.congestion_level
            channel_factor = 2 - network_params.channel_quality
            normalization = len(uncertainty) * congestion_factor * channel_factor
        elif self.params.load_calculation['normalization'] == 'T':
            normalization = self.params.T
        else:
            normalization = len(uncertainty)
        
        # Calculate loads based on selected function
        loads = []
        
        for d in delta_values:
            # Dynamic threshold adjustment
            if self.params.load_calculation['adaptive_threshold']:
                threshold_sensitivity = self.params.load_calculation['threshold_sensitivity']
                network_adjusted_d = d * (1 + threshold_sensitivity * network_params.congestion_level)
            else:
                network_adjusted_d = d
            
            if load_function == 'threshold_based':
                # Basic threshold-based calculation
                load = np.sum(uncertainty > network_adjusted_d) / normalization * 100
                
            elif load_function == 'weighted':
                # Weighted by uncertainty magnitude
                exceed_mask = uncertainty > network_adjusted_d
                weights = (uncertainty[exceed_mask] - network_adjusted_d)
                load = np.sum(weights) / normalization * 100
                
            elif load_function == 'dynamic_threshold':
                # Dynamic threshold with memory
                exceed_mask = uncertainty > network_adjusted_d
                
                # Apply temporal weighting
                if self.params.load_calculation['weighting_scheme'] == 'exponential_decay':
                    decay = self.params.load_calculation['decay_factor']
                    time_weights = decay ** np.arange(len(uncertainty))[::-1]
                    weighted_exceeds = exceed_mask * time_weights
                    load = np.sum(weighted_exceeds) / np.sum(time_weights) * 100
                else:
                    load = np.mean(exceed_mask) * 100
                    
                # Apply approach-specific sensitivity
                load_sensitivity = profile['load_sensitivity']
                load *= load_sensitivity
                
            else:
                # Default to threshold-based
                load = np.mean(uncertainty > network_adjusted_d) * 100
            
            # Apply network effects
            latency_penalty = 1 + 0.01 * network_params.latency
            bandwidth_boost = 1 - 0.2 * (network_params.bandwidth / 200)
            load = load * latency_penalty * bandwidth_boost
            
            loads.append(load)
        
        return np.array(loads)
    
    def simulate_step(self) -> Dict:
        """Simulate a single time step with current parameters."""
        step_results = {}
        
        # Update environment
        self.params.update_environment(self.current_step)
        
        # Record environment state
        self.environment_history.append({
            'step': self.current_step,
            'network': {
                'congestion': self.params.network_params.congestion_level,
                'latency': self.params.network_params.latency,
                'packet_loss': self.params.network_params.packet_loss,
                'channel_quality': self.params.network_params.channel_quality
            },
            'vehicle_density': self.params.vehicle_density
        })
        
        # Simulate each approach
        for approach_name, profile in self.params.approach_profiles.items():
            # Generate uncertainty for this step
            uncertainty = self.generate_dynamic_uncertainty(approach_name)
            self.uncertainty_data[approach_name].append(uncertainty)
            
            # Calculate signaling load
            loads = self.calculate_dynamic_signaling_load(uncertainty, approach_name)
            self.signaling_loads[approach_name].append(loads)
            
            # Calculate performance metrics
            metrics = self._calculate_step_metrics(approach_name, loads, uncertainty)
            self.performance_metrics[approach_name].append(metrics)
            
            step_results[approach_name] = {
                'loads': loads.copy(),
                'metrics': metrics.copy(),
                'uncertainty_stats': {
                    'mean': np.mean(uncertainty),
                    'std': np.std(uncertainty),
                    'min': np.min(uncertainty),
                    'max': np.max(uncertainty)
                }
            }
        
        self.current_step += 1
        return step_results
    
    def simulate(self, num_steps: int = 10) -> Dict:
        """Run multiple simulation steps."""
        all_results = {}
        
        for step in range(num_steps):
            step_results = self.simulate_step()
            all_results[f'step_{step}'] = step_results
        
        return all_results
    
    def _calculate_step_metrics(self, approach_name: str, loads: np.ndarray, 
                              uncertainty: np.ndarray) -> Dict:
        """Calculate comprehensive performance metrics for a step."""
        profile = self.params.approach_profiles[approach_name]
        weights = self.params.performance_weights
        
        # Basic statistics
        mean_load = np.mean(loads)
        std_load = np.std(loads)
        max_load = np.max(loads)
        min_load = np.min(loads)
        
        # Area under curve (normalized)
        auc = np.trapz(loads, self.delta) / (self.delta[-1] - self.delta[0])
        
        # Threshold sensitivity
        sensitivity = np.mean(np.abs(np.gradient(loads, self.delta)))
        
        # Uncertainty statistics
        uncertainty_mean = np.mean(uncertainty)
        uncertainty_std = np.std(uncertainty)
        
        # Efficiency score (lower is better)
        base_efficiency = mean_load * 0.6 + std_load * 0.2 + sensitivity * 0.2
        
        # Apply network effects
        network_penalty = 1 + 0.3 * self.params.network_params.congestion_level
        channel_boost = 1 - 0.2 * (1 - self.params.network_params.channel_quality)
        
        efficiency_score = base_efficiency * network_penalty * channel_boost
        
        # Robustness metric
        robustness = 1.0 / (std_load + 0.1)
        
        # Composite score
        composite_score = (
            weights['mean_load'] * (100 - mean_load) / 100 +
            weights['load_variability'] * (1.0 / (std_load + 0.01)) +
            weights['efficiency'] * (1.0 / (efficiency_score + 0.01)) +
            weights['robustness'] * robustness
        )
        
        return {
            'mean_load': float(mean_load),
            'std_load': float(std_load),
            'max_load': float(max_load),
            'min_load': float(min_load),
            'area_under_curve': float(auc),
            'threshold_sensitivity': float(sensitivity),
            'efficiency_score': float(efficiency_score),
            'robustness': float(robustness),
            'composite_score': float(composite_score),
            'uncertainty_mean': float(uncertainty_mean),
            'uncertainty_std': float(uncertainty_std)
        }
    
    def modify_parameter(self, param_path: str, value: Any):
        """Modify any parameter in the simulation."""
        parts = param_path.split('.')
        obj = self.params
        
        # Navigate to the target object
        for part in parts[:-1]:
            if hasattr(obj, part):
                obj = getattr(obj, part)
            elif isinstance(obj, dict) and part in obj:
                obj = obj[part]
            else:
                raise AttributeError(f"Cannot find {part} in {obj}")
        
        # Set the value
        final_part = parts[-1]
        if hasattr(obj, final_part):
            setattr(obj, final_part, value)
        elif isinstance(obj, dict) and final_part in obj:
            obj[final_part] = value
        else:
            raise AttributeError(f"Cannot set {final_part} in {obj}")
        
        print(f"Modified {param_path} = {value}")
    
    def get_current_performance_summary(self, step: int = -1) -> Dict:
        """Get performance summary for a specific step."""
        if step == -1:
            step = self.current_step - 1
        
        summary = {}
        
        for approach_name in self.params.approach_profiles.keys():
            if step < len(self.performance_metrics[approach_name]):
                metrics = self.performance_metrics[approach_name][step]
                summary[approach_name] = metrics
        
        return summary
    
    def run_sensitivity_analysis(self, param_name: str, 
                                value_range: np.ndarray,
                                num_steps_per_value: int = 3) -> Dict:
        """Run sensitivity analysis for a parameter."""
        original_values = {}
        
        # Store original values for all approaches if parameter exists in profiles
        for approach_name, profile in self.params.approach_profiles.items():
            if param_name in profile['uncertainty_profile'].__dict__:
                original_values[approach_name] = getattr(
                    profile['uncertainty_profile'], param_name
                )
        
        sensitivity_results = {}
        
        for value in value_range:
            print(f"Testing {param_name} = {value:.3f}")
            
            # Set parameter for all approaches
            for approach_name in original_values.keys():
                setattr(
                    self.params.approach_profiles[approach_name]['uncertainty_profile'],
                    param_name, value
                )
            
            # Reset and run simulation
            self.reset_state()
            results = self.simulate(num_steps=num_steps_per_value)
            
            # Collect average performance
            avg_metrics = {}
            for approach_name in self.params.approach_profiles.keys():
                if self.performance_metrics[approach_name]:
                    step_metrics = self.performance_metrics[approach_name][-1]
                    avg_metrics[approach_name] = {
                        'mean_load': step_metrics['mean_load'],
                        'efficiency_score': step_metrics['efficiency_score'],
                        'composite_score': step_metrics['composite_score']
                    }
            
            sensitivity_results[value] = {
                'results': results,
                'avg_metrics': avg_metrics
            }
        
        # Restore original values
        for approach_name, original_value in original_values.items():
            setattr(
                self.params.approach_profiles[approach_name]['uncertainty_profile'],
                param_name, original_value
            )
        
        return sensitivity_results
    
    def reset_state(self):
        """Reset simulation state while keeping parameters."""
        self.current_step = 0
        self.environment_history.clear()
        
        for approach_name in self.params.approach_profiles.keys():
            self.uncertainty_data[approach_name].clear()
            self.signaling_loads[approach_name].clear()
            self.performance_metrics[approach_name].clear()
        
        # Reset approach profiles
        for profile in self.params.approach_profiles.values():
            profile['uncertainty_profile'].time_step = 0
            profile['uncertainty_profile'].current_mean = profile['uncertainty_profile'].base_mean
            profile['uncertainty_profile'].current_std = profile['uncertainty_profile'].base_std
            profile['uncertainty_profile'].history.clear()

# ===============================
# Dynamic Visualization System
# ===============================

class DynamicVisualizationSystem:
    """System for visualizing dynamic simulation results."""
    
    @staticmethod
    def plot_dynamic_performance(simulator: DynamicAdaptiveSimulator, 
                                step: int = -1,
                                figsize: Tuple[int, int] = (16, 12)):
        """Plot dynamic performance results for a specific step."""
        if step == -1:
            step = simulator.current_step - 1
        
        # Get data for the step
        if step >= simulator.current_step:
            print(f"No data for step {step}. Current step: {simulator.current_step-1}")
            return
        
        # Get approach results
        approach_names = list(simulator.params.approach_profiles.keys())
        
        # Create figure with multiple subplots
        fig = plt.figure(figsize=figsize)
        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
        
        # Plot 1: Signaling Load vs Threshold
        ax1 = fig.add_subplot(gs[0:2, 0:2])
        DynamicVisualizationSystem._plot_signaling_loads(
            ax1, simulator, step, approach_names
        )
        
        # Plot 2: Performance Evolution
        ax2 = fig.add_subplot(gs[0, 2])
        DynamicVisualizationSystem._plot_performance_evolution(
            ax2, simulator, approach_names
        )
        
        # Plot 3: Environment State
        ax3 = fig.add_subplot(gs[1, 2])
        DynamicVisualizationSystem._plot_environment_state(
            ax3, simulator
        )
        
        # Plot 4: Performance Distribution
        ax4 = fig.add_subplot(gs[2, 0])
        DynamicVisualizationSystem._plot_performance_distribution(
            ax4, simulator, step, approach_names
        )
        
        # Plot 5: Parameter Adaptation
        ax5 = fig.add_subplot(gs[2, 1])
        DynamicVisualizationSystem._plot_parameter_adaptation(
            ax5, simulator, approach_names
        )
        
        # Plot 6: Network Effects
        ax6 = fig.add_subplot(gs[2, 2])
        DynamicVisualizationSystem._plot_network_effects(
            ax6, simulator, approach_names
        )
        
        plt.suptitle(f'Dynamic Performance Evaluation - Step {step}', fontsize=20)
        plt.tight_layout()
        plt.show()
    
    @staticmethod
    def _plot_signaling_loads(ax, simulator, step, approach_names):
        """Plot signaling loads for all approaches."""
        style = simulator.params.plot_style
        colors = style['color_palette']
        
        # Get plot styles for each approach
        for i, approach_name in enumerate(approach_names):
            profile = simulator.params.approach_profiles[approach_name]
            approach_type = profile['type']
            
            # Get data
            if step < len(simulator.signaling_loads[approach_name]):
                loads = simulator.signaling_loads[approach_name][step]
                
                # Determine style
                if approach_type == ApproachType.PROPOSED:
                    color = colors['proposed'][0]
                    linewidth = style['line_widths']['proposed']
                    linestyle = '-'
                    marker = ''
                    label = approach_name.replace('_', ' ')
                elif approach_type == ApproachType.HYBRID:
                    color = colors['hybrid'][0]
                    linewidth = style['line_widths']['hybrid']
                    linestyle = '--'
                    marker = style['markers'][i % len(style['markers'])]
                    label = approach_name.replace('_', ' ')
                else:
                    color_idx = i % len(colors['baseline'])
                    color = colors['baseline'][color_idx]
                    linewidth = style['line_widths']['baseline']
                    linestyle = '-'
                    marker = style['markers'][i % len(style['markers'])]
                    label = approach_name.replace('_', ' ')
                
                # Plot
                ax.plot(simulator.delta, loads, 
                       color=color, linewidth=linewidth,
                       linestyle=linestyle, marker=marker,
                       markersize=8, label=label)
        
        ax.set_xlabel('Update Threshold $\\delta$', fontsize=style['font_sizes']['labels'])
        ax.set_ylabel('Control Signaling Load (%)', fontsize=style['font_sizes']['labels'])
        ax.set_title('Signaling Load vs Threshold', fontsize=style['font_sizes']['labels']*0.8)
        ax.grid(**style['grid_style'])
        ax.legend(fontsize=style['font_sizes']['legend']*0.7, 
                 loc='upper right', ncol=2)
    
    @staticmethod
    def _plot_performance_evolution(ax, simulator, approach_names):
        """Plot performance evolution over time."""
        style = simulator.params.plot_style
        
        for i, approach_name in enumerate(approach_names):
            profile = simulator.params.approach_profiles[approach_name]
            approach_type = profile['type']
            
            # Get performance history
            if simulator.performance_metrics[approach_name]:
                composite_scores = [
                    m['composite_score'] 
                    for m in simulator.performance_metrics[approach_name]
                ]
                
                steps = range(len(composite_scores))
                
                # Determine style
                if approach_type == ApproachType.PROPOSED:
                    color = 'k'
                    linewidth = 3
                else:
                    color = simulator.params.plot_style['color_palette']['baseline'][i % 7]
                    linewidth = 1.5
                
                ax.plot(steps, composite_scores, color=color, 
                       linewidth=linewidth, label=approach_name.replace('_', ' '))
        
        ax.set_xlabel('Simulation Step', fontsize=style['font_sizes']['ticks'])
        ax.set_ylabel('Composite Score', fontsize=style['font_sizes']['ticks'])
        ax.set_title('Performance Evolution', fontsize=style['font_sizes']['labels']*0.8)
        ax.grid(**style['grid_style'])
        ax.legend(fontsize=style['font_sizes']['legend']*0.6)
    
    @staticmethod
    def _plot_environment_state(ax, simulator):
        """Plot environment state evolution."""
        if not simulator.environment_history:
            return
        
        style = simulator.params.plot_style
        
        steps = [h['step'] for h in simulator.environment_history]
        congestion = [h['network']['congestion'] for h in simulator.environment_history]
        latency = [h['network']['latency'] for h in simulator.environment_history]
        channel_quality = [h['network']['channel_quality'] for h in simulator.environment_history]
        
        ax.plot(steps, congestion, 'r-', linewidth=2, label='Congestion')
        ax.plot(steps, np.array(latency)/100, 'b-', linewidth=2, label='Latency (norm)')
        ax.plot(steps, channel_quality, 'g-', linewidth=2, label='Channel Quality')
        
        ax.set_xlabel('Simulation Step', fontsize=style['font_sizes']['ticks'])
        ax.set_ylabel('Normalized Value', fontsize=style['font_sizes']['ticks'])
        ax.set_title('Environment State', fontsize=style['font_sizes']['labels']*0.8)
        ax.legend(fontsize=style['font_sizes']['legend']*0.6)
        ax.grid(**style['grid_style'])
    
    @staticmethod
    def _plot_performance_distribution(ax, simulator, step, approach_names):
        """Plot performance distribution for current step."""
        style = simulator.params.plot_style
        
        mean_loads = []
        approach_labels = []
        
        for approach_name in approach_names:
            if step < len(simulator.performance_metrics[approach_name]):
                metrics = simulator.performance_metrics[approach_name][step]
                mean_loads.append(metrics['mean_load'])
                approach_labels.append(approach_name.replace('_', ' '))
        
        # Create bar chart
        bars = ax.bar(range(len(mean_loads)), mean_loads, alpha=0.7)
        
        # Color bars by approach type
        for i, (bar, approach_name) in enumerate(zip(bars, approach_names)):
            profile = simulator.params.approach_profiles[approach_name]
            if profile['type'] == ApproachType.PROPOSED:
                bar.set_color('k')
            elif profile['type'] == ApproachType.HYBRID:
                bar.set_color('purple')
            else:
                bar.set_color(style['color_palette']['baseline'][i % 7])
        
        ax.set_xticks(range(len(approach_labels)))
        ax.set_xticklabels(approach_labels, rotation=45, ha='right', 
                          fontsize=style['font_sizes']['ticks']*0.7)
        ax.set_ylabel('Mean Load (%)', fontsize=style['font_sizes']['ticks'])
        ax.set_title('Performance Distribution', fontsize=style['font_sizes']['labels']*0.8)
        ax.grid(True, alpha=0.3, axis='y')
    
    @staticmethod
    def _plot_parameter_adaptation(ax, simulator, approach_names):
        """Plot parameter adaptation over time."""
        style = simulator.params.plot_style
        
        for i, approach_name in enumerate(approach_names[:3]):  # Plot first 3 for clarity
            profile = simulator.params.approach_profiles[approach_name]
            
            if profile['uncertainty_profile'].history:
                steps = [h['step'] for h in profile['uncertainty_profile'].history]
                means = [h['mean'] for h in profile['uncertainty_profile'].history]
                
                ax.plot(steps, means, label=approach_name.replace('_', ' '),
                       linewidth=1.5, alpha=0.7)
        
        ax.set_xlabel('Time Step', fontsize=style['font_sizes']['ticks'])
        ax.set_ylabel('Uncertainty Mean', fontsize=style['font_sizes']['ticks'])
        ax.set_title('Parameter Adaptation', fontsize=style['font_sizes']['labels']*0.8)
        ax.legend(fontsize=style['font_sizes']['legend']*0.6)
        ax.grid(**style['grid_style'])
    
    @staticmethod
    def _plot_network_effects(ax, simulator, approach_names):
        """Plot network effects on performance."""
        if not simulator.environment_history:
            return
        
        style = simulator.params.plot_style
        
        # Get correlation between congestion and performance
        steps = min(20, len(simulator.environment_history))
        
        if steps > 1:
            congestion = []
            hydra_performance = []
            best_baseline_performance = []
            
            for i in range(steps):
                env = simulator.environment_history[i]
                congestion.append(env['network']['congestion'])
                
                # Get Hydra-RAN performance
                if i < len(simulator.performance_metrics['Hydra_RAN']):
                    hydra_performance.append(
                        simulator.performance_metrics['Hydra_RAN'][i]['composite_score']
                    )
                
                # Get best baseline performance
                baseline_scores = []
                for approach_name in approach_names:
                    profile = simulator.params.approach_profiles[approach_name]
                    if (profile['type'] == ApproachType.BASELINE and 
                        i < len(simulator.performance_metrics[approach_name])):
                        baseline_scores.append(
                            simulator.performance_metrics[approach_name][i]['composite_score']
                        )
                
                if baseline_scores:
                    best_baseline_performance.append(max(baseline_scores))
            
            if hydra_performance and best_baseline_performance:
                ax.scatter(congestion[:len(hydra_performance)], hydra_performance,
                          c='k', s=50, label='Hydra-RAN', alpha=0.7)
                ax.scatter(congestion[:len(best_baseline_performance)], best_baseline_performance,
                          c='r', s=50, label='Best Baseline', alpha=0.7)
                
                # Add trend lines
                z_hydra = np.polyfit(congestion[:len(hydra_performance)], hydra_performance, 1)
                z_baseline = np.polyfit(congestion[:len(best_baseline_performance)], 
                                       best_baseline_performance, 1)
                
                x_fit = np.linspace(min(congestion), max(congestion), 10)
                ax.plot(x_fit, np.poly1d(z_hydra)(x_fit), 'k-', linewidth=2)
                ax.plot(x_fit, np.poly1d(z_baseline)(x_fit), 'r--', linewidth=2)
        
        ax.set_xlabel('Network Congestion', fontsize=style['font_sizes']['ticks'])
        ax.set_ylabel('Composite Score', fontsize=style['font_sizes']['ticks'])
        ax.set_title('Network Effects on Performance', fontsize=style['font_sizes']['labels']*0.8)
        ax.legend(fontsize=style['font_sizes']['legend']*0.6)
        ax.grid(**style['grid_style'])

# ===============================
# Interactive Experiment Controller
# ===============================

class InteractiveExperimentController:
    """Controller for interactive simulation experiments."""
    
    def __init__(self):
        self.simulator = DynamicAdaptiveSimulator()
        self.visualizer = DynamicVisualizationSystem()
        self.experiment_history = []
    
    def run_basic_experiment(self, num_steps: int = 10):
        """Run basic simulation experiment."""
        print("=" * 80)
        print("RUNNING BASIC SIMULATION EXPERIMENT")
        print("=" * 80)
        
        # Reset and run
        self.simulator.reset_state()
        results = self.simulator.simulate(num_steps=num_steps)
        
        # Visualize last step
        self.visualizer.plot_dynamic_performance(self.simulator, step=num_steps-1)
        
        # Print summary
        self._print_performance_summary(num_steps-1)
        
        # Store experiment
        self.experiment_history.append({
            'name': 'Basic Experiment',
            'num_steps': num_steps,
            'results': results
        })
        
        return results
    
    def run_parameter_variation_experiment(self, param_name: str, 
                                          values: List[float],
                                          num_steps_per_value: int = 3):
        """Run experiment with parameter variation."""
        print("=" * 80)
        print(f"PARAMETER VARIATION EXPERIMENT: {param_name}")
        print("=" * 80)
        
        results = {}
        
        for value in values:
            print(f"\nSetting {param_name} = {value}")
            
            # Modify parameter
            self.simulator.modify_parameter(
                f"approach_profiles.Hydra_RAN.uncertainty_profile.{param_name}",
                value
            )
            
            # Reset and run
            self.simulator.reset_state()
            step_results = self.simulator.simulate(num_steps=num_steps_per_value)
            
            # Get final performance
            final_metrics = self.simulator.get_current_performance_summary()
            results[value] = {
                'final_metrics': final_metrics,
                'hydra_performance': final_metrics.get('Hydra_RAN', {})
            }
            
            print(f"  Hydra-RAN Composite Score: {final_metrics.get('Hydra_RAN', {}).get('composite_score', 0):.3f}")
        
        # Plot results
        self._plot_parameter_variation_results(param_name, values, results)
        
        return results
    
    def run_environment_change_experiment(self, scenario_changes: List[Dict]):
        """Run experiment with environment changes."""
        print("=" * 80)
        print("ENVIRONMENT CHANGE EXPERIMENT")
        print("=" * 80)
        
        all_results = {}
        
        for i, change in enumerate(scenario_changes):
            scenario_name = change.get('name', f'Scenario_{i+1}')
            print(f"\nRunning {scenario_name}")
            
            # Apply changes
            for param_path, value in change.get('parameters', {}).items():
                self.simulator.modify_parameter(param_path, value)
            
            # Reset and run
            self.simulator.reset_state()
            results = self.simulator.simulate(num_steps=5)
            
            # Store results
            all_results[scenario_name] = {
                'results': results,
                'final_summary': self.simulator.get_current_performance_summary()
            }
            
            # Visualize
            self.visualizer.plot_dynamic_performance(self.simulator, step=4)
        
        # Compare scenarios
        self._compare_scenarios(all_results)
        
        return all_results
    
    def run_sensitivity_analysis(self):
        """Run comprehensive sensitivity analysis."""
        print("=" * 80)
        print("COMPREHENSIVE SENSITIVITY ANALYSIS")
        print("=" * 80)
        
        sensitivity_results = {}
        
        # Test different parameters
        test_params = [
            ('base_mean', np.linspace(0.2, 0.8, 7)),
            ('base_std', np.linspace(0.05, 0.25, 5)),
            ('temporal_variation', np.linspace(0.01, 0.2, 5))
        ]
        
        fig, axes = plt.subplots(1, len(test_params), figsize=(18, 6))
        
        for idx, (param_name, value_range) in enumerate(test_params):
            results = self.simulator.run_sensitivity_analysis(
                param_name, value_range, num_steps_per_value=3
            )
            sensitivity_results[param_name] = results
            
            # Plot sensitivity
            ax = axes[idx] if len(test_params) > 1 else axes
            self._plot_sensitivity_curve(ax, param_name, value_range, results)
        
        plt.suptitle('Parameter Sensitivity Analysis', fontsize=16)
        plt.tight_layout()
        plt.show()
        
        return sensitivity_results
    
    def _print_performance_summary(self, step: int):
        """Print performance summary for a step."""
        summary = self.simulator.get_current_performance_summary(step)
        
        print("\n" + "=" * 80)
        print("PERFORMANCE SUMMARY")
        print("=" * 80)
        
        # Sort by composite score (higher is better)
        sorted_approaches = sorted(
            summary.items(),
            key=lambda x: x[1].get('composite_score', 0),
            reverse=True
        )
        
        for rank, (approach_name, metrics) in enumerate(sorted_approaches, 1):
            print(f"\n{rank}. {approach_name.replace('_', ' ')}:")
            print(f"   Composite Score: {metrics.get('composite_score', 0):.3f}")
            print(f"   Mean Load: {metrics.get('mean_load', 0):.2f}%")
            print(f"   Efficiency Score: {metrics.get('efficiency_score', 0):.3f}")
            print(f"   Load Variability: {metrics.get('std_load', 0):.2f}%")
        
        # Calculate improvements
        if 'Hydra_RAN' in summary:
            hydra_score = summary['Hydra_RAN']['composite_score']
            baseline_scores = [
                metrics['composite_score']
                for name, metrics in summary.items()
                if name != 'Hydra_RAN'
            ]
            
            if baseline_scores:
                best_baseline = max(baseline_scores)
                improvement = ((hydra_score - best_baseline) / best_baseline) * 100
                print(f"\nHydra-RAN Improvement: {improvement:+.1f}% over best baseline")
        
        print("=" * 80)
    
    def _plot_parameter_variation_results(self, param_name: str, 
                                         values: List[float], 
                                         results: Dict):
        """Plot results from parameter variation experiment."""
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        # Extract data
        composite_scores = []
        mean_loads = []
        efficiency_scores = []
        
        for value in values:
            if value in results and 'hydra_performance' in results[value]:
                perf = results[value]['hydra_performance']
                composite_scores.append(perf.get('composite_score', 0))
                mean_loads.append(perf.get('mean_load', 0))
                efficiency_scores.append(perf.get('efficiency_score', 0))
        
        # Plot 1: Composite Score
        axes[0].plot(values[:len(composite_scores)], composite_scores, 
                    'ko-', linewidth=3, markersize=10)
        axes[0].set_xlabel(param_name.replace('_', ' ').title())
        axes[0].set_ylabel('Composite Score')
        axes[0].set_title('Composite Score vs Parameter')
        axes[0].grid(True, alpha=0.3)
        
        # Plot 2: Mean Load
        axes[1].plot(values[:len(mean_loads)], mean_loads, 
                    'ro-', linewidth=3, markersize=10)
        axes[1].set_xlabel(param_name.replace('_', ' ').title())
        axes[1].set_ylabel('Mean Load (%)')
        axes[1].set_title('Mean Load vs Parameter')
        axes[1].grid(True, alpha=0.3)
        
        # Plot 3: Efficiency Score
        axes[2].plot(values[:len(efficiency_scores)], efficiency_scores, 
                    'bo-', linewidth=3, markersize=10)
        axes[2].set_xlabel(param_name.replace('_', ' ').title())
        axes[2].set_ylabel('Efficiency Score')
        axes[2].set_title('Efficiency vs Parameter')
        axes[2].grid(True, alpha=0.3)
        
        plt.suptitle(f'Parameter Variation: {param_name}', fontsize=16)
        plt.tight_layout()
        plt.show()
    
    def _compare_scenarios(self, all_results: Dict):
        """Compare performance across different scenarios."""
        scenario_names = list(all_results.keys())
        
        # Extract Hydra-RAN performance for each scenario
        hydra_scores = []
        hydra_loads = []
        
        for scenario_name, results in all_results.items():
            summary = results['final_summary']
            if 'Hydra_RAN' in summary:
                hydra_scores.append(summary['Hydra_RAN']['composite_score'])
                hydra_loads.append(summary['Hydra_RAN']['mean_load'])
        
        # Plot comparison
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        x = np.arange(len(scenario_names))
        
        # Composite scores
        bars1 = axes[0].bar(x, hydra_scores, color='k', alpha=0.7)
        axes[0].set_xticks(x)
        axes[0].set_xticklabels(scenario_names, rotation=45, ha='right')
        axes[0].set_ylabel('Composite Score')
        axes[0].set_title('Hydra-RAN Performance Across Scenarios')
        axes[0].grid(True, alpha=0.3, axis='y')
        
        # Add value labels
        for bar, score in zip(bars1, hydra_scores):
            height = bar.get_height()
            axes[0].text(bar.get_x() + bar.get_width()/2., height,
                        f'{score:.3f}', ha='center', va='bottom')
        
        # Mean loads
        bars2 = axes[1].bar(x, hydra_loads, color='r', alpha=0.7)
        axes[1].set_xticks(x)
        axes[1].set_xticklabels(scenario_names, rotation=45, ha='right')
        axes[1].set_ylabel('Mean Load (%)')
        axes[1].set_title('Hydra-RAN Load Across Scenarios')
        axes[1].grid(True, alpha=0.3, axis='y')
        
        # Add value labels
        for bar, load in zip(bars2, hydra_loads):
            height = bar.get_height()
            axes[1].text(bar.get_x() + bar.get_width()/2., height,
                        f'{load:.1f}%', ha='center', va='bottom')
        
        plt.suptitle('Scenario Comparison Results', fontsize=16)
        plt.tight_layout()
        plt.show()
    
    def _plot_sensitivity_curve(self, ax, param_name: str, 
                               value_range: np.ndarray, 
                               results: Dict):
        """Plot sensitivity curve for a parameter."""
        composite_scores = []
        efficiency_scores = []
        
        for value in value_range:
            if value in results and 'avg_metrics' in results[value]:
                avg_metrics = results[value]['avg_metrics']
                if 'Hydra_RAN' in avg_metrics:
                    composite_scores.append(avg_metrics['Hydra_RAN']['composite_score'])
                    efficiency_scores.append(avg_metrics['Hydra_RAN']['efficiency_score'])
        
        # Plot
        if composite_scores:
            ax.plot(value_range[:len(composite_scores)], composite_scores,
                   'ko-', linewidth=3, markersize=8, label='Composite Score')
            ax.plot(value_range[:len(efficiency_scores)], efficiency_scores,
                   'ro--', linewidth=2, markersize=6, label='Efficiency Score')
            
            ax.set_xlabel(param_name.replace('_', ' ').title())
            ax.set_ylabel('Score')
            ax.set_title(f'Sensitivity to {param_name.replace("_", " ")}')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        return ax

# ===============================
# Dynamic Performance Analyzer
# ===============================

class DynamicPerformanceAnalyzer:
    """Analyze dynamic performance results."""
    
    @staticmethod
    def calculate_dynamic_statistics(simulator: DynamicAdaptiveSimulator) -> Dict:
        """Calculate comprehensive dynamic statistics."""
        stats = {}
        
        for approach_name in simulator.params.approach_profiles.keys():
            if simulator.performance_metrics[approach_name]:
                metrics_list = simulator.performance_metrics[approach_name]
                
                # Extract time series
                composite_scores = [m['composite_score'] for m in metrics_list]
                mean_loads = [m['mean_load'] for m in metrics_list]
                efficiency_scores = [m['efficiency_score'] for m in metrics_list]
                
                # Calculate statistics
                stats[approach_name] = {
                    'composite_score': {
                        'mean': np.mean(composite_scores),
                        'std': np.std(composite_scores),
                        'min': np.min(composite_scores),
                        'max': np.max(composite_scores),
                        'trend': np.polyfit(range(len(composite_scores)), 
                                           composite_scores, 1)[0]
                    },
                    'mean_load': {
                        'mean': np.mean(mean_loads),
                        'std': np.std(mean_loads),
                        'trend': np.polyfit(range(len(mean_loads)), 
                                           mean_loads, 1)[0]
                    },
                    'efficiency_score': {
                        'mean': np.mean(efficiency_scores),
                        'std': np.std(efficiency_scores),
                        'trend': np.polyfit(range(len(efficiency_scores)), 
                                           efficiency_scores, 1)[0]
                    },
                    'stability': 1.0 / (np.std(composite_scores) + 0.01),
                    'improvement_potential': (
                        np.max(composite_scores) - np.min(composite_scores)
                    ) / np.mean(composite_scores)
                }
        
        return stats
    
    @staticmethod
    def identify_performance_drivers(simulator: DynamicAdaptiveSimulator) -> Dict:
        """Identify key drivers of performance variations."""
        drivers = {}
        
        # Correlate environment factors with performance
        if simulator.environment_history and simulator.current_step > 1:
            steps = min(20, len(simulator.environment_history))
            
            environment_factors = []
            hydra_performance = []
            
            for i in range(steps):
                env = simulator.environment_history[i]
                if i < len(simulator.performance_metrics['Hydra_RAN']):
                    perf = simulator.performance_metrics['Hydra_RAN'][i]
                    
                    environment_factors.append([
                        env['network']['congestion'],
                        env['network']['latency'] / 100,
                        env['network']['channel_quality'],
                        env['vehicle_density'] / 100
                    ])
                    hydra_performance.append(perf['composite_score'])
            
            if environment_factors and hydra_performance:
                # Calculate correlations
                env_array = np.array(environment_factors)
                perf_array = np.array(hydra_performance)
                
                correlations = []
                for j in range(env_array.shape[1]):
                    corr = np.corrcoef(env_array[:, j], perf_array)[0, 1]
                    correlations.append(corr)
                
                drivers['environment_correlations'] = {
                    'congestion': correlations[0],
                    'latency': correlations[1],
                    'channel_quality': correlations[2],
                    'vehicle_density': correlations[3]
                }
        
        return drivers
    
    @staticmethod
    def generate_performance_report(simulator: DynamicAdaptiveSimulator) -> str:
        """Generate comprehensive performance report."""
        report_lines = []
        
        report_lines.append("=" * 80)
        report_lines.append("DYNAMIC PERFORMANCE ANALYSIS REPORT")
        report_lines.append("=" * 80)
        report_lines.append(f"Simulation Steps: {simulator.current_step}")
        report_lines.append(f"Total Approaches: {len(simulator.params.approach_profiles)}")
        report_lines.append("")
        
        # Calculate statistics
        stats = DynamicPerformanceAnalyzer.calculate_dynamic_statistics(simulator)
        drivers = DynamicPerformanceAnalyzer.identify_performance_drivers(simulator)
        
        # Overall performance ranking
        report_lines.append("OVERALL PERFORMANCE RANKING:")
        report_lines.append("-" * 40)
        
        sorted_approaches = sorted(
            stats.items(),
            key=lambda x: x[1]['composite_score']['mean'],
            reverse=True
        )
        
        for rank, (approach_name, stat) in enumerate(sorted_approaches, 1):
            report_lines.append(
                f"{rank}. {approach_name.replace('_', ' ')}: "
                f"Composite Score = {stat['composite_score']['mean']:.3f} "
                f"(±{stat['composite_score']['std']:.3f})"
            )
        
        report_lines.append("")
        
        # Hydra-RAN detailed analysis
        if 'Hydra_RAN' in stats:
            hydra_stats = stats['Hydra_RAN']
            report_lines.append("HYDRA-RAN DETAILED ANALYSIS:")
            report_lines.append("-" * 40)
            report_lines.append(f"Mean Composite Score: {hydra_stats['composite_score']['mean']:.3f}")
            report_lines.append(f"Score Trend: {hydra_stats['composite_score']['trend']:+.4f} per step")
            report_lines.append(f"Performance Stability: {hydra_stats['stability']:.3f}")
            report_lines.append(f"Improvement Potential: {hydra_stats['improvement_potential']*100:.1f}%")
            report_lines.append("")
        
        # Performance drivers
        if 'environment_correlations' in drivers:
            report_lines.append("PERFORMANCE DRIVERS:")
            report_lines.append("-" * 40)
            for factor, corr in drivers['environment_correlations'].items():
                report_lines.append(f"{factor.replace('_', ' ').title()}: {corr:+.3f}")
            report_lines.append("")
        
        # Recommendations
        report_lines.append("RECOMMENDATIONS:")
        report_lines.append("-" * 40)
        
        # Find best and worst performing baselines
        baseline_stats = {
            name: stat for name, stat in stats.items()
            if name != 'Hydra_RAN' and 'composite_score' in stat
        }
        
        if baseline_stats:
            best_baseline = max(baseline_stats.items(), 
                               key=lambda x: x[1]['composite_score']['mean'])
            worst_baseline = min(baseline_stats.items(), 
                                key=lambda x: x[1]['composite_score']['mean'])
            
            report_lines.append(f"Best Baseline: {best_baseline[0].replace('_', ' ')}")
            report_lines.append(f"Worst Baseline: {worst_baseline[0].replace('_', ' ')}")
            
            if 'Hydra_RAN' in stats:
                hydra_mean = stats['Hydra_RAN']['composite_score']['mean']
                best_baseline_mean = best_baseline[1]['composite_score']['mean']
                improvement = ((hydra_mean - best_baseline_mean) / best_baseline_mean) * 100
                report_lines.append(f"Hydra-RAN Improvement: {improvement:+.1f}% over best baseline")
        
        report_lines.append("=" * 80)
        
        return "\n".join(report_lines)

# ===============================
# Main Execution
# ===============================

def main():
    """Main execution function."""
    print("Initializing Dynamic Adaptive Signaling Load Simulator...")
    print("=" * 80)
    
    # Create controller
    controller = InteractiveExperimentController()
    
    # Run basic experiment
    print("\n1. Running Basic Simulation Experiment...")
    controller.run_basic_experiment(num_steps=8)
    
    # Run parameter variation
    print("\n\n2. Running Parameter Variation Experiment...")
    controller.run_parameter_variation_experiment(
        param_name='base_mean',
        values=[0.2, 0.35, 0.5, 0.65, 0.8]
    )
    
    # Run environment changes
    print("\n\n3. Running Environment Change Experiment...")
    scenarios = [
        {
            'name': 'Low_Congestion',
            'parameters': {
                'vehicle_density': 30.0,
                'network_params.congestion_level': 0.2
            }
        },
        {
            'name': 'High_Congestion',
            'parameters': {
                'vehicle_density': 100.0,
                'network_params.congestion_level': 0.7
            }
        },
        {
            'name': 'Good_Channel',
            'parameters': {
                'network_params.channel_quality': 0.9,
                'network_params.packet_loss': 0.02
            }
        }
    ]
    
    controller.run_environment_change_experiment(scenarios)
    
    # Run sensitivity analysis
    print("\n\n4. Running Sensitivity Analysis...")
    controller.run_sensitivity_analysis()
    
    # Generate comprehensive report
    print("\n\n5. Generating Performance Report...")
    report = DynamicPerformanceAnalyzer.generate_performance_report(controller.simulator)
    print(report)
    
    print("\n" + "=" * 80)
    print("SIMULATION COMPLETE")
    print("=" * 80)
    
    return controller

def demo_quick_changes():
    """Demonstrate quick parameter changes and effects."""
    print("QUICK PARAMETER CHANGE DEMONSTRATION")
    print("=" * 80)
    
    # Create simulator
    simulator = DynamicAdaptiveSimulator()
    
    # Initial run
    print("\n1. Initial simulation...")
    simulator.simulate(num_steps=5)
    summary1 = simulator.get_current_performance_summary()
    hydra_score1 = summary1.get('Hydra_RAN', {}).get('composite_score', 0)
    print(f"  Hydra-RAN initial score: {hydra_score1:.3f}")
    
    # Change uncertainty parameters
    print("\n2. Increasing Hydra-RAN uncertainty mean from 0.35 to 0.6...")
    simulator.modify_parameter(
        "approach_profiles.Hydra_RAN.uncertainty_profile.base_mean",
        0.6
    )
    
    # Reset and rerun
    simulator.reset_state()
    simulator.simulate(num_steps=5)
    summary2 = simulator.get_current_performance_summary()
    hydra_score2 = summary2.get('Hydra_RAN', {}).get('composite_score', 0)
    print(f"  Hydra-RAN new score: {hydra_score2:.3f}")
    print(f"  Change: {(hydra_score2 - hydra_score1):+.3f}")
    
    # Change network conditions
    print("\n3. Increasing network congestion from 0.3 to 0.8...")
    simulator.modify_parameter("vehicle_density", 120.0)
    
    # Reset and rerun
    simulator.reset_state()
    simulator.simulate(num_steps=5)
    summary3 = simulator.get_current_performance_summary()
    hydra_score3 = summary3.get('Hydra_RAN', {}).get('composite_score', 0)
    print(f"  Hydra-RAN with high congestion: {hydra_score3:.3f}")
    print(f"  Change from initial: {(hydra_score3 - hydra_score1):+.3f}")
    
    # Change load calculation method
    print("\n4. Switching to weighted load calculation...")
    simulator.modify_parameter(
        "load_calculation.load_function",
        "weighted"
    )
    
    # Reset and rerun
    simulator.reset_state()
    simulator.simulate(num_steps=5)
    summary4 = simulator.get_current_performance_summary()
    hydra_score4 = summary4.get('Hydra_RAN', {}).get('composite_score', 0)
    print(f"  Hydra-RAN with weighted load: {hydra_score4:.3f}")
    print(f"  Change from initial: {(hydra_score4 - hydra_score1):+.3f}")
    
    # Visualize all results
    print("\n5. Visualizing results...")
    DynamicVisualizationSystem.plot_dynamic_performance(simulator)
    
    return simulator

# ===============================
# Execution Entry Points
# ===============================

if __name__ == "__main__":
    # Run full simulation
    controller = main()
    
    # Uncomment for quick demo
    # simulator = demo_quick_changes()
    
    # Example of interactive parameter changes
    print("\n" + "=" * 80)
    print("INTERACTIVE PARAMETER CHANGES EXAMPLE")
    print("=" * 80)
    print("\nYou can modify any parameter and see immediate effects:")
    print("1. Change uncertainty distribution:")
    print("   controller.simulator.modify_parameter(")
    print("       'approach_profiles.Hydra_RAN.uncertainty_profile.base_mean',")
    print("       0.25  # New value")
    print("   )")
    print("   controller.simulator.reset_state()")
    print("   controller.simulator.simulate(num_steps=5)")
    print("   controller.visualizer.plot_dynamic_performance(controller.simulator)")
    print("\n2. Change network parameters:")
    print("   controller.simulator.modify_parameter('vehicle_density', 80.0)")
    print("   controller.simulator.modify_parameter('data_rate', 150.0)")
    print("\n3. Change performance weights:")
    print("   controller.simulator.modify_parameter(")
    print("       'performance_weights.mean_load', 0.5)")



# Knowledge distillation gain versus training epochs. Hydra-RAN's hierarchical distillation architecture achieves sustained 8-12\% gains, outperforming conventional distillation approaches that saturate early.


# -*- coding: utf-8 -*-
"""
Dynamic Adaptive Knowledge Distillation Simulation Framework
Performance metrics dynamically respond to all input variables and parameters
with interactive visualization and real-time updates

Author: Enhanced Simulation Framework
Version: 2.0 (Fully Dynamic & Interactive)
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from typing import Dict, List, Callable, Any, Tuple, Optional, Union
from dataclasses import dataclass, field, asdict
from enum import Enum
import json
import ipywidgets as widgets
from IPython.display import display, clear_output
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# ===============================
# ENUMS AND DATA STRUCTURES
# ===============================

class LearningCurveType(Enum):
    """Types of learning curve models"""
    EXPONENTIAL = "exponential"
    SIGMOID = "sigmoid"
    POWER_LAW = "power_law"
    LOGARITHMIC = "logarithmic"
    MIXED = "mixed"
    LINEAR = "linear"
    PIECEWISE = "piecewise"

class DatasetType(Enum):
    """Types of input datasets"""
    SYNTHETIC = "synthetic"
    CIFAR10 = "cifar10"
    IMAGENET = "imagenet"
    CUSTOM = "custom"
    AUTONOMOUS_DRIVING = "autonomous_driving"
    MEDICAL_IMAGING = "medical_imaging"

class NoiseType(Enum):
    """Types of noise models"""
    NORMAL = "normal"
    UNIFORM = "uniform"
    TIME_CORRELATED = "time_correlated"
    EXPONENTIAL = "exponential"
    BURSTY = "bursty"
    NONE = "none"

class GainCalculationMethod(Enum):
    """Methods for calculating KD gain"""
    RELATIVE_PERCENTAGE = "relative_percentage"
    ABSOLUTE_DIFFERENCE = "absolute_difference"
    LOG_RATIO = "log_ratio"
    SPEEDUP_FACTOR = "speedup_factor"
    AREA_RATIO = "area_ratio"
    EFFECT_SIZE = "effect_size"

@dataclass
class DatasetProfile:
    """Dynamic dataset characteristics that affect learning"""
    name: str = "default"
    type: DatasetType = DatasetType.SYNTHETIC
    size: int = 10000
    complexity: float = 1.0  # Multiplicative factor affecting learning difficulty
    class_imbalance: float = 0.1  # 0=balanced, 1=highly imbalanced
    feature_noise: float = 0.05  # Noise in input features
    label_noise: float = 0.02  # Noise in labels
    dimensionality: int = 1000  # Number of features
    redundancy: float = 0.3  # Feature redundancy
    distribution_shift: float = 0.0  # Train-test distribution shift
    
    def get_difficulty_factor(self) -> float:
        """Calculate overall dataset difficulty"""
        return (self.complexity * 0.4 + 
                self.class_imbalance * 0.2 + 
                self.feature_noise * 0.15 + 
                self.label_noise * 0.15 + 
                (1 - self.redundancy) * 0.1)

@dataclass
class ModelArchitecture:
    """Dynamic model architecture parameters"""
    name: str
    parameter_count: int = 1000000
    depth: int = 10
    width: int = 256
    regularization_strength: float = 0.001
    dropout_rate: float = 0.2
    batch_norm: bool = True
    residual_connections: bool = False
    
    def get_capacity_factor(self) -> float:
        """Calculate model capacity factor"""
        capacity = np.log10(self.parameter_count) / 7.0  # Normalize
        capacity *= (1 + 0.1 * self.depth) * (1 + 0.05 * self.width)
        if self.residual_connections:
            capacity *= 1.2
        return min(capacity, 2.0)

@dataclass
class TrainingConfiguration:
    """Dynamic training parameters"""
    batch_size: int = 32
    learning_rate: float = 0.001
    optimizer: str = "adam"
    momentum: float = 0.9
    weight_decay: float = 0.0001
    lr_scheduler: str = "cosine"
    warmup_epochs: int = 5
    gradient_clip: float = 1.0
    
    def get_effective_lr(self, dataset_complexity: float) -> float:
        """Calculate effective learning rate adjusted for dataset"""
        return self.learning_rate / np.sqrt(dataset_complexity)

@dataclass
class KDParameters:
    """Centralized parameter configuration with full dynamic responsiveness"""
    
    # ===== SIMULATION CORE PARAMETERS =====
    epochs: Tuple[int, int, int] = (1, 101, 1)  # Start, stop, step
    seed: int = 42
    num_simulations: int = 5  # Number of Monte Carlo simulations for robustness
    
    # ===== DATASET PROFILES =====
    dataset_profiles: Dict[str, DatasetProfile] = field(default_factory=lambda: {
        'simple': DatasetProfile(name='simple', complexity=0.5, size=5000),
        'medium': DatasetProfile(name='medium', complexity=1.0, size=10000),
        'complex': DatasetProfile(name='complex', complexity=2.0, size=20000),
        'imbalanced': DatasetProfile(name='imbalanced', class_imbalance=0.5, size=10000),
        'noisy': DatasetProfile(name='noisy', feature_noise=0.2, label_noise=0.1)
    })
    
    # Active dataset profile (dynamically changeable)
    active_dataset: str = 'medium'
    
    # ===== MODEL ARCHITECTURES =====
    model_architectures: Dict[str, ModelArchitecture] = field(default_factory=lambda: {
        'Cooperative_Perception': ModelArchitecture(name='Cooperative_Perception', parameter_count=5000000),
        'V2X_ViTv2': ModelArchitecture(name='V2X_ViTv2', parameter_count=10000000, depth=12),
        'Semantic_Comm': ModelArchitecture(name='Semantic_Comm', parameter_count=3000000),
        'Task_Oriented': ModelArchitecture(name='Task_Oriented', parameter_count=8000000, residual_connections=True),
        'Digital_Twin': ModelArchitecture(name='Digital_Twin', parameter_count=6000000),
        'Edge_Collaborative': ModelArchitecture(name='Edge_Collaborative', parameter_count=7000000),
        'FedRSU': ModelArchitecture(name='FedRSU', parameter_count=9000000),
        'Hydra_RAN': ModelArchitecture(name='Hydra_RAN', parameter_count=15000000, depth=20, residual_connections=True)
    })
    
    # ===== DYNAMIC LEARNING CURVE PARAMETERS =====
    # These will be computed based on dataset and model characteristics
    baseline_performance_base: Dict[str, float] = field(default_factory=lambda: {
        'Cooperative_Perception': 0.72,
        'V2X_ViTv2': 0.78,
        'Semantic_Comm': 0.75,
        'Task_Oriented': 0.80,
        'Digital_Twin': 0.77,
        'Edge_Collaborative': 0.79,
        'FedRSU': 0.81,
        'Hydra_RAN': 0.83
    })
    
    # KD effectiveness factors (dynamically adjustable)
    kd_effectiveness: Dict[str, float] = field(default_factory=lambda: {
        'Cooperative_Perception': 1.0,
        'V2X_ViTv2': 1.2,
        'Semantic_Comm': 1.1,
        'Task_Oriented': 1.3,
        'Digital_Twin': 1.15,
        'Edge_Collaborative': 1.25,
        'FedRSU': 1.3,
        'Hydra_RAN': 2.0  # Proposed approach has higher effectiveness
    })
    
    # Learning curve types (can be dynamically changed)
    curve_types: Dict[str, LearningCurveType] = field(default_factory=lambda: {
        'Cooperative_Perception': LearningCurveType.EXPONENTIAL,
        'V2X_ViTv2': LearningCurveType.SIGMOID,
        'Semantic_Comm': LearningCurveType.EXPONENTIAL,
        'Task_Oriented': LearningCurveType.POWER_LAW,
        'Digital_Twin': LearningCurveType.EXPONENTIAL,
        'Edge_Collaborative': LearningCurveType.SIGMOID,
        'FedRSU': LearningCurveType.EXPONENTIAL,
        'Hydra_RAN': LearningCurveType.MIXED
    })
    
    # ===== TRAINING CONFIGURATIONS =====
    training_configs: Dict[str, TrainingConfiguration] = field(default_factory=lambda: {
        'baseline': TrainingConfiguration(learning_rate=0.001, batch_size=32),
        'distilled': TrainingConfiguration(learning_rate=0.0012, batch_size=64)
    })
    
    # ===== NOISE AND VARIABILITY PARAMETERS =====
    noise_config: Dict = field(default_factory=lambda: {
        'baseline_noise': 0.01,
        'distilled_noise': 0.008,
        'noise_type': NoiseType.NORMAL,
        'time_correlation': 0.3,
        'burst_probability': 0.05,
        'burst_intensity': 0.05,
        'noise_scaling_factor': 1.0  # Dynamic scaling factor
    })
    
    # ===== KNOWLEDGE DISTILLATION PARAMETERS =====
    kd_config: Dict = field(default_factory=lambda: {
        'temperature': 4.0,
        'alpha': 0.7,  # Weight for KD loss
        'beta': 0.3,   # Weight for ground truth loss
        'distillation_epochs': 20,  # When to start KD
        'teacher_performance': 0.90  # Teacher model performance
    })
    
    # ===== GAIN CALCULATION PARAMETERS =====
    gain_calculation: Dict = field(default_factory=lambda: {
        'method': GainCalculationMethod.RELATIVE_PERCENTAGE,
        'smoothing_window': 3,
        'normalization': 'baseline',
        'confidence_level': 0.95,
        'bootstrap_samples': 1000
    })
    
    # ===== SYSTEM AND HARDWARE PARAMETERS =====
    system_config: Dict = field(default_factory=lambda: {
        'compute_capacity': 1.0,  # Normalized compute capacity
        'memory_constraint': 1.0,  # Memory availability factor
        'communication_overhead': 0.05,  # For distributed learning
        'energy_efficiency': 1.0,
        'latency': 0.0  # Inference latency in ms
    })
    
    # ===== PLOT STYLING PARAMETERS =====
    plot_style: Dict = field(default_factory=lambda: {
        'figure_facecolor': '#EAEAF2',
        'axes_facecolor': '#EAEAF2',
        'grid_style': {'linestyle': '--', 'linewidth': 1.2, 'alpha': 0.6, 'color': 'gray'},
        'font_sizes': {'title': 38, 'labels': 38, 'ticks': 32, 'legend': 28, 'annotation': 24},
        'line_widths': {'baseline': 2.5, 'proposed': 5, 'confidence': 0.8},
        'markers': ['o-', 's-', '^-', 'd-', 'x-', '*-', 'p-', 'h-'],
        'color_palette': sns.color_palette("husl", 8),
        'transparency': {'confidence': 0.2, 'background': 0.1}
    })
    
    # ===== DYNAMIC RESPONSE FUNCTIONS =====
    # These functions define how parameters affect each other
    response_functions: Dict = field(default_factory=lambda: {
        'dataset_complexity_to_learning_rate': lambda x: 1.0 / np.sqrt(x),
        'model_capacity_to_performance': lambda x: 1.0 - np.exp(-0.5 * x),
        'noise_to_performance': lambda x: np.exp(-2 * x),
        'kd_effectiveness_to_gain': lambda x: 0.1 * x + 0.02 * x**2,
        'system_constraints_to_convergence': lambda compute, memory: compute * memory
    })

# ===============================
# DYNAMIC SIMULATION ENGINE
# ===============================

class DynamicKDSimulator:
    """Fully dynamic simulator with parameter-responsive performance evaluation"""
    
    def __init__(self, params: Optional[KDParameters] = None):
        """Initialize simulator with dynamic parameters"""
        self.params = params if params else KDParameters()
        np.random.seed(self.params.seed)
        self.epochs = np.arange(*self.params.epochs)
        
        # Dynamic state variables
        self.learning_curves = {'baseline': {}, 'distilled': {}, 'confidence': {}}
        self.kd_gains = {}
        self.performance_metrics = {}
        self.sensitivity_matrices = {}
        self.correlation_matrices = {}
        
        # Monte Carlo simulation results
        self.monte_carlo_results = []
        
        # Parameter change history
        self.parameter_history = []
        self.performance_history = []
        
        # Cache for performance
        self._cache = {}
        
    def compute_dynamic_parameters(self, model_name: str, is_distilled: bool = False) -> Dict:
        """Compute dynamic parameters based on current state"""
        dataset = self.params.dataset_profiles[self.params.active_dataset]
        model_arch = self.params.model_architectures[model_name]
        train_config = self.params.training_configs['distilled' if is_distilled else 'baseline']
        
        # Base performance adjusted by dataset difficulty
        base_perf = self.params.baseline_performance_base[model_name]
        difficulty = dataset.get_difficulty_factor()
        
        # Adjust max performance based on dataset and model
        if is_distilled:
            # Distilled models handle complexity better
            max_perf = base_perf * (1 - 0.1 * difficulty)
            max_perf += 0.05 * self.params.kd_effectiveness[model_name]
        else:
            max_perf = base_perf * (1 - 0.15 * difficulty)
        
        # Adjust learning rate based on model capacity and dataset
        base_lr = train_config.get_effective_lr(dataset.complexity)
        capacity = model_arch.get_capacity_factor()
        learning_rate = base_lr * (1 + 0.2 * capacity)
        
        # Adjust noise based on dataset noise
        noise_multiplier = 1 + dataset.feature_noise * 5 + dataset.label_noise * 3
        noise_level = (self.params.noise_config['distilled_noise' if is_distilled else 'baseline_noise'] 
                      * noise_multiplier 
                      * self.params.noise_config['noise_scaling_factor'])
        
        # Adjust KD boost based on effectiveness and dataset
        if is_distilled:
            boost = self.params.response_functions['kd_effectiveness_to_gain'](
                self.params.kd_effectiveness[model_name]
            ) * (1 - 0.3 * dataset.complexity)
        else:
            boost = 0.0
        
        return {
            'max_perf': max_perf,
            'learning_rate': learning_rate,
            'noise_level': noise_level,
            'boost': boost,
            'curve_type': self.params.curve_types[model_name],
            'capacity_factor': capacity,
            'difficulty_factor': difficulty
        }
    
    def _generate_dynamic_noise(self, size: int, noise_level: float, 
                              noise_type: NoiseType) -> np.ndarray:
        """Generate dynamic noise based on current configuration"""
        noise_config = self.params.noise_config
        
        if noise_type == NoiseType.NORMAL:
            return np.random.normal(0, noise_level, size)
        
        elif noise_type == NoiseType.UNIFORM:
            return np.random.uniform(-noise_level, noise_level, size)
        
        elif noise_type == NoiseType.TIME_CORRELATED:
            correlation = noise_config['time_correlation']
            noise = np.zeros(size)
            noise[0] = np.random.normal(0, noise_level)
            for i in range(1, size):
                noise[i] = (correlation * noise[i-1] + 
                           np.random.normal(0, noise_level * np.sqrt(1 - correlation**2)))
            return noise
        
        elif noise_type == NoiseType.EXPONENTIAL:
            # Noise decays over time (simulating learning stabilization)
            decay = np.exp(-np.linspace(0, 3, size))
            return np.random.normal(0, noise_level, size) * decay
        
        elif noise_type == NoiseType.BURSTY:
            # Bursty noise with occasional spikes
            noise = np.random.normal(0, noise_level * 0.5, size)
            burst_mask = np.random.random(size) < noise_config['burst_probability']
            noise[burst_mask] += np.random.normal(
                0, noise_config['burst_intensity'], np.sum(burst_mask)
            )
            return noise
        
        else:  # NoiseType.NONE
            return np.zeros(size)
    
    def learning_curve_model(self, epochs: np.ndarray, params: Dict) -> np.ndarray:
        """Generate learning curve with dynamic parameters"""
        max_perf = params['max_perf']
        learning_rate = params['learning_rate']
        curve_type = params['curve_type']
        noise_level = params['noise_level']
        capacity = params['capacity_factor']
        difficulty = params['difficulty_factor']
        
        # Adjust learning rate based on model capacity and dataset difficulty
        effective_lr = learning_rate * (1 + 0.1 * capacity) / (1 + 0.2 * difficulty)
        
        if curve_type == LearningCurveType.EXPONENTIAL:
            perf = max_perf * (1 - np.exp(-effective_lr * epochs))
        
        elif curve_type == LearningCurveType.SIGMOID:
            midpoint = 20 + 10 * difficulty  # Harder datasets take longer
            steepness = 0.1 * (1 + 0.5 * capacity)  # Better capacity = steeper learning
            perf = max_perf / (1 + np.exp(-steepness * (epochs - midpoint)))
        
        elif curve_type == LearningCurveType.POWER_LAW:
            exponent = 0.5 + 0.2 * capacity
            perf = max_perf * (1 - (1 + effective_lr * epochs) ** (-exponent))
        
        elif curve_type == LearningCurveType.LOGARITHMIC:
            perf = max_perf * np.log(1 + effective_lr * epochs) / np.log(1 + effective_lr * epochs.max())
        
        elif curve_type == LearningCurveType.MIXED:
            transition = 30 + 20 * difficulty
            early_idx = epochs <= transition
            late_idx = epochs > transition
            
            early_perf = max_perf * (1 - np.exp(-effective_lr * 2 * epochs[early_idx]))
            if np.any(late_idx):
                late_start = early_perf[-1] if len(early_perf) > 0 else 0
                late_growth = max_perf - late_start
                late_perf = late_start + late_growth * np.log(1 + effective_lr * (epochs[late_idx] - transition)) / np.log(1 + effective_lr * (epochs[late_idx].max() - transition))
                perf = np.concatenate([early_perf, late_perf])
            else:
                perf = early_perf
        
        elif curve_type == LearningCurveType.LINEAR:
            slope = effective_lr * max_perf
            perf = np.minimum(slope * epochs, max_perf)
        
        elif curve_type == LearningCurveType.PIECEWISE:
            # Piecewise learning: fast initial, then plateau, then slow improvement
            fast_end = 10
            plateau_end = 50
            
            fast_slope = effective_lr * max_perf * 2
            plateau_level = fast_slope * fast_end
            slow_slope = effective_lr * max_perf * 0.3
            
            perf = np.zeros_like(epochs)
            mask1 = epochs <= fast_end
            mask2 = (epochs > fast_end) & (epochs <= plateau_end)
            mask3 = epochs > plateau_end
            
            perf[mask1] = fast_slope * epochs[mask1]
            perf[mask2] = plateau_level
            if np.any(mask3):
                perf[mask3] = plateau_level + slow_slope * (epochs[mask3] - plateau_end)
            perf = np.minimum(perf, max_perf)
        
        else:
            perf = max_perf * (1 - np.exp(-effective_lr * epochs))
        
        # Add dynamic noise
        noise = self._generate_dynamic_noise(
            len(epochs), noise_level, self.params.noise_config['noise_type']
        )
        perf += noise
        
        # Clip and ensure monotonic increase (learning shouldn't decrease)
        perf = np.clip(perf, 0, max_perf * 1.1)
        perf = np.maximum.accumulate(perf) * 0.99 + perf * 0.01  # Mostly monotonic
        
        return perf
    
    def calculate_kd_gain(self, distilled_perf: np.ndarray, 
                         baseline_perf: np.ndarray,
                         method: GainCalculationMethod = None) -> np.ndarray:
        """Calculate KD gain with dynamic method selection"""
        if method is None:
            method = self.params.gain_calculation['method']
        
        window = self.params.gain_calculation['smoothing_window']
        
        # Apply smoothing
        if window > 1:
            kernel = np.ones(window) / window
            distilled_smooth = np.convolve(distilled_perf, kernel, mode='same')
            baseline_smooth = np.convolve(baseline_perf, kernel, mode='same')
        else:
            distilled_smooth = distilled_perf
            baseline_smooth = baseline_perf
        
        if method == GainCalculationMethod.RELATIVE_PERCENTAGE:
            return (distilled_smooth - baseline_smooth) / (baseline_smooth + 1e-10) * 100
        
        elif method == GainCalculationMethod.ABSOLUTE_DIFFERENCE:
            return (distilled_smooth - baseline_smooth) * 100
        
        elif method == GainCalculationMethod.LOG_RATIO:
            return np.log(distilled_smooth / (baseline_smooth + 1e-10)) * 100
        
        elif method == GainCalculationMethod.SPEEDUP_FACTOR:
            gains = np.zeros_like(distilled_smooth)
            for i in range(len(distilled_smooth)):
                target_perf = baseline_smooth[i]
                if target_perf > 0:
                    idx = np.where(distilled_smooth[:i+1] >= target_perf)[0]
                    if len(idx) > 0:
                        gains[i] = (i - idx[0]) / (i + 1) * 100
            return gains
        
        elif method == GainCalculationMethod.AREA_RATIO:
            # Ratio of areas under the curves
            baseline_area = np.trapz(baseline_smooth[:len(distilled_smooth)], self.epochs[:len(distilled_smooth)])
            distilled_area = np.trapz(distilled_smooth, self.epochs[:len(distilled_smooth)])
            return (distilled_area / (baseline_area + 1e-10) - 1) * 100
        
        elif method == GainCalculationMethod.EFFECT_SIZE:
            # Cohen's d effect size
            pooled_std = np.sqrt((np.std(baseline_smooth)**2 + np.std(distilled_smooth)**2) / 2)
            return ((np.mean(distilled_smooth) - np.mean(baseline_smooth)) / (pooled_std + 1e-10)) * 100
        
        else:
            return (distilled_smooth - baseline_smooth) / baseline_smooth * 100
    
    def run_monte_carlo_simulation(self, n_simulations: int = None) -> Dict:
        """Run Monte Carlo simulations for robustness analysis"""
        if n_simulations is None:
            n_simulations = self.params.num_simulations
        
        results = {
            'gains': [],
            'final_performances': {'baseline': [], 'distilled': []},
            'convergence_epochs': {'baseline': [], 'distilled': []}
        }
        
        original_seed = self.params.seed
        
        for i in range(n_simulations):
            # Change seed for each simulation
            self.params.seed = original_seed + i
            np.random.seed(self.params.seed)
            
            # Run simulation
            self.simulate_all_models()
            gains = self.kd_gains.copy()
            
            # Store results
            results['gains'].append(gains)
            
            for model_name in self.params.model_architectures.keys():
                if model_name in self.learning_curves['baseline']:
                    baseline_final = self.learning_curves['baseline'][model_name][-1]
                    distilled_final = self.learning_curves['distilled'][model_name][-1]
                    
                    results['final_performances']['baseline'].append(baseline_final)
                    results['final_performances']['distilled'].append(distilled_final)
        
        # Restore original seed
        self.params.seed = original_seed
        np.random.seed(self.params.seed)
        
        self.monte_carlo_results = results
        return results
    
    def simulate_all_models(self, custom_epochs: np.ndarray = None) -> Dict:
        """Run dynamic simulation for all configured models"""
        if custom_epochs is not None:
            self.epochs = custom_epochs
        
        # Clear previous results
        self.learning_curves = {'baseline': {}, 'distilled': {}, 'confidence': {}}
        self.kd_gains = {}
        
        # Generate learning curves for each model
        for model_name in self.params.model_architectures.keys():
            # Get dynamic parameters
            baseline_params = self.compute_dynamic_parameters(model_name, is_distilled=False)
            distilled_params = self.compute_dynamic_parameters(model_name, is_distilled=True)
            
            # Add KD boost to distilled
            distilled_params['max_perf'] += distilled_params['boost']
            
            # Generate curves
            baseline_perf = self.learning_curve_model(self.epochs, baseline_params)
            distilled_perf = self.learning_curve_model(self.epochs, distilled_params)
            
            self.learning_curves['baseline'][model_name] = baseline_perf
            self.learning_curves['distilled'][model_name] = distilled_perf
            
            # Calculate KD gain
            self.kd_gains[model_name] = self.calculate_kd_gain(distilled_perf, baseline_perf)
        
        # Calculate confidence intervals if Monte Carlo results exist
        if self.monte_carlo_results:
            self._calculate_confidence_intervals()
        
        return self.kd_gains
    
    def _calculate_confidence_intervals(self):
        """Calculate confidence intervals from Monte Carlo simulations"""
        if not self.monte_carlo_results:
            return
        
        for model_name in self.params.model_architectures.keys():
            gains_matrix = []
            for sim_gains in self.monte_carlo_results['gains']:
                if model_name in sim_gains:
                    gains_matrix.append(sim_gains[model_name])
            
            if gains_matrix:
                gains_matrix = np.array(gains_matrix)
                mean_gain = np.mean(gains_matrix, axis=0)
                std_gain = np.std(gains_matrix, axis=0)
                
                # Calculate confidence interval
                alpha = 1 - self.params.gain_calculation['confidence_level']
                z_score = stats.norm.ppf(1 - alpha/2)
                ci_lower = mean_gain - z_score * std_gain / np.sqrt(len(gains_matrix))
                ci_upper = mean_gain + z_score * std_gain / np.sqrt(len(gains_matrix))
                
                self.learning_curves['confidence'][model_name] = {
                    'mean': mean_gain,
                    'std': std_gain,
                    'ci_lower': ci_lower,
                    'ci_upper': ci_upper
                }
    
    def evaluate_performance(self, recalculate: bool = True) -> Dict:
        """Calculate comprehensive performance metrics"""
        if recalculate or not self.performance_metrics:
            self.performance_metrics = {}
            
            for model_name in self.params.model_architectures.keys():
                if model_name in self.kd_gains:
                    gains = self.kd_gains[model_name]
                    baseline_perf = self.learning_curves['baseline'][model_name]
                    distilled_perf = self.learning_curves['distilled'][model_name]
                    
                    # Basic statistics
                    metrics = {
                        'mean_gain': np.mean(gains),
                        'std_gain': np.std(gains),
                        'max_gain': np.max(gains),
                        'min_gain': np.min(gains),
                        'final_gain': gains[-1],
                        'final_baseline_perf': baseline_perf[-1],
                        'final_distilled_perf': distilled_perf[-1],
                        'area_under_gain_curve': np.trapz(np.clip(gains, 0, None), self.epochs),
                        'gain_stability': 1.0 / (np.std(np.diff(gains)) + 1e-10),
                        'convergence_speed': self._calculate_convergence_speed(model_name),
                        'performance_improvement': (distilled_perf[-1] - baseline_perf[-1]) * 100,
                        'early_gain': np.mean(gains[:10]),  # Gain in first 10 epochs
                        'late_gain': np.mean(gains[-10:]),  # Gain in last 10 epochs
                        'gain_consistency': np.mean(gains > 0)  # Percentage of epochs with positive gain
                    }
                    
                    # Add efficiency metrics
                    metrics.update(self._calculate_efficiency_metrics(model_name))
                    
                    self.performance_metrics[model_name] = metrics
        
        return self.performance_metrics
    
    def _calculate_convergence_speed(self, model_name: str) -> float:
        """Calculate convergence speedup"""
        baseline = self.learning_curves['baseline'][model_name]
        distilled = self.learning_curves['distilled'][model_name]
        
        target_perf = min(baseline[-1], distilled[-1]) * 0.95
        
        baseline_idx = np.where(baseline >= target_perf)[0]
        distilled_idx = np.where(distilled >= target_perf)[0]
        
        if len(baseline_idx) > 0 and len(distilled_idx) > 0:
            speedup = (baseline_idx[0] - distilled_idx[0]) / baseline_idx[0] * 100
            return speedup
        return 0.0
    
    def _calculate_efficiency_metrics(self, model_name: str) -> Dict:
        """Calculate computational efficiency metrics"""
        model_arch = self.params.model_architectures[model_name]
        system_config = self.params.system_config
        
        # Estimate computational cost
        baseline_cost = model_arch.parameter_count * self.epochs[-1]
        distilled_cost = model_arch.parameter_count * self.epochs[-1] * 1.1  # KD adds overhead
        
        # Performance per compute unit
        baseline_perf = self.learning_curves['baseline'][model_name][-1]
        distilled_perf = self.learning_curves['distilled'][model_name][-1]
        
        baseline_efficiency = baseline_perf / (baseline_cost + 1e-10)
        distilled_efficiency = distilled_perf / (distilled_cost + 1e-10)
        
        return {
            'compute_efficiency_ratio': distilled_efficiency / baseline_efficiency,
            'memory_efficiency': system_config['memory_constraint'] * model_arch.get_capacity_factor(),
            'energy_efficiency': system_config['energy_efficiency'] * distilled_efficiency / baseline_efficiency,
            'inference_latency': system_config['latency'] * (1 + 0.1 * model_arch.depth)
        }
    
    def parameter_sensitivity_analysis(self, param_name: str, 
                                     value_range: np.ndarray,
                                     models: List[str] = None) -> Dict:
        """Analyze sensitivity to parameter changes"""
        if models is None:
            models = list(self.params.model_architectures.keys())
        
        sensitivity_results = {}
        
        for value in value_range:
            # Store original state
            original_state = self._get_parameter_state()
            
            # Update parameter
            self._set_parameter(param_name, value)
            
            # Re-run simulation
            self.simulate_all_models()
            metrics = self.evaluate_performance()
            
            # Store results
            sensitivity_results[value] = {
                'gains': self.kd_gains.copy(),
                'metrics': {model: metrics.get(model, {}) for model in models}
            }
            
            # Restore original state
            self._restore_parameter_state(original_state)
        
        return sensitivity_results
    
    def _get_parameter_state(self) -> Dict:
        """Get current parameter state for restoration"""
        return {
            'params': asdict(self.params),
            'learning_curves': self.learning_curves.copy(),
            'kd_gains': self.kd_gains.copy(),
            'performance_metrics': self.performance_metrics.copy()
        }
    
    def _restore_parameter_state(self, state: Dict):
        """Restore parameter state"""
        self.params = KDParameters(**state['params'])
        self.learning_curves = state['learning_curves']
        self.kd_gains = state['kd_gains']
        self.performance_metrics = state['performance_metrics']
    
    def _set_parameter(self, param_path: str, value: Any):
        """Set parameter using dot notation path"""
        parts = param_path.split('.')
        obj = self.params
        
        # Navigate to the parent object
        for part in parts[:-1]:
            if hasattr(obj, part):
                obj = getattr(obj, part)
            elif isinstance(obj, dict) and part in obj:
                obj = obj[part]
            else:
                raise ValueError(f"Parameter path {param_path} not found")
        
        # Set the value
        last_part = parts[-1]
        if hasattr(obj, last_part):
            setattr(obj, last_part, value)
        elif isinstance(obj, dict) and last_part in obj:
            obj[last_part] = value
        else:
            raise ValueError(f"Parameter {last_part} not found in {parts[:-1]}")
    
    def run_comprehensive_analysis(self) -> Dict:
        """Run comprehensive analysis with multiple perspectives"""
        analysis_results = {}
        
        # 1. Basic simulation
        print("Running basic simulation...")
        self.simulate_all_models()
        analysis_results['basic'] = self.evaluate_performance()
        
        # 2. Monte Carlo analysis for robustness
        print("Running Monte Carlo simulations...")
        mc_results = self.run_monte_carlo_simulation()
        analysis_results['monte_carlo'] = mc_results
        
        # 3. Sensitivity analysis on key parameters
        print("Running sensitivity analysis...")
        sensitivity_params = [
            ('dataset_profiles.medium.complexity', np.linspace(0.5, 3.0, 6)),
            ('kd_config.temperature', np.linspace(1.0, 10.0, 5)),
            ('noise_config.noise_scaling_factor', np.linspace(0.1, 2.0, 5))
        ]
        
        sensitivity_results = {}
        for param_name, value_range in sensitivity_params:
            print(f"  Analyzing {param_name}...")
            sensitivity_results[param_name] = self.parameter_sensitivity_analysis(
                param_name, value_range, ['Hydra_RAN', 'V2X_ViTv2']
            )
        
        analysis_results['sensitivity'] = sensitivity_results
        
        # 4. Dataset comparison
        print("Comparing different datasets...")
        dataset_comparison = {}
        for dataset_name in self.params.dataset_profiles.keys():
            self.params.active_dataset = dataset_name
            self.simulate_all_models()
            dataset_comparison[dataset_name] = self.evaluate_performance()['Hydra_RAN']
        
        analysis_results['dataset_comparison'] = dataset_comparison
        
        # 5. Correlation analysis
        print("Performing correlation analysis...")
        self.correlation_matrices = self._calculate_correlations()
        analysis_results['correlations'] = self.correlation_matrices
        
        return analysis_results
    
    def _calculate_correlations(self) -> Dict:
        """Calculate correlations between parameters and performance"""
        # Collect data for correlation analysis
        data_points = []
        
        # Vary multiple parameters and collect performance
        for complexity in np.linspace(0.5, 2.5, 5):
            for noise_scale in np.linspace(0.1, 2.0, 5):
                for kd_effect in np.linspace(0.5, 3.0, 5):
                    # Update parameters
                    self.params.dataset_profiles['medium'].complexity = complexity
                    self.params.noise_config['noise_scaling_factor'] = noise_scale
                    self.params.kd_effectiveness['Hydra_RAN'] = kd_effect
                    
                    # Run simulation
                    self.simulate_all_models()
                    metrics = self.evaluate_performance()['Hydra_RAN']
                    
                    # Collect data
                    data_points.append({
                        'complexity': complexity,
                        'noise_scale': noise_scale,
                        'kd_effectiveness': kd_effect,
                        'mean_gain': metrics['mean_gain'],
                        'final_gain': metrics['final_gain'],
                        'convergence_speed': metrics['convergence_speed']
                    })
        
        # Calculate correlation matrix
        df = pd.DataFrame(data_points)
        correlation_matrix = df.corr()
        
        return {
            'data': df,
            'correlation_matrix': correlation_matrix,
            'top_correlations': correlation_matrix.abs().unstack().sort_values(ascending=False)[:10]
        }

# ===============================
# INTERACTIVE VISUALIZATION
# ===============================

class InteractiveSimulationDashboard:
    """Interactive dashboard for dynamic simulation exploration"""
    
    def __init__(self, simulator: DynamicKDSimulator):
        self.simulator = simulator
        self.fig = None
        self.axs = None
        
        # Create widgets
        self._create_widgets()
        
    def _create_widgets(self):
        """Create interactive widgets"""
        # Dataset selection
        self.dataset_dropdown = widgets.Dropdown(
            options=list(self.simulator.params.dataset_profiles.keys()),
            value=self.simulator.params.active_dataset,
            description='Dataset:',
            disabled=False
        )
        
        # Model selection
        self.model_dropdown = widgets.Dropdown(
            options=list(self.simulator.params.model_architectures.keys()),
            value='Hydra_RAN',
            description='Model:',
            disabled=False
        )
        
        # Parameter sliders
        self.complexity_slider = widgets.FloatSlider(
            value=1.0,
            min=0.1,
            max=3.0,
            step=0.1,
            description='Complexity:',
            continuous_update=False
        )
        
        self.noise_slider = widgets.FloatSlider(
            value=1.0,
            min=0.1,
            max=3.0,
            step=0.1,
            description='Noise Scale:',
            continuous_update=False
        )
        
        self.kd_effect_slider = widgets.FloatSlider(
            value=1.0,
            min=0.1,
            max=3.0,
            step=0.1,
            description='KD Effect:',
            continuous_update=False
        )
        
        # Learning curve type
        self.curve_type_dropdown = widgets.Dropdown(
            options=[(ct.value, ct) for ct in LearningCurveType],
            value=LearningCurveType.EXPONENTIAL,
            description='Curve Type:',
            disabled=False
        )
        
        # Gain calculation method
        self.gain_method_dropdown = widgets.Dropdown(
            options=[(gm.value, gm) for gm in GainCalculationMethod],
            value=GainCalculationMethod.RELATIVE_PERCENTAGE,
            description='Gain Method:',
            disabled=False
        )
        
        # Buttons
        self.run_button = widgets.Button(
            description='Run Simulation',
            button_style='success'
        )
        
        self.reset_button = widgets.Button(
            description='Reset Parameters',
            button_style='warning'
        )
        
        # Output widget
        self.output = widgets.Output()
        
        # Set up callbacks
        self.run_button.on_click(self._run_simulation)
        self.reset_button.on_click(self._reset_parameters)
        
        # Link widgets to parameters
        self.dataset_dropdown.observe(self._update_dataset, 'value')
        self.complexity_slider.observe(self._update_complexity, 'value')
        self.noise_slider.observe(self._update_noise, 'value')
        self.kd_effect_slider.observe(self._update_kd_effect, 'value')
        self.curve_type_dropdown.observe(self._update_curve_type, 'value')
        self.gain_method_dropdown.observe(self._update_gain_method, 'value')
    
    def _update_dataset(self, change):
        """Update dataset parameter"""
        self.simulator.params.active_dataset = change['new']
    
    def _update_complexity(self, change):
        """Update dataset complexity"""
        self.simulator.params.dataset_profiles[self.simulator.params.active_dataset].complexity = change['new']
    
    def _update_noise(self, change):
        """Update noise scaling"""
        self.simulator.params.noise_config['noise_scaling_factor'] = change['new']
    
    def _update_kd_effect(self, change):
        """Update KD effectiveness"""
        model = self.model_dropdown.value
        self.simulator.params.kd_effectiveness[model] = change['new']
    
    def _update_curve_type(self, change):
        """Update learning curve type"""
        model = self.model_dropdown.value
        self.simulator.params.curve_types[model] = change['new']
    
    def _update_gain_method(self, change):
        """Update gain calculation method"""
        self.simulator.params.gain_calculation['method'] = change['new']
    
    def _run_simulation(self, b):
        """Run simulation with current parameters"""
        with self.output:
            clear_output(wait=True)
            
            # Run simulation
            self.simulator.simulate_all_models()
            metrics = self.simulator.evaluate_performance()
            
            # Create visualization
            self._create_visualization(metrics)
            
            # Display metrics
            model = self.model_dropdown.value
            if model in metrics:
                model_metrics = metrics[model]
                print(f"\n{model} Performance Metrics:")
                print("-" * 40)
                print(f"Mean KD Gain: {model_metrics['mean_gain']:.2f}%")
                print(f"Final KD Gain: {model_metrics['final_gain']:.2f}%")
                print(f"Performance Improvement: {model_metrics['performance_improvement']:.2f}%")
                print(f"Convergence Speedup: {model_metrics['convergence_speed']:.1f}%")
                print(f"Compute Efficiency Ratio: {model_metrics['compute_efficiency_ratio']:.3f}")
    
    def _reset_parameters(self, b):
        """Reset parameters to defaults"""
        self.simulator.params = KDParameters()
        self.dataset_dropdown.value = self.simulator.params.active_dataset
        self.complexity_slider.value = 1.0
        self.noise_slider.value = 1.0
        self.kd_effect_slider.value = 1.0
        self.curve_type_dropdown.value = LearningCurveType.EXPONENTIAL
        self.gain_method_dropdown.value = GainCalculationMethod.RELATIVE_PERCENTAGE
        
        with self.output:
            clear_output(wait=True)
            print("Parameters reset to defaults.")
    
    def _create_visualization(self, metrics):
        """Create dynamic visualization"""
        if self.fig is None:
            self.fig, self.axs = plt.subplots(2, 2, figsize=(16, 12))
        
        # Clear previous plots
        for ax in self.axs.flat:
            ax.clear()
        
        model = self.model_dropdown.value
        epochs = self.simulator.epochs
        
        # Plot 1: Learning curves comparison
        ax1 = self.axs[0, 0]
        if model in self.simulator.learning_curves['baseline']:
            baseline = self.simulator.learning_curves['baseline'][model]
            distilled = self.simulator.learning_curves['distilled'][model]
            
            ax1.plot(epochs, baseline, 'b-', linewidth=2, label='Baseline')
            ax1.plot(epochs, distilled, 'r-', linewidth=3, label='Distilled')
            ax1.fill_between(epochs, baseline, distilled, alpha=0.2, color='green')
            ax1.set_xlabel('Training Epochs')
            ax1.set_ylabel('Performance')
            ax1.set_title(f'Learning Curves: {model}')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
        
        # Plot 2: KD gain over time
        ax2 = self.axs[0, 1]
        if model in self.simulator.kd_gains:
            gains = self.simulator.kd_gains[model]
            ax2.plot(epochs, gains, 'g-', linewidth=3)
            ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)
            ax2.fill_between(epochs, 0, gains, where=(gains > 0), alpha=0.3, color='green')
            ax2.fill_between(epochs, 0, gains, where=(gains < 0), alpha=0.3, color='red')
            ax2.set_xlabel('Training Epochs')
            ax2.set_ylabel('KD Gain (%)')
            ax2.set_title(f'Knowledge Distillation Gain')
            ax2.grid(True, alpha=0.3)
        
        # Plot 3: Model comparison (bar chart)
        ax3 = self.axs[1, 0]
        model_names = list(self.simulator.params.model_architectures.keys())
        mean_gains = [metrics.get(m, {}).get('mean_gain', 0) for m in model_names]
        
        colors = ['skyblue' if 'Hydra' not in m else 'red' for m in model_names]
        bars = ax3.bar(range(len(model_names)), mean_gains, color=colors)
        ax3.set_xticks(range(len(model_names)))
        ax3.set_xticklabels([m.replace('_', ' ') for m in model_names], rotation=45, ha='right')
        ax3.set_ylabel('Mean KD Gain (%)')
        ax3.set_title('Comparison Across Models')
        ax3.grid(True, alpha=0.3, axis='y')
        
        # Plot 4: Parameter effects
        ax4 = self.axs[1, 1]
        param_names = ['Complexity', 'Noise', 'KD Effect']
        current_values = [
            self.complexity_slider.value,
            self.noise_slider.value,
            self.kd_effect_slider.value
        ]
        
        ax4.bar(param_names, current_values, color=['blue', 'orange', 'green'])
        ax4.set_ylabel('Parameter Value')
        ax4.set_title('Current Parameter Settings')
        ax4.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.show()
    
    def display(self):
        """Display the dashboard"""
        # Arrange widgets
        controls = widgets.VBox([
            self.dataset_dropdown,
            self.model_dropdown,
            self.complexity_slider,
            self.noise_slider,
            self.kd_effect_slider,
            self.curve_type_dropdown,
            self.gain_method_dropdown,
            widgets.HBox([self.run_button, self.reset_button])
        ])
        
        dashboard = widgets.HBox([controls, self.output])
        display(dashboard)

# ===============================
# MAIN EXECUTION
# ===============================

def main():
    """Main execution with comprehensive analysis"""
    print("=" * 70)
    print("DYNAMIC KNOWLEDGE DISTILLATION SIMULATION FRAMEWORK")
    print("Performance metrics respond dynamically to all input changes")
    print("=" * 70)
    
    # Initialize simulator
    simulator = DynamicKDSimulator()
    
    # Run comprehensive analysis
    print("\n1. Running comprehensive analysis...")
    analysis_results = simulator.run_comprehensive_analysis()
    
    # Display key findings
    print("\n2. Key Findings:")
    print("-" * 40)
    
    # Best performing model
    metrics = analysis_results['basic']
    best_model = max(metrics.items(), key=lambda x: x[1]['mean_gain'])
    print(f"Best performing model: {best_model[0]}")
    print(f"  Mean KD Gain: {best_model[1]['mean_gain']:.2f}%")
    print(f"  Convergence Speedup: {best_model[1]['convergence_speed']:.1f}%")
    
    # Dataset effect
    dataset_effect = analysis_results['dataset_comparison']
    hardest_dataset = max(dataset_effect.items(), key=lambda x: x[1]['final_gain'])
    easiest_dataset = min(dataset_effect.items(), key=lambda x: x[1]['final_gain'])
    print(f"\nDataset Effect on Hydra-RAN:")
    print(f"  Highest gain on: {hardest_dataset[0]} ({hardest_dataset[1]['final_gain']:.2f}%)")
    print(f"  Lowest gain on: {easiest_dataset[0]} ({easiest_dataset[1]['final_gain']:.2f}%)")
    
    # Sensitivity analysis summary
    sensitivity = analysis_results['sensitivity']
    print(f"\nParameter Sensitivity (Hydra-RAN):")
    for param_name, results in sensitivity.items():
        gains = [r['metrics']['Hydra_RAN']['mean_gain'] for r in results.values()]
        sensitivity_range = max(gains) - min(gains)
        print(f"  {param_name.split('.')[-1]}: {sensitivity_range:.2f}% gain variation")
    
    # Correlation analysis
    correlations = analysis_results['correlations']['top_correlations']
    print(f"\nTop Parameter-Performance Correlations:")
    for (param1, param2), corr_value in correlations[:3].items():
        if param1 != param2 and 'gain' in param2:
            print(f"  {param1} ↔ {param2}: {corr_value:.3f}")
    
    # Create final visualization
    print("\n3. Creating comprehensive visualization...")
    fig, axs = plt.subplots(2, 3, figsize=(18, 12))
    
    # Plot 1: All model gains
    ax = axs[0, 0]
    for model_name, gains in simulator.kd_gains.items():
        style = 'k-' if 'Hydra' in model_name else '--'
        label = model_name.replace('_', ' ')
        ax.plot(simulator.epochs, gains, style, linewidth=2 if 'Hydra' in model_name else 1, label=label)
    ax.set_xlabel('Training Epochs')
    ax.set_ylabel('KD Gain (%)')
    ax.set_title('Knowledge Distillation Gains - All Models')
    ax.legend(fontsize=8, ncol=2)
    ax.grid(True, alpha=0.3)
    
    # Plot 2: Performance comparison
    ax = axs[0, 1]
    models = list(simulator.params.model_architectures.keys())
    baseline_final = [simulator.learning_curves['baseline'][m][-1] for m in models]
    distilled_final = [simulator.learning_curves['distilled'][m][-1] for m in models]
    
    x = np.arange(len(models))
    width = 0.35
    ax.bar(x - width/2, baseline_final, width, label='Baseline', alpha=0.7)
    ax.bar(x + width/2, distilled_final, width, label='Distilled', alpha=0.7)
    ax.set_xticks(x)
    ax.set_xticklabels([m.replace('_', '\n') for m in models], fontsize=8)
    ax.set_ylabel('Final Performance')
    ax.set_title('Final Performance Comparison')
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    
    # Plot 3: Gain distribution
    ax = axs[0, 2]
    gains_data = []
    labels = []
    for model_name, gains in simulator.kd_gains.items():
        gains_data.append(gains)
        labels.append(model_name.replace('_', ' '))
    
    ax.boxplot(gains_data, labels=labels)
    ax.set_ylabel('KD Gain (%)')
    ax.set_title('Gain Distribution Across Models')
    ax.grid(True, alpha=0.3, axis='y')
    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
    
    # Plot 4: Parameter sensitivity
    ax = axs[1, 0]
    param_names = ['Dataset\nComplexity', 'Noise\nLevel', 'KD\nEffectiveness']
    hydra_gains = []
    
    # Extract sensitivity results for Hydra-RAN
    for param_name in ['dataset_profiles.medium.complexity', 
                      'noise_config.noise_scaling_factor',
                      'kd_effectiveness.Hydra_RAN']:
        if param_name in sensitivity:
            results = sensitivity[param_name]
            gains = [r['metrics']['Hydra_RAN']['mean_gain'] for r in results.values()]
            hydra_gains.append(max(gains) - min(gains))
    
    ax.bar(param_names, hydra_gains, color=['blue', 'orange', 'green'])
    ax.set_ylabel('Gain Variation Range (%)')
    ax.set_title('Parameter Sensitivity for Hydra-RAN')
    ax.grid(True, alpha=0.3, axis='y')
    
    # Plot 5: Dataset comparison
    ax = axs[1, 1]
    dataset_names = list(dataset_effect.keys())
    dataset_gains = [dataset_effect[d]['final_gain'] for d in dataset_names]
    
    colors = plt.cm.viridis(np.linspace(0, 1, len(dataset_names)))
    ax.bar(dataset_names, dataset_gains, color=colors)
    ax.set_ylabel('Final KD Gain (%)')
    ax.set_title('Dataset Effect on Hydra-RAN Performance')
    ax.grid(True, alpha=0.3, axis='y')
    
    # Plot 6: Efficiency metrics
    ax = axs[1, 2]
    efficiency_metrics = ['Compute\nEfficiency', 'Convergence\nSpeed', 'Gain\nConsistency']
    hydra_values = [
        metrics['Hydra_RAN']['compute_efficiency_ratio'],
        metrics['Hydra_RAN']['convergence_speed'] / 100,
        metrics['Hydra_RAN']['gain_consistency']
    ]
    
    ax.bar(efficiency_metrics, hydra_values, color=['purple', 'brown', 'teal'])
    ax.set_ylabel('Normalized Value')
    ax.set_title('Hydra-RAN Efficiency Metrics')
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig('dynamic_kd_simulation_results.pdf', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Save results
    print("\n4. Saving results...")
    results_summary = {
        'simulation_parameters': asdict(simulator.params),
        'performance_metrics': simulator.performance_metrics,
        'analysis_summary': {
            'best_model': best_model[0],
            'best_mean_gain': best_model[1]['mean_gain'],
            'dataset_effects': dataset_effect,
            'parameter_sensitivity': {
                k: {val: r['metrics']['Hydra_RAN']['mean_gain'] 
                    for val, r in v.items()} 
                for k, v in sensitivity.items()
            }
        }
    }
    
    with open('simulation_results.json', 'w') as f:
        json.dump(results_summary, f, indent=2, default=str)
    
    print("\n5. Results Summary:")
    print("-" * 40)
    print(f"✓ Simulation completed successfully")
    print(f"✓ Results saved to: simulation_results.json")
    print(f"✓ Visualization saved to: dynamic_kd_simulation_results.pdf")
    print(f"✓ Best model: {best_model[0]} with {best_model[1]['mean_gain']:.2f}% mean gain")
    print(f"✓ Hydra-RAN outperforms baseline by {metrics['Hydra_RAN']['performance_improvement']:.2f}%")
    
    # Interactive dashboard option
    print("\n6. For interactive exploration, run:")
    print("   dashboard = InteractiveSimulationDashboard(simulator)")
    print("   dashboard.display()")
    
    return simulator, analysis_results

# ===============================
# EXAMPLE USAGE
# ===============================

if __name__ == "__main__":
    
    # Option 1: Run full analysis
    simulator, results = main()
    
    # Option 2: Interactive exploration
    # dashboard = InteractiveSimulationDashboard(simulator)
    # dashboard.display()
    
    # Option 3: Manual parameter exploration
    print("\n" + "=" * 70)
    print("MANUAL PARAMETER EXPLORATION EXAMPLES")
    print("=" * 70)
    
    print("\nExample 1: Change dataset complexity")
    print("  simulator.params.dataset_profiles['medium'].complexity = 2.0")
    print("  simulator.simulate_all_models()")
    print("  metrics = simulator.evaluate_performance()")
    print("  print(f\"Hydra-RAN gain: {metrics['Hydra_RAN']['mean_gain']:.2f}%\")")
    
    print("\nExample 2: Change noise characteristics")
    print("  simulator.params.noise_config['noise_type'] = NoiseType.BURSTY")
    print("  simulator.params.noise_config['burst_probability'] = 0.1")
    print("  simulator.simulate_all_models()")
    
    print("\nExample 3: Compare different KD methods")
    print("  for method in GainCalculationMethod:")
    print("    simulator.params.gain_calculation['method'] = method")
    print("    simulator.simulate_all_models()")
    print("    metrics = simulator.evaluate_performance()")
    print("    print(f\"{method.value}: {metrics['Hydra_RAN']['mean_gain']:.2f}%\")")
    
    print("\nExample 4: Run sensitivity analysis")
    print("  sensitivity = simulator.parameter_sensitivity_analysis(")
    print("      'kd_config.temperature', ")
    print("      np.linspace(1.0, 10.0, 10),")
    print("      ['Hydra_RAN', 'V2X_ViTv2']")
    print("  )")
    
    print("\nExample 5: Export results for further analysis")
    print("  import pandas as pd")
    print("  df = pd.DataFrame(simulator.performance_metrics).T")
    print("  df.to_csv('kd_performance_metrics.csv')")




# Knowledge Distillation Gain (%) vs Training Epochs


# -*- coding: utf-8 -*-
"""
Dynamic Parameter-Responsive Latency Simulation Framework
End-to-End Latency vs Vehicle Density with Real-Time Parameter Dependencies

Key Features:
1. Fully responsive to all parameter changes
2. Dynamic dependency graphs between parameters
3. Real-time performance recalculation
4. Interactive parameter exploration
5. Comprehensive sensitivity analysis

Author: Enhanced Dynamic Simulation Framework
IEEE Transactions on Communications - Enhanced Version
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import networkx as nx
from dataclasses import dataclass, field, asdict
from typing import Dict, List, Tuple, Optional, Callable, Any, Union
from enum import Enum, auto
import warnings
warnings.filterwarnings('ignore')
from scipy import stats
import seaborn as sns
from copy import deepcopy
import json
import ipywidgets as widgets
from IPython.display import display, clear_output

# ===============================
# IEEE TCOM Global Style
# ===============================
plt.rcParams.update({
    "font.family": "Times New Roman",
    "axes.linewidth": 1.2,
    "axes.edgecolor": "black",
    "axes.facecolor": "white",
    "figure.facecolor": "white",
})

# ===============================
# Enhanced Data Structures
# ===============================

class TrafficPattern(Enum):
    """Types of traffic patterns with dynamic characteristics."""
    UNIFORM = auto()
    BURSTY = auto()
    CONVOY = auto()
    PLATOON = auto()
    RANDOM = auto()
    SYNCHRONIZED = auto()
    CHAOTIC = auto()

class NetworkArchitecture(Enum):
    """Types of network architectures with efficiency factors."""
    CENTRALIZED = auto()
    DISTRIBUTED = auto()
    HYBRID = auto()
    EDGE_FOG = auto()
    CLOUD_EDGE = auto()
    MESH = auto()
    AD_HOC = auto()

class CommunicationTechnology(Enum):
    """Communication technologies with different characteristics."""
    LTE = auto()
    NR = auto()
    V2X = auto()
    DSRC = auto()
    WIFI = auto()
    SATELLITE = auto()
    HYBRID = auto()

@dataclass
class ParameterDependency:
    """Defines dependencies between parameters."""
    source_param: str
    target_param: str
    influence_factor: float  # -1 to 1
    function: Optional[Callable] = None
    description: str = ""
    
    def apply_dependency(self, source_value: float, current_target: float) -> float:
        """Apply dependency function."""
        if self.function:
            return self.function(source_value, current_target)
        else:
            return current_target * (1 + self.influence_factor * source_value)

@dataclass
class DynamicParameter:
    """A parameter that can change and affect other parameters."""
    name: str
    value: float
    min_value: float
    max_value: float
    default: float
    unit: str = ""
    category: str = ""
    sensitivity_factor: float = 1.0  # How much it affects performance
    dependencies: List[ParameterDependency] = field(default_factory=list)
    affects: List[str] = field(default_factory=list)  # Parameters this affects
    
    def update(self, new_value: float) -> bool:
        """Update parameter value with bounds checking."""
        if self.min_value <= new_value <= self.max_value:
            self.value = new_value
            return True
        return False

@dataclass
class DatasetProfile:
    """Dynamic dataset characteristics with computed metrics."""
    name: str = "urban_congested"
    traffic_pattern: TrafficPattern = TrafficPattern.BURSTY
    vehicle_density_range: Tuple[float, float] = (0.1, 1.0)
    density_points: int = 20
    average_speed: float = 30.0  # km/h
    speed_variance: float = 10.0  # km/h
    vehicle_spacing: float = 5.0  # meters
    lane_count: int = 4
    intersection_density: float = 0.8
    traffic_light_frequency: float = 2.0
    road_quality: float = 0.8  # 0-1
    weather_conditions: float = 0.7  # 0-1 (1=clear)
    time_of_day: float = 0.5  # 0-1 (0=night, 1=day)
    
    # Computed metrics
    _density_array: np.ndarray = field(init=False, repr=False)
    _computed_factors: Dict[str, float] = field(init=False, default_factory=dict)
    
    def __post_init__(self):
        """Initialize computed properties."""
        self._density_array = np.linspace(
            self.vehicle_density_range[0],
            self.vehicle_density_range[1],
            self.density_points
        )
        self._compute_factors()
    
    def _compute_factors(self):
        """Compute all traffic and environmental factors."""
        # Congestion factor (0-1, higher = more congested)
        congestion = 1 - (self.vehicle_spacing / 50.0)
        
        # Mobility factor (0-1, higher = more mobile)
        mobility = self.average_speed / 120.0
        
        # Environmental factor (0-1, higher = better conditions)
        environment = (self.road_quality * 0.4 + 
                      self.weather_conditions * 0.3 + 
                      self.time_of_day * 0.3)
        
        # Interaction intensity (0-1)
        interaction = (1 / (self.vehicle_spacing + 1)) * self.lane_count * 0.2
        
        # Traffic pattern factor
        pattern_factors = {
            TrafficPattern.UNIFORM: 0.2,
            TrafficPattern.BURSTY: 0.8,
            TrafficPattern.CONVOY: 0.4,
            TrafficPattern.PLATOON: 0.3,
            TrafficPattern.RANDOM: 0.6,
            TrafficPattern.SYNCHRONIZED: 0.1,
            TrafficPattern.CHAOTIC: 0.9
        }
        pattern = pattern_factors.get(self.traffic_pattern, 0.5)
        
        self._computed_factors = {
            'congestion_factor': congestion,
            'mobility_factor': mobility,
            'environment_factor': environment,
            'interaction_intensity': interaction,
            'pattern_complexity': pattern,
            'network_demand': congestion * interaction * 0.5,
            'communication_challenge': pattern * (1 - environment) * 0.7
        }
    
    def update_parameter(self, param_name: str, value: float):
        """Update a parameter and recompute factors."""
        if hasattr(self, param_name):
            setattr(self, param_name, value)
            self._compute_factors()
    
    def get_density_array(self) -> np.ndarray:
        """Get the vehicle density array."""
        return self._density_array
    
    def get_factors(self) -> Dict[str, float]:
        """Get computed traffic factors."""
        return self._computed_factors.copy()

@dataclass
class NetworkConfiguration:
    """Complete network configuration with dynamic parameters."""
    # Architecture parameters
    architecture: NetworkArchitecture = NetworkArchitecture.HYBRID
    technology: CommunicationTechnology = CommunicationTechnology.NR
    
    # Infrastructure parameters
    rsu_density: float = 0.5  # RSUs per km
    base_station_density: float = 0.3  # BS per km
    cloud_server_distance: float = 100.0  # km
    edge_node_density: float = 0.4  # Edge nodes per km
    
    # Communication parameters
    bandwidth: float = 100.0  # MHz
    bandwidth_variance: float = 10.0  # MHz
    transmission_power: float = 23.0  # dBm
    path_loss_exponent: float = 3.5
    interference_level: float = 0.2  # 0-1
    packet_loss_rate: float = 0.01  # 0-1
    
    # Processing parameters
    edge_compute_capacity: float = 10.0  # TFLOPs
    vehicle_compute_capacity: float = 0.5  # TFLOPs
    cloud_compute_capacity: float = 100.0  # TFLOPs
    processing_efficiency: float = 0.8  # 0-1
    
    # Quality of Service parameters
    reliability_threshold: float = 0.99  # 0-1
    availability: float = 0.999  # 0-1
    security_level: float = 0.8  # 0-1
    
    def compute_efficiency_factors(self) -> Dict[str, float]:
        """Compute network efficiency factors."""
        # Bandwidth efficiency (0-1)
        bw_efficiency = min(self.bandwidth / 1000.0, 1.0)
        
        # Coverage efficiency
        coverage = (self.rsu_density * 0.4 + 
                   self.base_station_density * 0.3 + 
                   self.edge_node_density * 0.3)
        
        # Compute efficiency
        compute = (self.edge_compute_capacity * 0.5 + 
                  self.vehicle_compute_capacity * 0.2 + 
                  self.cloud_compute_capacity * 0.3) / 100.0
        
        # Reliability factor
        reliability = (self.reliability_threshold * 0.4 + 
                      self.availability * 0.3 + 
                      self.security_level * 0.3)
        
        # Technology factor
        tech_factors = {
            CommunicationTechnology.LTE: 0.6,
            CommunicationTechnology.NR: 0.9,
            CommunicationTechnology.V2X: 0.8,
            CommunicationTechnology.DSRC: 0.7,
            CommunicationTechnology.WIFI: 0.5,
            CommunicationTechnology.SATELLITE: 0.4,
            CommunicationTechnology.HYBRID: 0.85
        }
        tech = tech_factors.get(self.technology, 0.7)
        
        # Architecture factor
        arch_factors = {
            NetworkArchitecture.CENTRALIZED: 0.5,
            NetworkArchitecture.DISTRIBUTED: 0.7,
            NetworkArchitecture.HYBRID: 0.85,
            NetworkArchitecture.EDGE_FOG: 0.8,
            NetworkArchitecture.CLOUD_EDGE: 0.75,
            NetworkArchitecture.MESH: 0.6,
            NetworkArchitecture.AD_HOC: 0.4
        }
        arch = arch_factors.get(self.architecture, 0.7)
        
        # Overall efficiency
        overall = (bw_efficiency * 0.3 + coverage * 0.2 + compute * 0.2 + 
                  reliability * 0.2 + tech * 0.05 + arch * 0.05)
        
        return {
            'bandwidth_efficiency': bw_efficiency,
            'coverage_efficiency': coverage,
            'compute_efficiency': compute,
            'reliability_factor': reliability,
            'technology_factor': tech,
            'architecture_factor': arch,
            'overall_efficiency': overall,
            'interference_impact': self.interference_level * 0.3,
            'packet_loss_impact': self.packet_loss_rate * 0.2
        }

@dataclass
class ApplicationProfile:
    """Application requirements and characteristics."""
    name: str = "safety_critical"
    
    # Timing requirements
    sensing_update_rate: float = 100.0  # Hz
    perception_processing_delay: float = 10.0  # ms
    decision_making_delay: float = 5.0  # ms
    control_update_rate: float = 50.0  # Hz
    
    # Quality requirements
    accuracy_requirement: float = 0.99  # 0-1
    reliability_requirement: float = 0.999  # 0-1
    safety_critical_threshold: float = 20.0  # ms
    
    # Resource requirements
    bandwidth_requirement: float = 10.0  # Mbps
    compute_requirement: float = 1.0  # TFLOPs
    memory_requirement: float = 2.0  # GB
    
    # Communication patterns
    message_size_small: float = 0.1  # MB
    message_size_medium: float = 1.0  # MB
    message_size_large: float = 10.0  # MB
    message_frequency: float = 100.0  # Hz
    
    def compute_requirements(self) -> Dict[str, float]:
        """Compute application requirements metrics."""
        # Latency budget
        total_latency_budget = (
            1000 / self.sensing_update_rate + 
            self.perception_processing_delay + 
            self.decision_making_delay + 
            1000 / self.control_update_rate
        )
        
        # Network demand
        network_demand = (
            self.message_frequency * 
            (self.message_size_small * 0.1 + 
             self.message_size_medium * 0.3 + 
             self.message_size_large * 0.6)
        )
        
        # Compute demand
        compute_demand = self.compute_requirement * self.message_frequency / 1000.0
        
        # Criticality factor
        criticality = 1.0 / (self.safety_critical_threshold / 100.0)
        
        return {
            'total_latency_budget': total_latency_budget,
            'network_demand_mbps': network_demand,
            'compute_demand_tflops': compute_demand,
            'criticality_factor': criticality,
            'stringency_index': (criticality * 0.4 + 
                                self.accuracy_requirement * 0.3 + 
                                self.reliability_requirement * 0.3),
            'update_cycle_ms': 1000 / self.sensing_update_rate,
            'control_cycle_ms': 1000 / self.control_update_rate
        }

@dataclass
class ApproachModel:
    """Mathematical model for each approach's performance."""
    name: str
    base_model: Callable  # Function that computes base latency
    
    # Model parameters
    parameters: Dict[str, float] = field(default_factory=lambda: {
        'base_latency': 30.0,
        'congestion_sensitivity': 1.0,
        'efficiency_factor': 1.0,
        'scaling_exponent': 2.0,
        'adaptive_gain': 0.0,
        'learning_rate': 0.1
    })
    
    # Enhancement functions
    enhancement_functions: Dict[str, Callable] = field(default_factory=dict)
    
    # Dependencies on external factors
    factor_weights: Dict[str, float] = field(default_factory=lambda: {
        'congestion_factor': 0.3,
        'network_efficiency': 0.25,
        'application_stringency': 0.2,
        'environment_factor': 0.15,
        'technology_factor': 0.1
    })
    
    def compute_latency(
        self,
        density: np.ndarray,
        dataset_factors: Dict[str, float],
        network_factors: Dict[str, float],
        app_requirements: Dict[str, float]
    ) -> np.ndarray:
        """Compute latency using the full model."""
        # Base latency calculation
        base = self.base_model(density, self.parameters)
        
        # Apply factor-weighted adjustments
        adjustments = []
        
        # Congestion adjustment
        congestion_adj = (1 + dataset_factors['congestion_factor'] * 
                         self.factor_weights['congestion_factor'])
        adjustments.append(congestion_adj)
        
        # Network efficiency adjustment
        network_adj = (1 - network_factors['overall_efficiency'] * 
                      self.factor_weights['network_efficiency'])
        adjustments.append(network_adj)
        
        # Application stringency adjustment
        app_adj = (1 + app_requirements['stringency_index'] * 
                  self.factor_weights['application_stringency'])
        adjustments.append(app_adj)
        
        # Environment adjustment
        env_adj = (1 - dataset_factors['environment_factor'] * 
                  self.factor_weights['environment_factor'])
        adjustments.append(env_adj)
        
        # Technology adjustment
        tech_adj = (1 - network_factors['technology_factor'] * 
                   self.factor_weights['technology_factor'])
        adjustments.append(tech_adj)
        
        # Combine all adjustments
        total_adjustment = np.prod(adjustments, axis=0) if isinstance(adjustments[0], np.ndarray) else np.prod(adjustments)
        
        # Apply enhancements if any
        if self.enhancement_functions:
            for func_name, func in self.enhancement_functions.items():
                base = func(base, density, self.parameters)
        
        # Final latency with bounds
        latency = base * total_adjustment
        return np.clip(latency, self.parameters['base_latency'] * 0.5, 1000)

# ===============================
# Dynamic Simulation Core
# ===============================

class DynamicParameterSystem:
    """Manages parameter dependencies and propagation."""
    
    def __init__(self):
        self.parameters: Dict[str, DynamicParameter] = {}
        self.dependency_graph = nx.DiGraph()
        self.change_history = []
        
    def add_parameter(self, param: DynamicParameter):
        """Add a parameter to the system."""
        self.parameters[param.name] = param
        self.dependency_graph.add_node(param.name)
        
        # Add dependencies
        for dep in param.dependencies:
            self.dependency_graph.add_edge(dep.source_param, param.name, 
                                          weight=dep.influence_factor,
                                          function=dep.function)
    
    def update_parameter(self, param_name: str, new_value: float) -> bool:
        """Update a parameter and propagate changes."""
        if param_name not in self.parameters:
            return False
        
        param = self.parameters[param_name]
        if not param.update(new_value):
            return False
        
        # Record change
        self.change_history.append({
            'parameter': param_name,
            'old_value': param.value,
            'new_value': new_value,
            'timestamp': len(self.change_history)
        })
        
        # Propagate changes through dependency graph
        affected_params = self._propagate_changes(param_name)
        
        return True
    
    def _propagate_changes(self, changed_param: str) -> List[str]:
        """Propagate changes through the dependency graph."""
        affected = []
        
        # Get all nodes reachable from changed parameter
        for target in nx.dfs_preorder_nodes(self.dependency_graph, changed_param):
            if target == changed_param:
                continue
            
            # Get all paths from changed_param to target
            try:
                # Apply dependencies along each path
                for path in nx.all_simple_paths(self.dependency_graph, changed_param, target):
                    current_value = self.parameters[target].value
                    
                    for i in range(len(path) - 1):
                        source = path[i]
                        dest = path[i + 1]
                        
                        edge_data = self.dependency_graph.get_edge_data(source, dest)
                        if edge_data:
                            dep_func = edge_data.get('function')
                            if dep_func:
                                current_value = dep_func(
                                    self.parameters[source].value,
                                    current_value
                                )
                            else:
                                influence = edge_data.get('weight', 0)
                                current_value *= (1 + influence * 
                                                self.parameters[source].value)
                    
                    # Update the target parameter
                    self.parameters[target].update(current_value)
                    affected.append(target)
                    
            except nx.NetworkXNoPath:
                continue
        
        return affected

class DynamicLatencySimulator:
    """Main simulator with fully dynamic parameter response."""
    
    def __init__(self):
        # Core components
        self.dataset = DatasetProfile()
        self.network = NetworkConfiguration()
        self.application = ApplicationProfile()
        self.param_system = DynamicParameterSystem()
        
        # Approach models
        self.approaches: Dict[str, ApproachModel] = {}
        self.results: Dict[str, Dict[str, Any]] = {}
        
        # Performance metrics
        self.metrics: Dict[str, Dict[str, float]] = {}
        
        # Initialize parameter system
        self._initialize_parameter_system()
        
        # Initialize approaches
        self._initialize_approaches()
    
    def _initialize_parameter_system(self):
        """Initialize the dynamic parameter system."""
        # Define core parameters
        core_params = [
            DynamicParameter(
                name="vehicle_density",
                value=0.5,
                min_value=0.1,
                max_value=1.0,
                default=0.5,
                unit="normalized",
                category="traffic"
            ),
            DynamicParameter(
                name="bandwidth",
                value=100.0,
                min_value=10.0,
                max_value=1000.0,
                default=100.0,
                unit="MHz",
                category="network"
            ),
            DynamicParameter(
                name="compute_capacity",
                value=10.0,
                min_value=0.1,
                max_value=100.0,
                default=10.0,
                unit="TFLOPs",
                category="compute"
            ),
            DynamicParameter(
                name="sensing_rate",
                value=100.0,
                min_value=1.0,
                max_value=1000.0,
                default=100.0,
                unit="Hz",
                category="application"
            )
        ]
        
        # Add parameters to system
        for param in core_params:
            self.param_system.add_parameter(param)
        
        # Define dependencies
        dependencies = [
            ParameterDependency(
                source_param="vehicle_density",
                target_param="network_demand",
                influence_factor=0.8,
                function=lambda density, demand: demand * (1 + density * 0.5)
            ),
            ParameterDependency(
                source_param="bandwidth",
                target_param="network_efficiency",
                influence_factor=0.6,
                function=lambda bw, efficiency: efficiency * (1 + np.log10(bw/10) * 0.1)
            ),
            ParameterDependency(
                source_param="compute_capacity",
                target_param="processing_delay",
                influence_factor=-0.7,
                function=lambda compute, delay: delay * (1 - np.log10(compute/0.1) * 0.15)
            )
        ]
        
        # Add dependency parameters
        for dep in dependencies:
            if dep.target_param not in self.param_system.parameters:
                target_param = DynamicParameter(
                    name=dep.target_param,
                    value=1.0,
                    min_value=0.0,
                    max_value=10.0,
                    default=1.0,
                    unit="factor",
                    category="derived"
                )
                self.param_system.add_parameter(target_param)
            
            self.param_system.dependency_graph.add_edge(
                dep.source_param,
                dep.target_param,
                weight=dep.influence_factor,
                function=dep.function
            )
    
    def _initialize_approaches(self):
        """Initialize all approach models."""
        
        # Helper functions for different approaches
        def exponential_latency(density, params):
            base = params['base_latency']
            sensitivity = params['congestion_sensitivity']
            return base * (1 + sensitivity * density ** params['scaling_exponent'])
        
        def adaptive_latency(density, params):
            base = params['base_latency']
            # Adaptive model that reduces growth at higher densities
            growth = params['congestion_sensitivity'] * density
            adaptive_term = params['adaptive_gain'] * np.exp(-params['learning_rate'] * density)
            return base * (1 + growth - adaptive_term)
        
        def hybrid_latency(density, params):
            # Combination of linear and saturation
            linear = params['base_latency'] * (1 + 0.5 * density)
            saturation = 100 / (1 + np.exp(-10 * (density - 0.5)))
            return np.maximum(linear, saturation) * params['efficiency_factor']
        
        # Define enhancement functions
        def compression_enhancement(latency, density, params):
            compression_ratio = params.get('compression_ratio', 0.5)
            return latency * (1 - compression_ratio * 0.3)
        
        def load_balancing_enhancement(latency, density, params):
            load_balance_gain = params.get('load_balance_gain', 0.2)
            return latency * (1 - load_balance_gain * density)
        
        # Create approach models
        self.approaches = {
            'Cooperative_Perception': ApproachModel(
                name='Cooperative Perception',
                base_model=exponential_latency,
                parameters={
                    'base_latency': 41.0,
                    'congestion_sensitivity': 3.5,
                    'efficiency_factor': 0.7,
                    'scaling_exponent': 2.0,
                    'adaptive_gain': 0.0,
                    'learning_rate': 0.0
                },
                factor_weights={
                    'congestion_factor': 0.4,
                    'network_efficiency': 0.2,
                    'application_stringency': 0.2,
                    'environment_factor': 0.1,
                    'technology_factor': 0.1
                }
            ),
            'V2X_ViTv2': ApproachModel(
                name='V2X-ViTv2',
                base_model=hybrid_latency,
                parameters={
                    'base_latency': 41.0,
                    'congestion_sensitivity': 3.0,
                    'efficiency_factor': 0.8,
                    'scaling_exponent': 1.8,
                    'adaptive_gain': 0.5,
                    'learning_rate': 2.0,
                    'compression_ratio': 0.3
                },
                enhancement_functions={'compression': compression_enhancement},
                factor_weights={
                    'congestion_factor': 0.3,
                    'network_efficiency': 0.25,
                    'application_stringency': 0.25,
                    'environment_factor': 0.1,
                    'technology_factor': 0.1
                }
            ),
            'Semantic_Communication': ApproachModel(
                name='Semantic Communication',
                base_model=exponential_latency,
                parameters={
                    'base_latency': 34.0,
                    'congestion_sensitivity': 2.2,
                    'efficiency_factor': 0.85,
                    'scaling_exponent': 1.5,
                    'adaptive_gain': 0.3,
                    'learning_rate': 1.5,
                    'compression_ratio': 0.6
                },
                enhancement_functions={'compression': compression_enhancement},
                factor_weights={
                    'congestion_factor': 0.25,
                    'network_efficiency': 0.3,
                    'application_stringency': 0.25,
                    'environment_factor': 0.1,
                    'technology_factor': 0.1
                }
            ),
            'Task_Oriented': ApproachModel(
                name='Task-Oriented',
                base_model=adaptive_latency,
                parameters={
                    'base_latency': 34.0,
                    'congestion_sensitivity': 1.9,
                    'efficiency_factor': 0.9,
                    'scaling_exponent': 1.3,
                    'adaptive_gain': 0.8,
                    'learning_rate': 3.0,
                    'load_balance_gain': 0.2
                },
                enhancement_functions={'load_balancing': load_balancing_enhancement},
                factor_weights={
                    'congestion_factor': 0.2,
                    'network_efficiency': 0.3,
                    'application_stringency': 0.3,
                    'environment_factor': 0.1,
                    'technology_factor': 0.1
                }
            ),
            'Hydra_RAN': ApproachModel(
                name='Hydra-RAN (Goal-Oriented)',
                base_model=adaptive_latency,
                parameters={
                    'base_latency': 28.0,
                    'congestion_sensitivity': 1.2,
                    'efficiency_factor': 0.95,
                    'scaling_exponent': 1.1,
                    'adaptive_gain': 2.0,
                    'learning_rate': 5.0,
                    'compression_ratio': 0.8,
                    'load_balance_gain': 0.4,
                    'prediction_gain': 0.3
                },
                enhancement_functions={
                    'compression': compression_enhancement,
                    'load_balancing': load_balancing_enhancement,
                    'prediction': lambda lat, den, params: lat * (1 - params.get('prediction_gain', 0) * 0.2)
                },
                factor_weights={
                    'congestion_factor': 0.15,
                    'network_efficiency': 0.35,
                    'application_stringency': 0.3,
                    'environment_factor': 0.1,
                    'technology_factor': 0.1
                }
            )
        }
    
    def update_parameter(self, param_path: str, new_value: float) -> Dict[str, Any]:
        """
        Update any parameter in the system and trigger recalculation.
        
        Args:
            param_path: Path to parameter (e.g., "dataset.average_speed")
            new_value: New parameter value
        
        Returns:
            Dictionary with update results
        """
        update_result = {
            'success': False,
            'affected_parameters': [],
            'performance_changes': {}
        }
        
        # Parse parameter path
        parts = param_path.split('.')
        
        try:
            if len(parts) == 1:
                # Direct parameter in param system
                success = self.param_system.update_parameter(parts[0], new_value)
                update_result['success'] = success
                if success:
                    update_result['affected_parameters'] = self.param_system._propagate_changes(parts[0])
            elif len(parts) == 2:
                # Component parameter
                component, param = parts
                if component == 'dataset' and hasattr(self.dataset, param):
                    old_value = getattr(self.dataset, param)
                    self.dataset.update_parameter(param, new_value)
                    update_result['success'] = True
                    update_result['affected_parameters'] = ['dataset_factors']
                elif component == 'network' and hasattr(self.network, param):
                    setattr(self.network, param, new_value)
                    update_result['success'] = True
                    update_result['affected_parameters'] = ['network_factors']
                elif component == 'application' and hasattr(self.application, param):
                    setattr(self.application, param, new_value)
                    update_result['success'] = True
                    update_result['affected_parameters'] = ['app_requirements']
                elif component == 'approach':
                    # Update approach parameter
                    approach_name, approach_param = param.split('.')
                    if approach_name in self.approaches:
                        self.approaches[approach_name].parameters[approach_param] = new_value
                        update_result['success'] = True
                        update_result['affected_parameters'] = [f'approach.{approach_name}']
            
            # Recalculate performance if any parameters changed
            if update_result['success']:
                self.run_simulation()
                update_result['performance_changes'] = self._calculate_performance_changes()
                
        except Exception as e:
            update_result['error'] = str(e)
        
        return update_result
    
    def run_simulation(self):
        """Run simulation with current parameters."""
        # Get current factors
        density = self.dataset.get_density_array()
        dataset_factors = self.dataset.get_factors()
        network_factors = self.network.compute_efficiency_factors()
        app_requirements = self.application.compute_requirements()
        
        # Calculate latency for each approach
        self.results = {}
        for name, model in self.approaches.items():
            latency = model.compute_latency(
                density, dataset_factors, network_factors, app_requirements
            )
            
            # Calculate performance metrics
            metrics = self._calculate_metrics(name, latency, density)
            
            self.results[name] = {
                'latency': latency,
                'density': density,
                'metrics': metrics,
                'model': model
            }
        
        # Update overall metrics
        self.metrics = self._calculate_overall_metrics()
    
    def _calculate_metrics(self, approach_name: str, latency: np.ndarray, density: np.ndarray) -> Dict[str, float]:
        """Calculate comprehensive performance metrics."""
        safety_threshold = self.application.safety_critical_threshold
        
        # Basic statistics
        metrics = {
            'average_latency': np.mean(latency),
            'median_latency': np.median(latency),
            'max_latency': np.max(latency),
            'min_latency': np.min(latency),
            'std_latency': np.std(latency),
            'latency_growth': (latency[-1] - latency[0]) / latency[0] * 100,
            'latency_variability': np.std(np.diff(latency)),
        }
        
        # Threshold-based metrics
        metrics.update({
            'safety_compliance_rate': np.mean(latency <= safety_threshold) * 100,
            'density_at_safety_threshold': self._find_threshold_density(density, latency, safety_threshold),
            'density_at_20ms': self._find_threshold_density(density, latency, 20),
            'density_at_100ms': self._find_threshold_density(density, latency, 100),
            'worst_case_latency': np.percentile(latency, 95),
            'best_case_latency': np.percentile(latency, 5),
        })
        
        # Efficiency metrics
        latency_area = np.trapz(latency, density)
        ideal_area = np.trapz(np.full_like(latency, latency[0]), density)
        metrics['efficiency_score'] = (1 - latency_area / (ideal_area * 10)) * 100
        
        return metrics
    
    def _find_threshold_density(self, density: np.ndarray, latency: np.ndarray, threshold: float) -> float:
        """Find density at which latency exceeds threshold."""
        idx = np.where(latency > threshold)[0]
        if len(idx) > 0:
            return float(density[idx[0]])
        return float(density[-1])
    
    def _calculate_overall_metrics(self) -> Dict[str, Dict[str, float]]:
        """Calculate overall performance metrics across all approaches."""
        overall = {}
        
        for name, result in self.results.items():
            overall[name] = result['metrics']
        
        # Comparative metrics
        if len(self.results) > 1:
            best_latency = min(overall.values(), key=lambda x: x['average_latency'])
            worst_latency = max(overall.values(), key=lambda x: x['average_latency'])
            
            overall['comparative'] = {
                'best_approach': min(overall.items(), key=lambda x: x[1]['average_latency'])[0],
                'worst_approach': max(overall.items(), key=lambda x: x[1]['average_latency'])[0],
                'latency_range': worst_latency['average_latency'] - best_latency['average_latency'],
                'improvement_over_worst': (
                    (worst_latency['average_latency'] - best_latency['average_latency']) / 
                    worst_latency['average_latency'] * 100
                )
            }
        
        return overall
    
    def _calculate_performance_changes(self) -> Dict[str, Dict[str, float]]:
        """Calculate performance changes after parameter update."""
        # This would compare with previous results
        # For simplicity, return current metrics
        return {name: data['metrics'] for name, data in self.results.items()}
    
    def get_sensitivity_matrix(self) -> pd.DataFrame:
        """Calculate sensitivity of performance to parameter changes."""
        # Store current state
        original_state = {
            'dataset': deepcopy(self.dataset),
            'network': deepcopy(self.network),
            'application': deepcopy(self.application),
            'approaches': deepcopy(self.approaches)
        }
        
        # Define parameters to test
        test_params = [
            ('dataset.average_speed', 30, 100),
            ('dataset.vehicle_spacing', 2, 50),
            ('network.bandwidth', 10, 1000),
            ('network.compute_capacity', 1, 100),
            ('application.sensing_update_rate', 10, 1000)
        ]
        
        sensitivity_data = []
        
        for param_path, min_val, max_val in test_params:
            # Test multiple values
            test_values = np.linspace(min_val, max_val, 5)
            
            for value in test_values:
                # Update parameter
                self.update_parameter(param_path, value)
                
                # Record performance
                for approach_name, result in self.results.items():
                    sensitivity_data.append({
                        'parameter': param_path,
                        'value': value,
                        'approach': approach_name,
                        'average_latency': result['metrics']['average_latency'],
                        'safety_compliance': result['metrics']['safety_compliance_rate']
                    })
        
        # Restore original state
        self.dataset = original_state['dataset']
        self.network = original_state['network']
        self.application = original_state['application']
        self.approaches = original_state['approaches']
        self.run_simulation()
        
        return pd.DataFrame(sensitivity_data)
    
    def plot_results(self, save_path: Optional[str] = None):
        """Create comprehensive visualization of results."""
        fig = plt.figure(figsize=(20, 12))
        
        # Create subplots
        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
        
        # Plot 1: Latency vs Density
        ax1 = fig.add_subplot(gs[0, :2])
        self._plot_latency_curves(ax1)
        
        # Plot 2: Performance Comparison (Bar chart)
        ax2 = fig.add_subplot(gs[0, 2])
        self._plot_performance_bars(ax2)
        
        # Plot 3: Parameter Sensitivity
        ax3 = fig.add_subplot(gs[1, 0])
        self._plot_sensitivity_heatmap(ax3)
        
        # Plot 4: Threshold Analysis
        ax4 = fig.add_subplot(gs[1, 1])
        self._plot_threshold_analysis(ax4)
        
        # Plot 5: Efficiency Metrics
        ax5 = fig.add_subplot(gs[1, 2])
        self._plot_efficiency_radar(ax5)
        
        # Plot 6: Current Parameter State
        ax6 = fig.add_subplot(gs[2, 0])
        self._plot_parameter_state(ax6)
        
        # Plot 7: Performance Distribution
        ax7 = fig.add_subplot(gs[2, 1])
        self._plot_performance_distribution(ax7)
        
        # Plot 8: Improvement Analysis
        ax8 = fig.add_subplot(gs[2, 2])
        self._plot_improvement_analysis(ax8)
        
        plt.suptitle(
            f"Dynamic Latency Simulation Results\n"
            f"Scenario: {self.dataset.name} | "
            f"Network: {self.network.architecture.name} | "
            f"Application: {self.application.name}",
            fontsize=16,
            fontweight='bold'
        )
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        plt.show()
    
    def _plot_latency_curves(self, ax):
        """Plot latency vs density curves."""
        colors = plt.cm.tab10(np.linspace(0, 1, len(self.results)))
        
        for (name, result), color in zip(self.results.items(), colors):
            ax.plot(
                result['density'],
                result['latency'],
                label=name,
                linewidth=3 if 'Hydra' in name else 2,
                color=color,
                alpha=0.8
            )
        
        # Add safety threshold
        safety_threshold = self.application.safety_critical_threshold
        ax.axhline(
            y=safety_threshold,
            color='red',
            linestyle='--',
            alpha=0.5,
            label=f'Safety Threshold ({safety_threshold} ms)'
        )
        
        ax.set_xlabel('Vehicle Density (Normalized)')
        ax.set_ylabel('End-to-End Latency (ms)')
        ax.set_title('Latency vs Vehicle Density')
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=8, loc='upper left')
    
    def _plot_performance_bars(self, ax):
        """Plot performance comparison bar chart."""
        approaches = list(self.results.keys())
        avg_latencies = [self.results[name]['metrics']['average_latency'] for name in approaches]
        
        colors = ['red' if 'Hydra' in name else 'blue' for name in approaches]
        
        bars = ax.barh(approaches, avg_latencies, color=colors, alpha=0.7)
        ax.set_xlabel('Average Latency (ms)')
        ax.set_title('Performance Comparison')
        ax.grid(True, alpha=0.3, axis='x')
        
        # Add value labels
        for bar, lat in zip(bars, avg_latencies):
            ax.text(
                bar.get_width() + 1,
                bar.get_y() + bar.get_height()/2,
                f'{lat:.1f}',
                va='center',
                fontsize=9
            )
    
    def _plot_sensitivity_heatmap(self, ax):
        """Plot parameter sensitivity heatmap."""
        sensitivity_df = self.get_sensitivity_matrix()
        
        if not sensitivity_df.empty:
            # Pivot for heatmap
            pivot_df = sensitivity_df.pivot_table(
                index='parameter',
                columns='approach',
                values='average_latency',
                aggfunc='mean'
            )
            
            # Normalize
            normalized = (pivot_df - pivot_df.min()) / (pivot_df.max() - pivot_df.min())
            
            im = ax.imshow(normalized.values, cmap='viridis', aspect='auto')
            ax.set_xticks(range(len(normalized.columns)))
            ax.set_xticklabels(normalized.columns, rotation=45, ha='right')
            ax.set_yticks(range(len(normalized.index)))
            ax.set_yticklabels([p.split('.')[-1] for p in normalized.index])
            
            ax.set_title('Parameter Sensitivity')
            plt.colorbar(im, ax=ax, label='Normalized Latency Impact')
    
    def _plot_threshold_analysis(self, ax):
        """Plot density at which thresholds are exceeded."""
        thresholds = [20, 50, 100, self.application.safety_critical_threshold]
        approaches = list(self.results.keys())
        
        data = []
        for threshold in thresholds:
            row = []
            for approach in approaches:
                metric_name = f'density_at_{int(threshold)}ms' if threshold != self.application.safety_critical_threshold else 'density_at_safety_threshold'
                if metric_name in self.results[approach]['metrics']:
                    row.append(self.results[approach]['metrics'][metric_name])
                else:
                    row.append(0)
            data.append(row)
        
        x = np.arange(len(approaches))
        width = 0.2
        
        for i, (threshold, row) in enumerate(zip(thresholds, data)):
            offset = (i - len(thresholds)/2) * width
            ax.bar(x + offset, row, width, label=f'{threshold} ms')
        
        ax.set_xlabel('Approach')
        ax.set_ylabel('Density at Threshold')
        ax.set_title('Threshold Exceedance Analysis')
        ax.set_xticks(x)
        ax.set_xticklabels([a[:10] for a in approaches], rotation=45)
        ax.legend(fontsize=8)
        ax.grid(True, alpha=0.3, axis='y')
    
    def _plot_efficiency_radar(self, ax):
        """Plot radar chart of efficiency metrics."""
        if not self.results:
            return
        
        # Select metrics for radar chart
        metrics_to_plot = ['efficiency_score', 'safety_compliance_rate', 
                          'latency_growth', 'latency_variability']
        
        approaches = list(self.results.keys())
        num_vars = len(metrics_to_plot)
        
        # Compute angles
        angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
        angles += angles[:1]  # Close the loop
        
        for approach in approaches:
            values = []
            for metric in metrics_to_plot:
                if metric in self.results[approach]['metrics']:
                    # Normalize values
                    val = self.results[approach]['metrics'][metric]
                    # Different normalization for different metrics
                    if metric in ['efficiency_score', 'safety_compliance_rate']:
                        values.append(val / 100)
                    elif metric == 'latency_growth':
                        values.append(1 - min(val / 500, 1))
                    else:
                        values.append(1 - min(val / 50, 1))
                else:
                    values.append(0)
            
            values += values[:1]  # Close the loop
            
            ax.plot(angles, values, 'o-', linewidth=2, label=approach[:15])
            ax.fill(angles, values, alpha=0.1)
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels([m.replace('_', '\n') for m in metrics_to_plot])
        ax.set_title('Efficiency Radar Chart')
        ax.legend(fontsize=7, loc='upper right', bbox_to_anchor=(1.3, 1.0))
    
    def _plot_parameter_state(self, ax):
        """Plot current parameter state."""
        # Gather current parameters
        params = {
            'Vehicle Speed': self.dataset.average_speed,
            'Vehicle Spacing': self.dataset.vehicle_spacing,
            'Bandwidth': self.network.bandwidth,
            'Compute Capacity': self.network.edge_compute_capacity,
            'Sensing Rate': self.application.sensing_update_rate
        }
        
        names = list(params.keys())
        values = list(params.values())
        
        # Normalize for visualization
        max_vals = [120, 50, 1000, 100, 1000]  # Max possible values
        normalized = [v/max_v for v, max_v in zip(values, max_vals)]
        
        ax.barh(names, normalized, color='steelblue', alpha=0.7)
        ax.set_xlabel('Normalized Value')
        ax.set_title('Current Parameter State')
        ax.set_xlim(0, 1)
        ax.grid(True, alpha=0.3, axis='x')
        
        # Add actual values
        for i, (name, value) in enumerate(params.items()):
            ax.text(
                normalized[i] + 0.02,
                i,
                f'{value:.1f}',
                va='center',
                fontsize=9
            )
    
    def _plot_performance_distribution(self, ax):
        """Plot performance distribution box plot."""
        if not self.results:
            return
        
        data = []
        labels = []
        
        for name, result in self.results.items():
            data.append(result['latency'])
            labels.append(name[:10])
        
        ax.boxplot(data, labels=labels, showmeans=True)
        ax.set_ylabel('Latency (ms)')
        ax.set_title('Performance Distribution')
        ax.grid(True, alpha=0.3, axis='y')
        ax.tick_params(axis='x', rotation=45)
    
    def _plot_improvement_analysis(self, ax):
        """Plot improvement analysis."""
        if len(self.results) < 2:
            return
        
        # Find baseline (worst) and proposed (best)
        approaches = list(self.results.keys())
        baseline = approaches[0]  # First approach as baseline
        proposed = 'Hydra_RAN' if 'Hydra_RAN' in approaches else approaches[-1]
        
        if baseline == proposed:
            return
        
        baseline_latency = self.results[baseline]['latency']
        proposed_latency = self.results[proposed]['latency']
        density = self.results[baseline]['density']
        
        # Calculate improvement percentage
        improvement = (baseline_latency - proposed_latency) / baseline_latency * 100
        
        ax.plot(density, improvement, 'g-', linewidth=2)
        ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)
        ax.fill_between(density, 0, improvement, where=(improvement > 0), 
                       alpha=0.3, color='green', label='Improvement')
        ax.fill_between(density, 0, improvement, where=(improvement < 0), 
                       alpha=0.3, color='red', label='Degradation')
        
        ax.set_xlabel('Vehicle Density')
        ax.set_ylabel('Improvement (%)')
        ax.set_title(f'Improvement: {proposed} vs {baseline}')
        ax.grid(True, alpha=0.3)
        ax.legend()

# ===============================
# Interactive Dashboard
# ===============================

class InteractiveDashboard:
    """Interactive dashboard for parameter exploration."""
    
    def __init__(self, simulator: DynamicLatencySimulator):
        self.simulator = simulator
        self.output = widgets.Output()
        
        # Create widgets
        self._create_parameter_widgets()
        self._create_control_widgets()
        
        # Assemble dashboard
        self.dashboard = self._assemble_dashboard()
    
    def _create_parameter_widgets(self):
        """Create parameter adjustment widgets."""
        
        # Dataset parameters
        self.dataset_widgets = {
            'average_speed': widgets.FloatSlider(
                value=self.simulator.dataset.average_speed,
                min=10,
                max=120,
                step=5,
                description='Avg Speed (km/h):',
                continuous_update=False
            ),
            'vehicle_spacing': widgets.FloatSlider(
                value=self.simulator.dataset.vehicle_spacing,
                min=2,
                max=50,
                step=1,
                description='Spacing (m):',
                continuous_update=False
            ),
            'lane_count': widgets.IntSlider(
                value=self.simulator.dataset.lane_count,
                min=1,
                max=8,
                step=1,
                description='Lanes:',
                continuous_update=False
            )
        }
        
        # Network parameters
        self.network_widgets = {
            'bandwidth': widgets.FloatSlider(
                value=self.simulator.network.bandwidth,
                min=10,
                max=1000,
                step=10,
                description='Bandwidth (MHz):',
                continuous_update=False
            ),
            'edge_compute_capacity': widgets.FloatSlider(
                value=self.simulator.network.edge_compute_capacity,
                min=0.1,
                max=100,
                step=1,
                description='Edge Compute (TFLOPs):',
                continuous_update=False
            ),
            'rsu_density': widgets.FloatSlider(
                value=self.simulator.network.rsu_density,
                min=0.1,
                max=2.0,
                step=0.1,
                description='RSU Density (/km):',
                continuous_update=False
            )
        }
        
        # Application parameters
        self.application_widgets = {
            'sensing_update_rate': widgets.FloatSlider(
                value=self.simulator.application.sensing_update_rate,
                min=1,
                max=1000,
                step=10,
                description='Sensing Rate (Hz):',
                continuous_update=False
            ),
            'safety_critical_threshold': widgets.FloatSlider(
                value=self.simulator.application.safety_critical_threshold,
                min=10,
                max=500,
                step=10,
                description='Safety Threshold (ms):',
                continuous_update=False
            )
        }
        
        # Set up change handlers
        for widget_dict in [self.dataset_widgets, self.network_widgets, self.application_widgets]:
            for param_name, widget in widget_dict.items():
                widget.observe(lambda change, pn=param_name: self._on_parameter_change(pn, change), 'value')
    
    def _create_control_widgets(self):
        """Create control widgets."""
        self.run_button = widgets.Button(
            description='Run Simulation',
            button_style='success',
            icon='play'
        )
        self.run_button.on_click(self._run_simulation)
        
        self.reset_button = widgets.Button(
            description='Reset Parameters',
            button_style='warning',
            icon='refresh'
        )
        self.reset_button.on_click(self._reset_parameters)
        
        self.scenario_dropdown = widgets.Dropdown(
            options=['Urban Congested', 'Highway Free Flow', 'City Center', 'Suburban'],
            value='Urban Congested',
            description='Scenario:'
        )
        self.scenario_dropdown.observe(self._on_scenario_change, 'value')
    
    def _assemble_dashboard(self):
        """Assemble the complete dashboard."""
        # Create tabs for different parameter categories
        dataset_tab = widgets.VBox(list(self.dataset_widgets.values()))
        network_tab = widgets.VBox(list(self.network_widgets.values()))
        application_tab = widgets.VBox(list(self.application_widgets.values()))
        
        parameter_tabs = widgets.Tab()
        parameter_tabs.children = [dataset_tab, network_tab, application_tab]
        parameter_tabs.set_title(0, 'Traffic Parameters')
        parameter_tabs.set_title(1, 'Network Parameters')
        parameter_tabs.set_title(2, 'Application Parameters')
        
        # Control panel
        control_panel = widgets.VBox([
            self.scenario_dropdown,
            widgets.HBox([self.run_button, self.reset_button])
        ])
        
        # Assemble layout
        left_panel = widgets.VBox([
            widgets.Label('Parameter Controls', fontweight='bold'),
            parameter_tabs,
            widgets.HR(),
            control_panel
        ])
        
        # Full dashboard
        dashboard = widgets.HBox([
            left_panel,
            self.output
        ], layout=widgets.Layout(width='100%'))
        
        return dashboard
    
    def _on_parameter_change(self, param_name: str, change):
        """Handle parameter change."""
        param_path = f"dataset.{param_name}"
        
        # Determine correct component
        if param_name in self.network_widgets:
            param_path = f"network.{param_name}"
        elif param_name in self.application_widgets:
            param_path = f"application.{param_name}"
        
        # Update parameter in simulator
        result = self.simulator.update_parameter(param_path, change['new'])
        
        # Update display if simulation was successful
        if result.get('success', False):
            with self.output:
                clear_output(wait=True)
                self.simulator.plot_results()
    
    def _on_scenario_change(self, change):
        """Handle scenario change."""
        scenario_map = {
            'Urban Congested': {
                'dataset.average_speed': 30,
                'dataset.vehicle_spacing': 5,
                'dataset.lane_count': 4
            },
            'Highway Free Flow': {
                'dataset.average_speed': 100,
                'dataset.vehicle_spacing': 50,
                'dataset.lane_count': 3
            },
            'City Center': {
                'dataset.average_speed': 20,
                'dataset.vehicle_spacing': 2,
                'dataset.lane_count': 6
            },
            'Suburban': {
                'dataset.average_speed': 50,
                'dataset.vehicle_spacing': 20,
                'dataset.lane_count': 2
            }
        }
        
        if change['new'] in scenario_map:
            scenario = scenario_map[change['new']]
            for param_path, value in scenario.items():
                self.simulator.update_parameter(param_path, value)
            
            # Update widget values
            for param, value in scenario.items():
                component, param_name = param_path.split('.')
                if component == 'dataset' and param_name in self.dataset_widgets:
                    self.dataset_widgets[param_name].value = value
            
            with self.output:
                clear_output(wait=True)
                self.simulator.plot_results()
    
    def _run_simulation(self, b):
        """Run full simulation."""
        with self.output:
            clear_output(wait=True)
            print("Running simulation with current parameters...")
            self.simulator.run_simulation()
            self.simulator.plot_results()
            print("\nSimulation complete!")
    
    def _reset_parameters(self, b):
        """Reset parameters to defaults."""
        # Reset simulator
        self.simulator = DynamicLatencySimulator()
        
        # Reset widget values
        for param_name, widget in self.dataset_widgets.items():
            widget.value = getattr(self.simulator.dataset, param_name)
        
        for param_name, widget in self.network_widgets.items():
            widget.value = getattr(self.simulator.network, param_name)
        
        for param_name, widget in self.application_widgets.items():
            widget.value = getattr(self.simulator.application, param_name)
        
        with self.output:
            clear_output(wait=True)
            print("Parameters reset to defaults.")
            self.simulator.plot_results()
    
    def display(self):
        """Display the dashboard."""
        display(self.dashboard)
        # Initial plot
        with self.output:
            self.simulator.plot_results()

# ===============================
# Main Execution
# ===============================

def main():
    """Main execution function."""
    print("=" * 80)
    print("DYNAMIC PARAMETER-RESPONSIVE LATENCY SIMULATION FRAMEWORK")
    print("IEEE Transactions on Communications - Enhanced Version")
    print("=" * 80)
    
    # Create simulator
    simulator = DynamicLatencySimulator()
    
    # Run initial simulation
    print("\nRunning initial simulation with default parameters...")
    simulator.run_simulation()
    
    # Display initial results
    print("\nInitial Performance Summary:")
    print("-" * 80)
    
    for name, result in simulator.results.items():
        metrics = result['metrics']
        print(f"\n{name}:")
        print(f"  Average Latency: {metrics['average_latency']:.1f} ms")
        print(f"  Safety Compliance: {metrics['safety_compliance_rate']:.1f}%")
        print(f"  Efficiency Score: {metrics['efficiency_score']:.1f}")
        print(f"  Max Latency: {metrics['max_latency']:.1f} ms")
    
    # Demonstrate parameter responsiveness
    print("\n" + "=" * 80)
    print("DEMONSTRATING PARAMETER RESPONSIVENESS")
    print("=" * 80)
    
    # Example 1: Increase vehicle density effect
    print("\n1. Increasing vehicle density from 0.5 to 0.8...")
    simulator.update_parameter("dataset.vehicle_spacing", 2)  # More dense
    hydra_metrics = simulator.results['Hydra_RAN']['metrics']
    print(f"   Hydra-RAN average latency: {hydra_metrics['average_latency']:.1f} ms")
    print(f"   Safety compliance: {hydra_metrics['safety_compliance_rate']:.1f}%")
    
    # Example 2: Improve network bandwidth
    print("\n2. Doubling network bandwidth to 200 MHz...")
    simulator.update_parameter("network.bandwidth", 200)
    hydra_metrics = simulator.results['Hydra_RAN']['metrics']
    print(f"   Hydra-RAN average latency: {hydra_metrics['average_latency']:.1f} ms")
    
    # Example 3: Change application requirements
    print("\n3. Changing to high-frequency sensing (500 Hz)...")
    simulator.update_parameter("application.sensing_update_rate", 500)
    hydra_metrics = simulator.results['Hydra_RAN']['metrics']
    print(f"   Hydra-RAN average latency: {hydra_metrics['average_latency']:.1f} ms")
    print(f"   Safety compliance: {hydra_metrics['safety_compliance_rate']:.1f}%")
    
    # Run sensitivity analysis
    print("\n" + "=" * 80)
    print("RUNNING SENSITIVITY ANALYSIS")
    print("=" * 80)
    
    sensitivity_df = simulator.get_sensitivity_matrix()
    if not sensitivity_df.empty:
        print("\nTop parameter impacts on average latency:")
        grouped = sensitivity_df.groupby('parameter')['average_latency'].std().sort_values(ascending=False)
        for param, impact in grouped.head(5).items():
            print(f"  {param}: ±{impact:.1f} ms variation")
    
    # Create comprehensive visualization
    print("\n" + "=" * 80)
    print("GENERATING COMPREHENSIVE VISUALIZATION")
    print("=" * 80)
    
    simulator.plot_results(save_path='dynamic_latency_analysis.pdf')
    
    # Save results
    results_summary = {
        'parameters': {
            'dataset': asdict(simulator.dataset),
            'network': asdict(simulator.network),
            'application': asdict(simulator.application)
        },
        'performance_metrics': simulator.metrics,
        'sensitivity_analysis': sensitivity_df.to_dict('records') if not sensitivity_df.empty else {}
    }
    
    with open('simulation_results.json', 'w') as f:
        json.dump(results_summary, f, indent=2, default=str)
    
    print("\n" + "=" * 80)
    print("SIMULATION COMPLETE")
    print("=" * 80)
    print("\nSummary:")
    print(f"✓ Results saved to: simulation_results.json")
    print(f"✓ Visualization saved to: dynamic_latency_analysis.pdf")
    print(f"✓ Best approach: {simulator.metrics.get('comparative', {}).get('best_approach', 'N/A')}")
    print(f"✓ Worst approach: {simulator.metrics.get('comparative', {}).get('worst_approach', 'N/A')}")
    print(f"✓ Overall improvement: {simulator.metrics.get('comparative', {}).get('improvement_over_worst', 0):.1f}%")
    
    # Interactive dashboard option
    print("\nFor interactive parameter exploration:")
    print("  dashboard = InteractiveDashboard(simulator)")
    print("  dashboard.display()")
    
    return simulator

# ===============================
# Example Usage Patterns
# ===============================

if __name__ == "__main__":
    
    # Option 1: Run full analysis
    simulator = main()
    
    # Option 2: Interactive exploration
    # dashboard = InteractiveDashboard(simulator)
    # dashboard.display()
    
    # Option 3: Programmatic parameter exploration
    print("\n" + "=" * 80)
    print("PROGRAMMATIC PARAMETER EXPLORATION EXAMPLES")
    print("=" * 80)
    
    print("\nExample 1: Test different traffic scenarios")
    print("  scenarios = ['Urban Congested', 'Highway Free Flow', 'City Center']")
    print("  for scenario in scenarios:")
    print("    simulator.update_parameter('dataset.average_speed', SPEED_MAP[scenario])")
    print("    simulator.run_simulation()")
    print("    print(f'{scenario}: {simulator.results['Hydra_RAN']['metrics']['average_latency']:.1f} ms')")
    
    print("\nExample 2: Explore bandwidth vs compute tradeoff")
    print("  bandwidth_range = [50, 100, 200, 500]")
    print("  compute_range = [5, 10, 20, 50]")
    print("  for bw in bandwidth_range:")
    print("    for comp in compute_range:")
    print("      simulator.update_parameter('network.bandwidth', bw)")
    print("      simulator.update_parameter('network.edge_compute_capacity', comp)")
    print("      simulator.run_simulation()")
    print("      # Analyze results...")
    
    print("\nExample 3: Sensitivity to application requirements")
    print("  sensing_rates = [10, 30, 100, 300, 1000]")
    print("  thresholds = [10, 20, 50, 100, 200]")
    print("  for rate in sensing_rates:")
    print("    for threshold in thresholds:")
    print("      simulator.update_parameter('application.sensing_update_rate', rate)")
    print("      simulator.update_parameter('application.safety_critical_threshold', threshold)")
    print("      success = simulator.results['Hydra_RAN']['metrics']['safety_compliance_rate']")
    print("      print(f'Rate={rate}Hz, Threshold={threshold}ms: {success:.1f}% compliant')")
    
    print("\nExample 4: Compare approaches under stress conditions")
    print("  # Create high-stress scenario")
    print("  simulator.update_parameter('dataset.vehicle_spacing', 2)")
    print("  simulator.update_parameter('network.bandwidth', 20)")
    print("  simulator.update_parameter('application.sensing_update_rate', 500)")
    print("  simulator.run_simulation()")
    print("  ")
    print("  # Find which approaches still meet safety requirements")
    print("  for name, result in simulator.results.items():")
    print("    if result['metrics']['safety_compliance_rate'] > 90:")
    print("      print(f'{name}: SAFE')")
    print("    else:")
    print("      print(f'{name}: UNSAFE')")



# 



